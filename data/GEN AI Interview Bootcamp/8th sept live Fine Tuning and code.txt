WEBVTT

00:05:09.390 --> 00:05:11.930
okay so i think i'm audible and visible to all of you right

00:05:11.930 --> 00:05:18.370
guys yeah so just in chat say yes or no if i'm audible and

00:05:18.370 --> 00:05:23.370
visible to all of you uh okay i'm audible and visible to all

00:05:23.370 --> 00:05:27.450
fine so let's get started and today i'll be talking about

00:05:27.450 --> 00:05:29.810
another project pipeline and from tomorrow onwards i'll

00:05:29.810 --> 00:05:32.970
start talking about the interview preparation so for today's

00:05:32.970 --> 00:05:36.130
class i have already prepared a code base which i'll share

00:05:36.130 --> 00:05:38.170
obviously i'm not going to discuss about the code base

00:05:38.170 --> 00:05:43.970
because it's very big and lengthy and this is out of the

00:05:43.970 --> 00:05:46.690
scope of this particular batch so you can look into the code

00:05:46.690 --> 00:05:50.050
base but yeah i'll just give you a demo plus i'll be talking

00:05:50.050 --> 00:05:55.230
about the entire flow so technically i'm going to talk about

00:05:55.890 --> 00:06:00.170
model fine tuning so inside the industry so whenever you

00:06:00.170 --> 00:06:04.870
will join majority of time again majority of team nowadays

00:06:04.870 --> 00:06:08.350
are working on a model fine tuning because the model which

00:06:08.350 --> 00:06:12.610
is available out there so maybe those models are not a

00:06:12.610 --> 00:06:15.930
proprietary model that's a first thing for example open ai

00:06:15.930 --> 00:06:20.430
model or maybe a cloud dissonant model and the major concern

00:06:20.430 --> 00:06:24.130
with those proprietary model is that that they will not be

00:06:24.130 --> 00:06:27.150
able to use it because of the security concern or because of

00:06:27.150 --> 00:06:31.030
the api token consumptions so majority of company or

00:06:31.030 --> 00:06:33.370
majority of team let's suppose if i'm working into a

00:06:33.370 --> 00:06:36.250
specific domain maybe a healthcare research domain or maybe

00:06:36.250 --> 00:06:36.250
a healthcare research domain or maybe a healthcare research

00:06:36.270 --> 00:06:40.470
domain or maybe into a finance domain so i will be looking

00:06:40.470 --> 00:06:45.410
for my own model and which i have trained on my own data set

00:06:45.410 --> 00:06:49.850
and technically we have fine tuned on top of our own data

00:06:49.850 --> 00:06:52.010
set so obviously we'll be looking for such kind of a

00:06:52.010 --> 00:06:56.730
situation and in that kind of a situation so how things goes

00:06:56.730 --> 00:07:00.210
how we are going to do our data preparation what kind of a

00:07:00.210 --> 00:07:03.870
format generally we try to adopt for a different different

00:07:03.870 --> 00:07:07.310
kind of a fine tuning because there is no like nothing like

00:07:07.310 --> 00:07:09.810
only one kind of a fine tuning we do there are multiple

00:07:09.810 --> 00:07:12.470
different kind of fine tuning which happens which i'm going

00:07:12.470 --> 00:07:16.590
to discuss in today's class obviously in theory and again so

00:07:16.590 --> 00:07:21.410
a data preparation strategy and how much cost in general one

00:07:21.410 --> 00:07:26.110
single fine tuning takes how much time in general a single

00:07:26.110 --> 00:07:29.870
fine tuning takes what kind of a team we need to do any kind

00:07:29.870 --> 00:07:33.410
of a fine tuning all of these things we are going to discuss

00:07:33.410 --> 00:07:36.270
so which which happens with respect to every kind of fine

00:07:36.270 --> 00:07:40.110
tuning that we do across our industry so salvage start i

00:07:40.110 --> 00:07:43.070
believe context is pretty much clear to all of us salvage

00:07:43.070 --> 00:07:44.710
start everyone yeah

00:07:46.790 --> 00:07:53.540
and like i said so i have prepared a code base for today's

00:07:53.540 --> 00:07:56.220
class so i'll give you a demo and parallelly i'll try to

00:07:56.220 --> 00:07:59.360
share the code base which you can execute it's very easy to

00:07:59.360 --> 00:08:02.860
execute this is how i have prepared the code base and you

00:08:02.860 --> 00:08:06.960
all you know can go through a code if you want what's

00:08:09.330 --> 00:08:12.070
today's topic so basically fine tuning across the industry

00:08:12.190 --> 00:08:16.350
that's a topic that i'm going to discuss okay so let me open

00:08:16.350 --> 00:08:18.270
up my scribble inc and share my screen

00:08:24.830 --> 00:08:27.930
so let's i believe my screen is visible right everyone yeah

00:08:27.930 --> 00:08:31.170
just say yes if it is visible to all of you okay

00:08:36.350 --> 00:08:44.230
fine so let's go ahead with the fine tuning so as we all

00:08:44.230 --> 00:08:47.830
know that fine tuning is a process so where you are going to

00:08:47.830 --> 00:08:51.410
take an existing model and then with respect to your own

00:08:51.410 --> 00:08:55.110
data set we try to train the model so that it will be able

00:08:55.110 --> 00:08:58.690
to understand my proprietary data set or a data set which

00:08:58.690 --> 00:09:01.930
belongs to me just me basically right so if i have to train

00:09:01.930 --> 00:09:05.890
any any kind of a model now so when fine tuning comes into a

00:09:05.890 --> 00:09:09.590
picture so not just one single approach but i'm going to

00:09:09.590 --> 00:09:13.090
talk about approximately nine to ten different different

00:09:13.090 --> 00:09:16.610
kind of approach by which we generally try to do a fine

00:09:16.610 --> 00:09:19.190
tuning and again so all of this approach we try to follow

00:09:19.190 --> 00:09:22.910
across all industry so there is no like a case to be found

00:09:22.910 --> 00:09:24.010
in which i am go ahead and just go ahead and apply a the

00:09:24.010 --> 00:09:27.290
software

00:09:27.290 --> 00:09:32.590
to make sure that all these cases so the reason you guys

00:09:32.590 --> 00:09:36.450
feel it is because if

00:09:36.450 --> 00:09:43.310
an industry and it's something is out Gomez of work a

00:09:43.310 --> 00:09:47.530
company i have to pay

00:09:47.530 --> 00:09:52.310
my teacher including you the

00:09:52.310 --> 00:09:55.070
fees required so those a data or you have to prepare a data

00:09:55.070 --> 00:09:57.550
into a different different format so that is something that

00:09:57.550 --> 00:10:01.970
i'll be talking about in this entire lecture in depth and

00:10:01.970 --> 00:10:10.510
along with this i have like i said prepared a demo so let me

00:10:10.510 --> 00:10:14.650
open up my demo screen yeah

00:10:16.470 --> 00:10:20.070
so basically i have prepared a demo for a three different

00:10:20.070 --> 00:10:22.590
different kind of of no four four different different kind

00:10:22.590 --> 00:10:25.430
of fine tuning by the way so there is a fine tuning called

00:10:25.430 --> 00:10:28.390
as embedding based fine tuning there is a fine tuning called

00:10:28.390 --> 00:10:31.970
as re-ranker based fine tuning so for embedding and re

00:10:31.970 --> 00:10:35.210
-ranker i have already prepared a complete dashboard so

00:10:35.210 --> 00:10:38.150
where you will be able to come you will be able to see a

00:10:38.150 --> 00:10:40.950
data set you will be able to even play with the parameter

00:10:40.950 --> 00:10:44.110
you will be able to select a model you will be able to

00:10:44.110 --> 00:10:47.550
select a training parameter by the way you will be able to

00:10:47.550 --> 00:10:51.590
even evaluate those model in a real time and that with

00:10:51.590 --> 00:10:55.390
visualization and with respect to you and data set and for

00:10:55.390 --> 00:10:58.210
example if i'm talking about embedding fine tuning model so

00:10:58.210 --> 00:11:01.270
you will be even able to test it with in a multiple format

00:11:01.270 --> 00:11:04.810
if i'll talk about a re-ranking so again you will be able to

00:11:04.810 --> 00:11:09.170
test all of these re-ranker model and here so i have already

00:11:09.170 --> 00:11:11.930
mentioned a model detail that what kind of a model i'm

00:11:11.930 --> 00:11:15.110
taking over here but yeah these are just a one set of the

00:11:15.110 --> 00:11:18.450
model you can try to take like any kind of a model from

00:11:18.450 --> 00:11:21.110
millions of model which is available out there i have done

00:11:21.110 --> 00:11:23.830
this fine tuning in my local machine and according i have

00:11:23.830 --> 00:11:27.390
chosen this model the model size is less but yeah so if you

00:11:27.390 --> 00:11:30.130
are going at with the big one then in that case you need a

00:11:30.130 --> 00:11:33.030
heavy computation here you must need to do a fine tuning so

00:11:33.030 --> 00:11:36.290
i'll be talking about this entire dashboard one by one so

00:11:36.290 --> 00:11:38.730
once i will be able to show you all these things

00:11:38.730 --> 00:11:42.950
theoretically so here i'm showing you two approach embedding

00:11:42.950 --> 00:11:47.490
and re-ranker re-ranker fine tuning model and here so i'm

00:11:47.490 --> 00:11:50.670
going to show you two approaches one for laura and one for

00:11:50.670 --> 00:11:53.170
cora basically so there is something called as pre-fit

00:11:53.170 --> 00:11:57.510
parameter efficient fine tuning so inside that you will be

00:11:57.510 --> 00:12:00.650
able to find out two major fine tuning approach one is

00:12:00.650 --> 00:12:03.390
called as low rank adaption fine tuning and another one is

00:12:03.390 --> 00:12:06.590
called as quantize low rank adaption fine tuning so i have

00:12:06.590 --> 00:12:09.890
already prepared these things with respect to a multiple

00:12:09.890 --> 00:12:14.470
model so distal gpt or gpt2 dialog gpt a small medium large

00:12:14.470 --> 00:12:17.930
so with all of this just with the help of this ui interface

00:12:17.930 --> 00:12:21.530
you will be able to do a fine tuning without like you know

00:12:21.530 --> 00:12:25.070
writing even of a code so whether it's a cora whether it's a

00:12:25.070 --> 00:12:29.050
laura so and inside a cora so again i have attached like a

00:12:29.050 --> 00:12:33.250
dialog large direct medium a small gpt2 distal gpt gpt nano

00:12:33.250 --> 00:12:37.770
gpt neo sorry gpt nano 125 million parameter gpt neo 350

00:12:37.770 --> 00:12:41.190
million parameter and again so i have given you a control so

00:12:41.190 --> 00:12:44.130
that you know you will be able to increase or decrease all

00:12:44.130 --> 00:12:47.170
of these parameter and then you will be able to initialize

00:12:47.170 --> 00:12:51.770
the fine tuning plus data preparation strategy i will be

00:12:51.770 --> 00:12:54.310
talking about like i said so preparation strategy is not

00:12:54.310 --> 00:12:57.890
going to be same. As my fine tuning approach is going to

00:12:57.890 --> 00:13:00.690
change, my data preparation strategy is going to change. So

00:13:00.690 --> 00:13:03.090
I'll be talking about couple of data preparation strategy as

00:13:03.090 --> 00:13:06.430
well. And then the holistic overview that what should be the

00:13:06.430 --> 00:13:09.610
team size, what should be the time or maybe a number of

00:13:09.610 --> 00:13:12.330
month which is required to do this entire fine tuning,

00:13:12.510 --> 00:13:16.350
everything, everything across like this entire fine tuning

00:13:16.350 --> 00:13:21.210
approach. Yeah, fine. So self start guys. Yeah. So yeah, I

00:13:21.210 --> 00:13:23.910
believe we are able to set the context what we are going to

00:13:23.910 --> 00:13:27.130
discuss and this is something that you can write down inside

00:13:27.130 --> 00:13:29.790
your resume directly. This is something that you can, this

00:13:29.790 --> 00:13:32.150
piece of a code that you can publish directly inside your

00:13:32.150 --> 00:13:34.210
GitHub and then you can claim that, yeah, I have done that.

00:13:34.330 --> 00:13:37.950
I have not attached your own or my name anywhere by the way,

00:13:37.990 --> 00:13:42.170
uh, so that you will be able to use it. Uh, and uh, you can,

00:13:42.270 --> 00:13:45.650
you can just like a claim this entire code base as yours. I

00:13:45.650 --> 00:13:49.970
don't have any kind of a issue. Yeah. So service start with

00:13:49.970 --> 00:13:53.970
the fine tuning. Fine tuning approach if context is clear.

00:13:55.860 --> 00:13:59.960
Yeah. Okay. So let's get started with the fine tuning

00:13:59.960 --> 00:14:04.880
approach. So here, uh, I believe, uh, as we all aware, aware

00:14:04.880 --> 00:14:07.780
about a generative AI, at least a little bit. So we all are

00:14:07.780 --> 00:14:10.320
aware about even a fine tuning approach, right? Or maybe a

00:14:10.320 --> 00:14:13.120
definition of the fine tuning. So definition of the fine

00:14:13.120 --> 00:14:16.000
tuning says that that don't try to build a model from the

00:14:16.000 --> 00:14:19.160
scratch, although it's possible, but it requires a very,

00:14:19.260 --> 00:14:22.500
very huge amount of the data set and it requires a huge

00:14:22.500 --> 00:14:26.180
compute, a huge team member for a data labeling, which is a

00:14:26.180 --> 00:14:29.160
low level task, but still it's a very huge one. So in

00:14:29.160 --> 00:14:31.880
general, what we do, so we try to go ahead with some of the

00:14:31.880 --> 00:14:34.940
best pre-trained model, which is already aware about the

00:14:34.940 --> 00:14:37.640
world's data, right? Which is already aware about the

00:14:37.640 --> 00:14:41.740
world's data. And then what we do, so we try to add some

00:14:41.740 --> 00:14:44.960
layer, maybe parallely, maybe horizontally. We try to add

00:14:44.960 --> 00:14:49.680
some of the layers so that without even touching the actual

00:14:49.680 --> 00:14:52.360
weights, right? Actual weights. The weights of the model, I

00:14:52.360 --> 00:14:55.980
can try to attach our weights, which will be trained or

00:14:55.980 --> 00:14:59.520
which will be influenced. You can say with respect to my

00:14:59.520 --> 00:15:03.340
small teeny tiny data, right? So when I say teeny tiny, so

00:15:03.340 --> 00:15:05.800
I'm still talking about a millions of tokens. I'm not just

00:15:05.800 --> 00:15:08.540
talking about one or two line because again, that will not

00:15:08.540 --> 00:15:12.360
be sufficient enough to do such kind of a fine tuning so

00:15:12.360 --> 00:15:17.140
that I will be able to build a model in a low cost, but

00:15:17.140 --> 00:15:20.900
still it will be able to serve the purposes with respect to

00:15:20.900 --> 00:15:23.740
model. My domain. And with respect to finance, with respect

00:15:23.740 --> 00:15:27.460
to a medical majority of times, you will be able to see that

00:15:27.460 --> 00:15:31.640
people are doing a lot of fine tuning, even with respect to

00:15:31.640 --> 00:15:34.300
education, right? With respect to education. So people are

00:15:34.300 --> 00:15:38.100
doing a lot of fine tuning. So across our industry, you will

00:15:38.100 --> 00:15:42.600
be able to see that majority of people are not trying to

00:15:42.600 --> 00:15:45.600
build a model from the scratch, but they are just trying to

00:15:45.600 --> 00:15:48.600
do a fine tuning. The very first reason is obviously a

00:15:48.600 --> 00:15:51.540
security. Okay. The second reason is obviously a data

00:15:51.540 --> 00:15:54.060
awareness, proprietary data awareness, because model is

00:15:54.060 --> 00:15:57.760
aware about a public data, but a model is not aware about

00:15:57.760 --> 00:16:02.560
the private data now. So then you can say that, that, okay,

00:16:02.660 --> 00:16:05.700
uh, maybe with respect to a private data, if I have to give

00:16:05.700 --> 00:16:08.660
an answer, so maybe I can try to use a RAG, right? Okay.

00:16:08.740 --> 00:16:12.980
That's good. You will be able to use a RAG, but again, so

00:16:12.980 --> 00:16:16.820
RAG comes with a pros and cons. So you have to store a data

00:16:16.820 --> 00:16:20.240
into a vector. Plus you have to host a LLM. As well, which

00:16:20.240 --> 00:16:23.100
by default you have to do it. So again, you have to build

00:16:23.100 --> 00:16:27.300
the entire pipeline. So where we have to use RAG and where

00:16:27.300 --> 00:16:31.040
we have to use basically a fine tuned model. So a place

00:16:31.040 --> 00:16:36.580
where we are not changing or my data set is not changing

00:16:36.580 --> 00:16:39.120
very frequently. So in that case I can go ahead with the

00:16:39.120 --> 00:16:42.700
fine tuning model, a place where data set is changing

00:16:42.700 --> 00:16:45.740
frequently. We can go ahead with the RAG approach in

00:16:45.740 --> 00:16:47.820
general, right? In general. So again, there are like other

00:16:47.820 --> 00:16:50.280
differentiation you can do. Yeah. These, this is the major

00:16:50.280 --> 00:16:53.640
differentiation. So wherever data is not changing much and

00:16:53.640 --> 00:16:56.280
still I have to give an answer, generate an answer with the

00:16:56.280 --> 00:16:58.480
private data. In that case I can go ahead with the fine

00:16:58.480 --> 00:17:01.500
tuning and wherever my data set is changing like a rapidly

00:17:01.500 --> 00:17:04.400
on a regular basis, maybe on a daily basis, maybe on an

00:17:04.400 --> 00:17:06.920
hourly basis. So in that case I can go ahead with the RAG.

00:17:07.780 --> 00:17:10.460
Majority of times we also go ahead with the hybrid approach.

00:17:10.620 --> 00:17:13.780
So where instead of hosting a LLMs model, right? So we try

00:17:13.780 --> 00:17:16.620
to fine tune our model and we try to host our own LLM with

00:17:16.620 --> 00:17:19.800
the fine tuning. So majority of the industry goes ahead with

00:17:19.800 --> 00:17:23.420
hybrid approach as well. And like I said, so this is a kind

00:17:23.420 --> 00:17:25.960
of a work you will be able to find out everywhere means

00:17:25.960 --> 00:17:29.720
literally everywhere. So let's get started. First of all,

00:17:29.760 --> 00:17:33.420
I'll try to list down the kind of a fine tuning that we do.

00:17:33.500 --> 00:17:35.920
Like I said, nine or 10 different, different kinds of

00:17:35.920 --> 00:17:39.640
approach I'm going to write down and I'll talk about that a

00:17:39.640 --> 00:17:42.500
little bit. And if you are interested, so maybe you can try

00:17:42.500 --> 00:17:45.960
to explore more about this entire fine tuning approach.

00:17:46.160 --> 00:17:49.780
Okay. Assuming that you all are aware about some of the fine

00:17:49.780 --> 00:17:52.300
tuning as it's an interview batch. So you are preparing for

00:17:52.300 --> 00:17:55.340
the interview. So I'm assuming that you are not completely

00:17:55.340 --> 00:17:58.520
blank and I'm not supposed to like go into the topic from

00:17:58.520 --> 00:18:01.400
the scratch, right? This is my assumption, keeping that

00:18:01.400 --> 00:18:03.160
assumption in our mind. So I'm going to discuss this

00:18:03.160 --> 00:18:03.780
particular topic.

00:18:11.100 --> 00:18:15.380
Okay. So fine tuning now. So let me list down that what type

00:18:15.380 --> 00:18:18.100
of the fine tuning which exists so that it can give you a

00:18:18.100 --> 00:18:21.560
kind of a recap plus if you are not aware about these names,

00:18:21.660 --> 00:18:25.360
so maybe you will be aware about. This particular name. So

00:18:25.360 --> 00:18:29.920
the very first approach that we try to follow is a full fine

00:18:31.160 --> 00:18:37.570
tuning as its name suggests. So obviously we are going to

00:18:37.570 --> 00:18:40.710
update the entire model bates. So let's suppose we have a

00:18:40.710 --> 00:18:44.630
neural network. So obviously we'll try to change all the

00:18:44.630 --> 00:18:48.750
bates by passing our data. So all of these bates we are

00:18:48.750 --> 00:18:52.610
going to change by passing our data set and then we will be

00:18:52.610 --> 00:18:55.390
able to get the output. So whatever bates that we have. So

00:18:55.390 --> 00:18:59.430
we try to. We try to change everything now. So in what kind

00:18:59.430 --> 00:19:02.810
of cases you are going to need it. So whenever you need a

00:19:02.810 --> 00:19:06.690
model to completely specialize in a particular domain, for

00:19:06.690 --> 00:19:09.650
example, biomedical elements, if you are trying to prepare,

00:19:09.790 --> 00:19:12.570
for example, even there are like some biomedical like

00:19:12.570 --> 00:19:15.430
elements which is available. You can try to like a search by

00:19:15.430 --> 00:19:18.770
the name PubMed kind of file lens and PubMed GPT, right?

00:19:18.830 --> 00:19:21.850
Basically its name is PubMed GPT. So if you are trying to

00:19:21.850 --> 00:19:25.410
build such kind of a GPT where you just wanted to, you know,

00:19:25.410 --> 00:19:29.010
train the entire millions of parameter or billions of

00:19:29.010 --> 00:19:32.350
parameter in general, seven B model, a 20 B model. So in

00:19:32.350 --> 00:19:36.810
that case you have to go for the full fine tuning. And

00:19:36.810 --> 00:19:40.030
again, all of these things can be controlled just by one or

00:19:40.030 --> 00:19:42.370
two parameters when we go ahead with generally a fine

00:19:42.370 --> 00:19:47.570
tuning. And yes. So when if you are going ahead with a full

00:19:47.570 --> 00:19:51.630
fine tuning, so in that case you need a data set with

00:19:51.630 --> 00:19:57.150
minimum, right? 10 million plus tokens. Means the new data

00:19:57.150 --> 00:20:00.890
set. It's not like I just have a two line of data and then I

00:20:00.890 --> 00:20:03.850
can go ahead with the full fine tuning. It will be of no use

00:20:03.850 --> 00:20:07.450
by the way, right? So if you have a 10 million plus tokens,

00:20:07.610 --> 00:20:10.370
and again, this is the costliest approach because this is

00:20:10.370 --> 00:20:13.750
going to be slow. It is going to be a resource consuming

00:20:13.750 --> 00:20:16.770
approach by the way, because I'm changing the entire weight

00:20:16.770 --> 00:20:20.350
of the model. So if I have a 7 billion parameter trainable

00:20:20.350 --> 00:20:23.470
parameters, so I'm changing all. If I have 120, I'm changing

00:20:23.470 --> 00:20:26.550
all billion parameter I'm talking about. Right? So in that

00:20:26.550 --> 00:20:29.830
case, obviously you need a heavy compute, maybe a 100 kind

00:20:29.830 --> 00:20:33.330
of a GPU, H one and kind of a GPU, not just one, a multiple

00:20:33.330 --> 00:20:36.190
cluster of the GPU you need. And again, so I believe I have

00:20:36.190 --> 00:20:39.010
already discussed about this, uh, GPU setup and everything

00:20:39.010 --> 00:20:41.830
in my life classes this weekend itself. So if you have not

00:20:41.830 --> 00:20:45.170
gone through it, maybe you can go through that right now. So

00:20:45.170 --> 00:20:47.330
it's, this approach is going to be very, very expensive,

00:20:47.570 --> 00:20:51.510
literally very, very expensive. Now the second one, right?

00:20:51.590 --> 00:20:55.310
The second approach, uh, which is not expensive. Approach.

00:20:55.430 --> 00:20:57.810
And generally we try to adopt this particular approach

00:20:57.810 --> 00:21:02.730
across the industry 90% I will say a time that is called as

00:21:02.730 --> 00:21:08.790
basically we fit basically P E F T full form is very simple

00:21:08.790 --> 00:21:11.550
parameter efficient fine tuning parameter

00:21:14.720 --> 00:21:16.240
efficient

00:21:19.570 --> 00:21:21.030
fine

00:21:24.580 --> 00:21:28.520
tuning gap now. So this is just a concept by the way. So

00:21:28.520 --> 00:21:31.280
this parameter efficient fine tuning is not a approach

00:21:31.280 --> 00:21:35.600
basically like it's just a concept. It means we try to

00:21:35.600 --> 00:21:39.180
attach some of the parameter and then we just try to fine

00:21:39.180 --> 00:21:42.620
tune that parameter. We are not going to change the entire

00:21:42.620 --> 00:21:46.280
weight of the model. It's just an approach. Now inside this

00:21:46.280 --> 00:21:49.120
P fit, right? A parameter efficient fine tuning. There are

00:21:49.120 --> 00:21:51.920
different different approaches. You will be able to find out

00:21:51.920 --> 00:21:55.520
the very first approach that you will be able to see inside

00:21:55.520 --> 00:21:59.160
a P fit, a parameter efficient fine tune. Like I said, this

00:21:59.160 --> 00:22:02.680
is just a concept, right? It's just a conceptual thing. It's

00:22:02.680 --> 00:22:05.400
not like a real. By the way. Inside this, there are

00:22:05.400 --> 00:22:08.200
subtopics which is real and which we try to follow and by

00:22:08.200 --> 00:22:11.200
which we try to do a fine tuning. So there is something

00:22:11.200 --> 00:22:16.060
called as L O R A Laura basically, right? So it's called as

00:22:16.060 --> 00:22:23.460
low rank adoption, low rank adoption.

00:22:28.030 --> 00:22:31.630
So what we do in general in this one. So basically we try to

00:22:31.630 --> 00:22:34.950
like create a new weights means we already have an existing

00:22:34.950 --> 00:22:38.690
weight in parallel to this, right? Let's suppose we have a

00:22:38.690 --> 00:22:41.270
neural network, technically like your transformers are going

00:22:41.270 --> 00:22:44.290
to be this neural network. Just imagine, right? A complex

00:22:44.290 --> 00:22:47.730
complex neural network, right? So parallel to this, so we

00:22:47.730 --> 00:22:52.190
try to build a small neural network. I can say we try to

00:22:52.190 --> 00:22:55.570
pass a data over here and then combinedly it is going to

00:22:55.570 --> 00:22:59.650
give me a final output, right? So we are not going to touch

00:22:59.650 --> 00:23:03.170
the actual weights. We are going to freeze this entire

00:23:03.170 --> 00:23:05.690
weight. By the way, we are going to freeze this entire

00:23:05.690 --> 00:23:09.690
weight. We are just going to train a teeny tiny weights over

00:23:09.690 --> 00:23:12.250
here. But when it will try to give the answer, so obviously

00:23:12.250 --> 00:23:15.410
it will try to give an answer from both so that it will be

00:23:15.410 --> 00:23:18.890
having a world's understanding plus it will be having my

00:23:18.890 --> 00:23:22.790
data understanding as well. Plus I can avoid tuning the

00:23:22.790 --> 00:23:26.330
entire weights, which is a very, very expensive approach.

00:23:26.490 --> 00:23:30.530
It's called as Laura low rank adoption, right? So where new

00:23:30.530 --> 00:23:33.990
weight is nothing but you can say old debate plus delta of

00:23:33.990 --> 00:23:37.290
the weights, right? Delta of the weights you will be able to

00:23:37.290 --> 00:23:41.570
find out. So this is called as a Laura now. So basically

00:23:41.570 --> 00:23:46.210
this is a like a very small transformer layer we try to add

00:23:46.210 --> 00:23:48.750
or you can say a neural network in a lemon way. You can say

00:23:48.750 --> 00:23:51.250
that. Okay, fine. So we are adding a very small neural

00:23:51.250 --> 00:23:54.490
network layer parallel to the model that I have. And then we

00:23:54.490 --> 00:23:57.730
are trying to train those delta, right? We are just trying

00:23:57.730 --> 00:24:01.490
to train those delta. So technically we are going to train

00:24:01.490 --> 00:24:06.270
approximately one to 5% of weights. Of the original

00:24:06.270 --> 00:24:09.530
parameter weight in that ratio. You can just try to imagine

00:24:09.530 --> 00:24:14.470
right in that ratio and it will be able to give me an answer

00:24:14.470 --> 00:24:18.950
with respect to my private data. Yeah, my private data. For

00:24:18.950 --> 00:24:22.950
example, if I have to teach my elements to generate a legal

00:24:22.950 --> 00:24:27.030
contract, right? Based on my way of generating a legal

00:24:27.030 --> 00:24:29.030
contract, let's suppose I have a thousands of legal

00:24:29.030 --> 00:24:32.030
contracts since last 10 year, which I have generated for my

00:24:32.030 --> 00:24:35.290
company. So maybe I can try to provide that data. Okay. Then

00:24:35.290 --> 00:24:38.410
in, in this Delta, right in this Delta, and then I can say

00:24:38.410 --> 00:24:41.830
that, okay, fine. So just try to train based on my data. And

00:24:41.830 --> 00:24:44.470
then whenever I'm going to ask you a question that generate

00:24:44.470 --> 00:24:48.130
a legal contract, you are aware about my pattern, basically

00:24:48.130 --> 00:24:50.750
you are aware about my pattern. So basically you will try to

00:24:50.750 --> 00:24:54.510
generate it based on my pattern now. So this is called as

00:24:54.510 --> 00:24:57.950
Laura, which comes under parameter efficient fine tuning. So

00:24:57.950 --> 00:25:01.130
where we are going to train couple of parameter, a small set

00:25:01.130 --> 00:25:04.790
of the parameter you can say, but not the all now. Now, so

00:25:04.790 --> 00:25:08.510
B, there is something called as Q Laura Laura.

00:25:11.420 --> 00:25:15.460
Now what it does by the way, yeah, you Laura. So basically,

00:25:15.580 --> 00:25:20.380
uh, what we do over here. So Q means quantized by the way.

00:25:20.420 --> 00:25:23.420
Now what is the meaning of quantized quantized in a layman

00:25:23.420 --> 00:25:27.560
way? If I have to explain you not in a mathematical way. So

00:25:27.560 --> 00:25:32.240
quantization is nothing but, uh, representing a data into a

00:25:32.240 --> 00:25:35.580
smaller bit by the way. So instead of representing a data

00:25:35.580 --> 00:25:38.220
into a 16 bit. Okay. By the way, your numbers will be

00:25:38.220 --> 00:25:40.740
represented in a 16 bit. Let's suppose inside a computer

00:25:40.740 --> 00:25:44.820
system in a 0 1 0 1 0 1 0 1. So instead of that, if we can

00:25:44.820 --> 00:25:48.540
try to represent our data into a smaller bit, maybe a four

00:25:48.540 --> 00:25:51.220
bit, right? Maybe in a four bit. So that is something called

00:25:51.220 --> 00:25:54.620
as quantization. So where I'm changing the representation of

00:25:54.620 --> 00:25:59.060
the data itself, so that my model will be my overall model

00:25:59.060 --> 00:26:01.880
is going to be lightweighted, right? It's going to be

00:26:01.880 --> 00:26:04.660
lightweighted. It is going to be very, very small. So that

00:26:04.660 --> 00:26:08.760
if I have to run my model. On my mobile devices or maybe in

00:26:08.760 --> 00:26:12.200
a edge device, right? We have like, uh, so many different,

00:26:12.280 --> 00:26:14.840
different kinds of, uh, as devices, for example, from a

00:26:14.840 --> 00:26:17.580
Google, there is a coral raspberry PI. I believe we all are

00:26:17.580 --> 00:26:20.800
aware about it, right? Raspberry PI. Maybe if I am trying to

00:26:20.800 --> 00:26:23.860
burn this entire model into some of the robotic system, so

00:26:23.860 --> 00:26:27.040
where compute will be very less, maybe if I have to use this

00:26:27.040 --> 00:26:30.760
model into a mobile device itself, it's also possible. I

00:26:30.760 --> 00:26:33.400
have, I think I have already shown like a lot of example in

00:26:33.400 --> 00:26:36.820
my past, uh, with respect to a mobile device. Uh. Model

00:26:36.820 --> 00:26:39.420
hosting, right? A big model, how you can try to quantize it

00:26:39.420 --> 00:26:42.240
and then how we can try to host it in a mobile device. And

00:26:42.240 --> 00:26:45.480
we, we does that a lot, right? So that, uh, you don't need

00:26:45.480 --> 00:26:48.840
an internet access in a local with a very small processing

00:26:48.840 --> 00:26:52.840
unit, you will be able to use those model. So in that kind

00:26:52.840 --> 00:26:56.000
of a situation, we always try to host a quantized model. Now

00:26:56.000 --> 00:26:59.160
quantization is not a new concept, right? It's a very, very

00:26:59.160 --> 00:27:03.060
old concept. And even in a computer vision or even any kind

00:27:03.060 --> 00:27:05.580
of a model, right, that we try to build, which is a bulky,

00:27:05.620 --> 00:27:07.920
bulky model. It's bulkier in nature, especially computer

00:27:07.920 --> 00:27:10.900
vision model or NLP model, this LLMs and everything, right?

00:27:11.100 --> 00:27:15.620
So a transformer based model. So over there, whenever I have

00:27:15.620 --> 00:27:18.900
a need to host a model on an edge devices, maybe on a

00:27:18.900 --> 00:27:21.100
Raspberry Pi, maybe on a Jetson Nano, maybe on a Google

00:27:21.100 --> 00:27:24.040
Corel, something like that, or maybe on some robotic or

00:27:24.040 --> 00:27:27.660
robotic processing unit. In that case, I always look for a

00:27:27.660 --> 00:27:31.180
quantized low rank adoption model. I can't go ahead with the

00:27:31.180 --> 00:27:34.420
LoRa, right? Or even majority of time. Right. You have to be

00:27:34.420 --> 00:27:36.780
able to find out that you can't depend upon the internet,

00:27:36.980 --> 00:27:40.520
right? You have to do a local processing. Local processing

00:27:40.520 --> 00:27:43.620
means in device processing. And obviously you need a very

00:27:43.620 --> 00:27:46.340
lightweight, but it's still a efficient model. And this is

00:27:46.340 --> 00:27:50.800
where this quantized low rank adoption, the full form of

00:27:50.800 --> 00:27:52.540
this Q LoRa is nothing but quantized.

00:27:57.100 --> 00:28:01.200
Quantized low rank adoption.

00:28:03.540 --> 00:28:06.500
Yeah. So we always go with the quantized low rank adoption

00:28:06.500 --> 00:28:08.760
model. So where we just try to change the representation.

00:28:08.760 --> 00:28:11.420
Instead of representing my data into a 16 bit, I'll try to

00:28:11.420 --> 00:28:14.480
represent in a smaller bit, maybe a four bit, right? Maybe a

00:28:14.480 --> 00:28:17.940
four bit. So my compute uses will be less. Now what we do in

00:28:17.940 --> 00:28:21.860
general in this case, right? So here, uh, it's nothing but

00:28:21.860 --> 00:28:25.020
it's basically, uh, as, as its name suggests, it's a

00:28:25.020 --> 00:28:28.460
quantized plus you can say, right? Maybe plus just for the

00:28:28.460 --> 00:28:30.860
understanding purpose, you can try to say that quantized

00:28:30.860 --> 00:28:35.180
plus low rank adoption means whatever weight that we have,

00:28:35.280 --> 00:28:37.560
because in an original model, how we are trying to decrease

00:28:37.560 --> 00:28:41.260
the size of the model. Or when model size will increase, if

00:28:41.260 --> 00:28:45.800
number of bits will be more, right? And number of parameter

00:28:45.800 --> 00:28:49.520
will be more, then obviously size will be more because size

00:28:49.520 --> 00:28:53.160
is nothing but a space occupied by those weights, those

00:28:53.160 --> 00:28:56.780
numbers, right? Inside a hardware. And this is where like a,

00:28:56.820 --> 00:29:00.920
your model became like a 420 GB model, 500 GB model, 3 GB, 8

00:29:00.920 --> 00:29:04.620
GB kind of a models. So if I'm going to decrease a bit

00:29:04.620 --> 00:29:08.060
itself means it is going to occupy a less space, almost one

00:29:08.060 --> 00:29:10.520
by fourth time, one by fifth time space it is going to

00:29:10.520 --> 00:29:13.540
utilize is my model size will be reduced. So same 8 GB

00:29:13.540 --> 00:29:16.520
model, maybe one by fourth, right? Maybe I will be able to

00:29:16.520 --> 00:29:20.420
like a hosted in 2 GB, 100 MB of model, maybe just 25 MB of

00:29:20.420 --> 00:29:23.280
model I will be able to make out of this one. So this is

00:29:23.280 --> 00:29:25.600
where this quantized low rank adoption comes into a picture.

00:29:25.680 --> 00:29:30.380
So where we try to quantize the original model, right? We

00:29:30.380 --> 00:29:33.520
try to quantize the original model. So base model we can

00:29:33.520 --> 00:29:37.400
say, so here combine the definition says that that combine.

00:29:40.300 --> 00:29:43.180
Combine four bit

00:29:48.500 --> 00:29:52.940
quantization of base model, the big one model.

00:29:55.300 --> 00:29:58.760
And anyhow, this Delta W, right, which is nothing but a low

00:29:58.760 --> 00:30:00.860
rank adoption. So which is going to help me out. So this

00:30:00.860 --> 00:30:03.320
will be aware about what this will be aware about the

00:30:03.320 --> 00:30:06.260
world's data, world's information plus what we are going to

00:30:06.260 --> 00:30:10.440
do. So plus Delta W. So Delta is coming from my low rank

00:30:10.440 --> 00:30:14.080
adoption. And basically this is nothing but a training on

00:30:14.080 --> 00:30:16.920
the new data so that it will be aware about my old data as

00:30:16.920 --> 00:30:20.620
well as new data. But size will be reduced significantly. So

00:30:20.620 --> 00:30:24.400
here combine a four bit quantized base model plus LoRa, you

00:30:24.400 --> 00:30:29.200
can say, right, LoRa means Delta W, LoRa. So this is

00:30:29.200 --> 00:30:31.700
something called as QLoRa. So this is again one of the

00:30:31.700 --> 00:30:34.740
approach that we try to follow whenever we are going ahead

00:30:34.740 --> 00:30:37.940
with the parameter efficient fine tuning, just to like a

00:30:37.940 --> 00:30:40.780
host these models on a small devices, even with a computer

00:30:40.780 --> 00:30:43.680
vision, we do a lot. Because let's suppose if I'm trying to

00:30:43.680 --> 00:30:47.460
build maybe some robotic simulation of cell driven car, and

00:30:47.460 --> 00:30:50.760
I want a local processing. So obviously, I can't keep on

00:30:50.760 --> 00:30:53.640
sending a data over the internet, I don't have that much of

00:30:53.640 --> 00:30:56.540
like a latency, right? Because my car is supposed to work

00:30:56.540 --> 00:30:59.940
everywhere, even a place where like a network availability

00:30:59.940 --> 00:31:04.400
is nil, right? Because otherwise, my car will crash if car

00:31:04.400 --> 00:31:06.840
is trying to drive itself. So in that case, obviously, I

00:31:06.840 --> 00:31:12.040
need a local onboard processing. And these are the places I

00:31:12.040 --> 00:31:16.260
can't keep like an entire, you know, server in my car, by

00:31:16.260 --> 00:31:18.880
the way. So obviously, I need something so which will be

00:31:18.880 --> 00:31:22.740
able to do the inferencing, even with the very low compute.

00:31:22.880 --> 00:31:25.080
And to do that, obviously, I have to decrease the model

00:31:25.080 --> 00:31:28.080
size, right? But I have to retain, right, I should not

00:31:28.080 --> 00:31:31.060
compromise with the accuracy. So this is where this

00:31:31.060 --> 00:31:34.080
quantization concept of quantization comes into a picture

00:31:34.080 --> 00:31:36.460
which we implement in majority of the places, not just in

00:31:36.460 --> 00:31:39.860
LLMs. So this concept is actually not coming from LLMs or

00:31:39.860 --> 00:31:43.220
just for LLMs. This is a very old concept that we used to

00:31:43.220 --> 00:31:46.240
like learn even in a computer vision a lot. And, but it's

00:31:46.240 --> 00:31:48.300
not. So again, from same places, this has been inherited.

00:31:49.060 --> 00:31:53.260
Now the another parameter efficient fine tuning you will be

00:31:53.260 --> 00:31:56.260
able to find out is called as prefix tuning or prompt

00:31:56.260 --> 00:31:59.740
tuning. This is again another approach which falls under a

00:31:59.740 --> 00:32:05.330
pre-fit itself, prefix tuning,

00:32:07.150 --> 00:32:08.770
or you can say prompt tuning,

00:32:13.300 --> 00:32:17.800
prompt tuning. Now maybe many of you are not aware about

00:32:17.800 --> 00:32:20.640
this one, this particular approach, because many people are

00:32:20.640 --> 00:32:23.780
not like talking about it. But yeah, these are the another

00:32:23.780 --> 00:32:26.280
approach. I don't know. Has a drone lab before as well.

00:32:26.460 --> 00:32:29.520
Yeah. So over there we used to like, you know, there's a lot

00:32:29.520 --> 00:32:29.860
of things.

00:32:33.600 --> 00:32:36.820
Claiming your code as yours, will it be fair? Yeah. Like

00:32:36.820 --> 00:32:39.520
everything is fair. It's, it's completely fine. Don't think

00:32:39.520 --> 00:32:43.580
about fair and unfair much. So everything is fair if it is

00:32:43.580 --> 00:32:47.520
like a working for you, that's it. That's my definition of

00:32:47.520 --> 00:32:50.740
like a fairness by the way. So if it works for you, just use

00:32:50.740 --> 00:32:56.000
it. I don't have any issue. Okay. Is it fine guys? Everyone?

00:33:03.990 --> 00:33:05.510
Yeah. Okay.

00:33:09.960 --> 00:33:13.340
So let's go ahead with the third approach, prefix fine

00:33:13.340 --> 00:33:16.580
tuning by the way. So here what we do, prefix or prompt

00:33:16.580 --> 00:33:20.580
tuning. So again, this is one of the easy approach I would

00:33:20.580 --> 00:33:25.080
say. So where, what we do. So we try to attach a virtual

00:33:25.080 --> 00:33:30.580
token, right? We try to attach a virtual token and then we

00:33:30.580 --> 00:33:34.560
try to like send a data. So here let's suppose if I'm giving

00:33:34.560 --> 00:33:40.200
you an example. That. Like a, my name is S

00:33:44.620 --> 00:33:49.400
U D H. So what do we do? So we try to attach a token with

00:33:49.400 --> 00:33:55.320
this one, with this one. And then in between, let me like a

00:33:55.320 --> 00:33:59.800
break it. CLS. Yeah. So we try to attach a token with this

00:33:59.800 --> 00:34:02.360
particular sentence, which I'm trying to give. And then

00:34:02.360 --> 00:34:05.800
basically this is technically called as a virtual token. And

00:34:05.800 --> 00:34:10.140
basically this is optimized during a training. With respect

00:34:10.140 --> 00:34:13.920
to a virtual token. We will be able to do a bit of aateach.

00:34:14.180 --> 00:34:17.460
And that's it. And it will be able to like, uh, tune the

00:34:17.460 --> 00:34:20.160
entire model based on the prefix and it will be able to

00:34:20.160 --> 00:34:23.660
understand your entire sentences in a better way. So

00:34:23.660 --> 00:34:26.260
wherever we are building up best or better sentiment

00:34:26.260 --> 00:34:31.520
classifier based on my data, or maybe if I have fine tune a

00:34:31.520 --> 00:34:35.980
tone of the data, right? Tone of the data. Uh, so in that

00:34:35.980 --> 00:34:39.700
case, for example, a regional tone. If I had to adopt. so in

00:34:39.700 --> 00:34:43.200
that case model is already aware about this entire like a

00:34:43.200 --> 00:34:46.940
line basically and what we are trying to do so we are just

00:34:46.940 --> 00:34:49.540
trying to attach the instruction or maybe i'm trying to

00:34:49.540 --> 00:34:53.400
attach my own tone over there so that it will be able to

00:34:53.400 --> 00:34:56.020
adopt it so you must have seen that many people are building

00:34:56.020 --> 00:34:58.560
a different different kind of a voice cloning right

00:34:58.560 --> 00:35:01.540
different different kind of a voice cloning or even if i'll

00:35:01.540 --> 00:35:04.840
talk about the english so tone is not going to be same or

00:35:04.840 --> 00:35:10.260
like the kind of a model that everyone is trying to build so

00:35:10.260 --> 00:35:13.520
tone and the nature and behavior of the model is not going

00:35:13.520 --> 00:35:16.200
to be same so basically we can try to control or we can try

00:35:16.200 --> 00:35:19.900
to adopt those things with the help of this prefix or prompt

00:35:19.900 --> 00:35:22.480
tuning with the help of this prompt tuning approach so

00:35:22.480 --> 00:35:26.480
wherever this small adoption is required with respect to a

00:35:26.480 --> 00:35:30.040
toning right with respect to tone so in that case we can go

00:35:30.040 --> 00:35:32.520
ahead with the prefix fine tuning again it's one of the

00:35:32.520 --> 00:35:36.520
light-weighted approach and it again falls under basically a

00:35:36.520 --> 00:35:39.560
pre-fit a parameter pre-fit efficient fine tuning yeah

00:35:39.560 --> 00:35:44.160
parametric efficient fine tuning then we have another

00:35:44.160 --> 00:35:47.860
approach inside a pre-fit itself called as adapter fine

00:35:47.860 --> 00:35:52.720
tuning adapter fine tuning so what is the meaning of this

00:35:52.720 --> 00:35:55.940
adapter fine tuning by the way so basically we try to attach

00:35:55.940 --> 00:36:01.160
a small neural network rare inside a transformer block

00:36:01.160 --> 00:36:04.540
itself so here in this case we are attaching it outside

00:36:04.540 --> 00:36:07.900
right we are trying to attach it outside by the way but in

00:36:07.900 --> 00:36:11.480
case of adapter just like like adapter we try to use it

00:36:11.480 --> 00:36:14.140
right you know with respect to our switch boards basically

00:36:14.140 --> 00:36:17.700
right in our household so that it's nothing it's not going

00:36:17.700 --> 00:36:19.900
to change the nature of the switchboard by the way so we

00:36:19.900 --> 00:36:23.260
just try to attach adapter so that this one adapter and then

00:36:23.260 --> 00:36:26.800
I can try to you know connect with a multiple electric

00:36:26.800 --> 00:36:29.960
appliances by the way in the similar manner this is called

00:36:29.960 --> 00:36:33.760
as adapter so where we try to attach a small neural network

00:36:33.760 --> 00:36:37.580
inside a transformer itself inside that element self not

00:36:37.720 --> 00:36:40.340
outside not parallel by the way so inside it that is

00:36:40.340 --> 00:36:43.220
something called as adapter approach again this is one of

00:36:43.220 --> 00:36:47.860
the like a cost efficient approach right one of the light

00:36:47.860 --> 00:36:51.740
weighted approach so let me write down the definition add

00:36:51.740 --> 00:37:00.370
the small neural network inside transformer

00:37:03.310 --> 00:37:06.370
simple so the transformer layer that we are using inside the

00:37:06.370 --> 00:37:09.370
LLM so we just try to add the another neural network over

00:37:09.370 --> 00:37:12.390
there so these are the approaches by the way which falls

00:37:12.390 --> 00:37:15.130
under a prefet parameter efficient fine tuning so everywhere

00:37:15.130 --> 00:37:17.690
we are playing with just a parameter nothing much by the way

00:37:17.690 --> 00:37:20.630
yeah nothing much in case of full fine tuning so basically

00:37:20.630 --> 00:37:23.130
we go ahead with all the parameter changes and in case of

00:37:23.130 --> 00:37:25.710
this one so we just go ahead with a small small small small

00:37:25.710 --> 00:37:28.710
parameter or fraction of parameter not the 100 parameter by

00:37:28.710 --> 00:37:32.570
the way then there is something called as the another third

00:37:32.570 --> 00:37:36.470
approach so approach number two was prefet approach number

00:37:36.470 --> 00:37:41.550
three is called as basically instruction instruction

00:37:50.800 --> 00:37:56.320
tuning by the way so based on the task instruction for a

00:37:56.320 --> 00:37:59.600
specific or a specialized task let's suppose if I am just

00:37:59.600 --> 00:38:03.080
building a model for a research paper and that to research

00:38:03.080 --> 00:38:07.680
paper in a particular category yeah maybe like in a medical

00:38:07.680 --> 00:38:11.360
for a particular category so in that case we can go ahead

00:38:11.360 --> 00:38:14.540
with the instruction attuning and this is where this kind of

00:38:14.540 --> 00:38:18.420
a fine tuning comes into a picture so basically in this way

00:38:18.420 --> 00:38:22.560
or maybe like let's suppose I'm a human right so we have to

00:38:22.560 --> 00:38:25.660
give a instruction that okay do this do that or like always

00:38:25.660 --> 00:38:27.940
try to follow this approach then this approach then this

00:38:27.940 --> 00:38:31.000
approach so maybe if I have documented each and everything

00:38:31.000 --> 00:38:34.440
to handle or to give a instruction to a robot or maybe to

00:38:34.440 --> 00:38:38.160
some other system and if I would like to learn from those

00:38:38.160 --> 00:38:43.180
data right learn from those data so in that case we can try

00:38:43.180 --> 00:38:47.540
to go ahead with the instruction fine tuning by the way so

00:38:47.540 --> 00:38:51.260
here what we generally do so we try to give a data set in

00:38:51.260 --> 00:38:54.720
such a way that where there will be a task instruction for a

00:38:54.720 --> 00:38:56.480
particular task task

00:38:59.680 --> 00:39:06.560
instruction plus we try to give a input output input output

00:39:06.560 --> 00:39:11.240
pairs by the way so here task instruction and then input

00:39:11.240 --> 00:39:14.040
repair means so let's suppose if I'm giving some tasks so

00:39:14.040 --> 00:39:16.360
for that task what should be the input and what should be

00:39:16.360 --> 00:39:18.960
the output because for every task that is just like a

00:39:18.960 --> 00:39:21.480
function right executing a function so So for every function

00:39:21.480 --> 00:39:24.040
so what will be the parameter which will go in and what is

00:39:24.040 --> 00:39:27.140
the return that I will be able to take it as a output. So

00:39:27.140 --> 00:39:29.540
this is where this instruction fine tuning comes into a

00:39:29.540 --> 00:39:33.820
picture. Then the fourth kind of a fine tuning that you will

00:39:33.820 --> 00:39:38.600
be able to see in almost all the places is called as RLHF

00:39:38.600 --> 00:39:41.280
reinforcement learning with the human feedback or feedback

00:39:41.280 --> 00:39:44.000
based training we used to say. So reinforcement

00:39:44.000 --> 00:39:45.540
reinforcement

00:39:51.230 --> 00:39:53.150
learning with

00:39:57.340 --> 00:40:01.290
human feedback

00:40:04.390 --> 00:40:09.390
by the way. So here basically we try to attach a reward

00:40:09.390 --> 00:40:14.450
based system. So if model is able to predict like a well so

00:40:14.450 --> 00:40:17.670
we are going to give a reward just like we try to give you

00:40:17.670 --> 00:40:19.610
know a chocolate to a children so whenever they are going to

00:40:19.610 --> 00:40:22.290
perform some sort of a task accurately one chocolate two

00:40:22.290 --> 00:40:24.290
chocolate three chocolate five chocolate so maybe these are

00:40:24.290 --> 00:40:27.110
the rewards. So in the similar manner. So here as a human.

00:40:27.110 --> 00:40:31.290
You will sit and then you will try to give a reward based on

00:40:31.290 --> 00:40:34.150
the model instruction and based on that reward feedback

00:40:34.150 --> 00:40:38.210
system is going to learn and then system is going to adopt.

00:40:38.510 --> 00:40:41.710
So maybe that reward can be a numeric system that reward can

00:40:41.710 --> 00:40:45.710
be like a prompt input that okay no you are not supposed to

00:40:45.710 --> 00:40:48.250
like generate output in this way maybe you should generate

00:40:48.250 --> 00:40:52.130
in that way. So in this way we can try to basically like a

00:40:52.130 --> 00:40:55.990
build or we can try to do a fine tuning. Generally we try to

00:40:55.990 --> 00:41:02.490
adopt this thing. This is Dev when why not give a reward

00:41:02.490 --> 00:41:06.410
based system. But sometimes while we are carrying the load

00:41:06.410 --> 00:41:09.710
so we can have this notion that like you know it takes again

00:41:09.710 --> 00:41:11.130
10 or but these are consider kind of functionally but it is

00:41:11.130 --> 00:41:16.490
a pattern. One of them is that if you want to have a reward

00:41:16.490 --> 00:41:17.290
based system you could set up a reward And then a reward

00:41:17.290 --> 00:41:21.570
based system or just I will nicely multiply a reward based

00:41:21.570 --> 00:41:22.330
system I will get that reward based system ready to get that

00:41:22.330 --> 00:41:26.570
reward based system. So that is the way structured.

00:41:27.110 --> 00:41:29.410
complete training we can go ahead with the fine tuning

00:41:29.410 --> 00:41:33.850
basically and over there so human will be able to give a

00:41:33.850 --> 00:41:37.650
feedback and this is something that we do every time with

00:41:37.650 --> 00:41:40.630
every model whatever public model that you will be able to

00:41:40.630 --> 00:41:45.570
see which people use us to release so obviously rlhf is

00:41:45.570 --> 00:41:49.590
something that we all use us to do before making it publicly

00:41:49.590 --> 00:41:53.810
available because if i'm trying to pass a data and if it

00:41:53.810 --> 00:41:56.530
will start giving me some sort of output which is private if

00:41:56.530 --> 00:41:59.910
it is it will start sharing the data or if it will you know

00:41:59.910 --> 00:42:04.650
start giving some sort of a slangs right so again this is

00:42:04.650 --> 00:42:08.650
not going to be good because again slangs are nothing but

00:42:08.650 --> 00:42:11.790
for model is just a token right it's just a token just a

00:42:11.790 --> 00:42:15.670
piece of word right or any data set for a model is just a

00:42:15.670 --> 00:42:18.050
piece of word so you have to give an instruction by the way

00:42:18.050 --> 00:42:20.710
right you have to give an instruction and that too when you

00:42:20.710 --> 00:42:23.890
are training with the billions of data right billions of

00:42:23.890 --> 00:42:27.590
tokens billions of record so basically this rlhf is been

00:42:27.590 --> 00:42:31.610
used for majorly right majorly you will be able to find out

00:42:31.610 --> 00:42:35.370
it's been used for a safety alignment and we also do it so

00:42:35.370 --> 00:42:37.790
by default doesn't matter which approach you are going to

00:42:37.790 --> 00:42:39.710
use or whether you are going to train a model from the

00:42:39.710 --> 00:42:43.570
scratch but yeah rlhf is something that we always try to

00:42:43.570 --> 00:42:46.910
implement on top of in general the model which i'm making it

00:42:46.910 --> 00:42:50.770
available for the entire public maybe for your client so uh

00:42:50.770 --> 00:42:55.510
for a safety concern for our safety alignment we

00:42:57.880 --> 00:43:01.180
try to use a rlhf approach by the way i mean it's a fine

00:43:01.180 --> 00:43:03.880
tuning approach so we are trying to train the entire model

00:43:03.880 --> 00:43:07.520
so that it will be able to like you know learn all the

00:43:07.520 --> 00:43:11.440
helpful behaviors and it will not try to learn or maybe

00:43:13.360 --> 00:43:17.380
unlearn a harmless behaviour right that was the whole idea

00:43:17.380 --> 00:43:22.280
and also it will be able to learn some sort of a tone it

00:43:22.280 --> 00:43:23.980
will be able to learn some sort of a politism it will be

00:43:23.980 --> 00:43:25.620
able to learn some sort of a politely Because at the end of

00:43:25.620 --> 00:43:28.780
the day it is a machine right, it is a machine so it will be

00:43:28.780 --> 00:43:31.180
able to learn those politeness, it will be able to learn the

00:43:31.180 --> 00:43:35.780
tone, this is the fourth approach. Now the fifth approach

00:43:35.780 --> 00:43:38.940
which is nothing but extension of this RLHF itself called as

00:43:38.940 --> 00:43:43.240
DPO, inside DPO again there are like multiple subcategory

00:43:43.240 --> 00:43:47.120
but let's talk about just a DPO over here. So DPO is nothing

00:43:47.120 --> 00:43:49.080
but a direct preference optimization,

00:43:55.210 --> 00:43:57.530
direct preference,

00:44:00.520 --> 00:44:01.480
even

00:44:05.380 --> 00:44:07.480
this approach you will be able to find out in majority of

00:44:07.480 --> 00:44:10.000
the places. So this is nothing but extension of this RLHF,

00:44:10.140 --> 00:44:12.940
the reinforcement learning with the human feedback that we

00:44:12.940 --> 00:44:18.660
have already discussed. So in general like this RLHF right,

00:44:18.820 --> 00:44:24.840
so this RLHF is called as PPO, this other name of this RLHF

00:44:24.840 --> 00:44:30.840
is also called as PPO, in general, PPO. So what is the

00:44:30.840 --> 00:44:33.300
meaning of this PPO? So this meaning of PPO is approximate

00:44:33.300 --> 00:44:36.480
policy optimization. So we are trying to like learn the

00:44:36.480 --> 00:44:39.080
tone, politeness or those things. So this is called as

00:44:39.080 --> 00:44:41.880
policy optimization or proximal policy optimization. So

00:44:41.880 --> 00:44:44.260
where it will try to unlearn the harmful behavior and then

00:44:44.260 --> 00:44:46.500
it will try to learn the helpful behavior, tone and all

00:44:46.500 --> 00:44:51.500
those things. Now so here right, so here in case of a direct

00:44:51.500 --> 00:44:56.420
preference optimization, what it will do? So basically it

00:44:56.420 --> 00:45:02.800
will try to avoid this costly RLHF right, so costly RLHF

00:45:02.800 --> 00:45:06.440
based approach, so where we are trying to give all the

00:45:06.440 --> 00:45:11.260
preferences over here, all the optimized way to learn each

00:45:11.260 --> 00:45:15.860
and everything. So what we try to do? So we try to like you

00:45:15.860 --> 00:45:21.220
know instead of giving like instruction, all the instruction

00:45:21.220 --> 00:45:27.140
by humans right, so what we try to do, so we try to create a

00:45:27.140 --> 00:45:31.100
proper subset of the data and we try to just send those data

00:45:31.100 --> 00:45:33.340
that okay fine, try to learn this, unlearn these things, try

00:45:33.340 --> 00:45:36.140
to take this particular tone or maybe try to learn this

00:45:36.140 --> 00:45:39.000
politeness on top of this particular data. So it's

00:45:39.000 --> 00:45:43.180
technically RLHF right but here, so instead of giving a

00:45:43.180 --> 00:45:46.100
instruction from a human side because again and again you

00:45:46.100 --> 00:45:48.760
are trying to train a big models and if you are trying to

00:45:48.760 --> 00:45:51.720
give all those data just by human, chatting with the human,

00:45:51.880 --> 00:45:54.480
it will be a very costly approach. Maybe you have to wait

00:45:54.480 --> 00:45:58.020
for a month long pipeline and you have to deploy so many

00:45:58.020 --> 00:46:01.200
human being right. Because like there are, there could be a

00:46:01.200 --> 00:46:04.680
trillions of context which is possible. So instead of doing

00:46:04.680 --> 00:46:08.540
that, we try to do a like a, we try to give a data set which

00:46:08.540 --> 00:46:12.760
is already a prepared one based on some sort of a tone or

00:46:12.760 --> 00:46:15.240
some sort of a behavior and we say that okay fine, just

00:46:15.240 --> 00:46:17.960
learn this much of behavior and model will be able to learn

00:46:17.960 --> 00:46:20.180
this much of behavior. Beyond that it will not be able to

00:46:20.180 --> 00:46:23.300
respond. That is something called as direct preference

00:46:23.300 --> 00:46:28.620
optimization which is subset of the RLHF itself. Then there

00:46:28.620 --> 00:46:33.320
is something called as a style or persona optimization.

00:46:33.320 --> 00:46:39.960
Persona fine tuning, style or persona based fine tuning,

00:46:46.400 --> 00:46:50.740
persona based fine tuning. So basically let's suppose if I

00:46:50.740 --> 00:46:53.540
am trying to train a speech model and if I am trying to

00:46:53.540 --> 00:46:57.800
train a speech model for you know a sales right, so for like

00:46:57.800 --> 00:47:00.800
bank maybe just to provide your services, there can be one

00:47:00.800 --> 00:47:03.640
model, there can be a model which I can train just for the

00:47:03.640 --> 00:47:07.680
sales and in that situation, so obviously it should aligned

00:47:07.680 --> 00:47:12.460
with my sales tone, sales speech. It should be a real human

00:47:12.460 --> 00:47:15.660
being and the way we try to pitch the product because

00:47:15.660 --> 00:47:18.840
product pitching is not going to be same from a company side

00:47:18.840 --> 00:47:21.720
and it depends upon customers as well. What kind of a

00:47:21.720 --> 00:47:24.380
customer, what kind of a TG, we generally try to say that as

00:47:24.380 --> 00:47:27.360
a TG, a target group basically. So what kind of a target

00:47:27.360 --> 00:47:31.280
group to whom you are trying to cater. So in that case

00:47:31.280 --> 00:47:34.540
obviously you have to learn the style, you have to learn the

00:47:34.540 --> 00:47:37.860
persona if you are trying to build that kind of a assistance

00:47:37.860 --> 00:47:40.640
which will be able to give a response on behalf of you.

00:47:41.260 --> 00:47:43.820
Let's suppose if I am trying to build maybe a model which

00:47:43.820 --> 00:47:47.260
can reply to my mails right, so every company's tones are

00:47:47.260 --> 00:47:51.120
going to be different. So wherever we have to adopt like a

00:47:51.120 --> 00:47:53.940
style or where we have to adopt a persona, so in that case

00:47:53.940 --> 00:47:57.420
we can go ahead with a persona or a style based fine tuning.

00:47:58.020 --> 00:48:01.380
Even let's suppose if I have to build my own clone which

00:48:01.380 --> 00:48:05.720
would teach just like me right, by like giving a multiple

00:48:05.720 --> 00:48:10.400
context, by establishing a stories, by giving an example, by

00:48:10.400 --> 00:48:13.440
talking about a same topic again and again, multiple times.

00:48:13.600 --> 00:48:17.100
So in that case it will be called as a style or persona

00:48:17.100 --> 00:48:20.500
based fine tuning. So it's applicable everywhere right, it's

00:48:20.500 --> 00:48:22.940
applicable everywhere. So wherever you have to specialise a

00:48:22.940 --> 00:48:25.720
model, so in that case you have to go with the style or

00:48:25.720 --> 00:48:29.720
persona based. And again so it's been used all across a

00:48:29.720 --> 00:48:33.300
brand specific AI assistance. So whoever and again it's a

00:48:33.300 --> 00:48:36.080
requirement for every company right, it's a requirement for

00:48:36.080 --> 00:48:40.560
every company. Now. So this is one of the like a. One of

00:48:40.560 --> 00:48:42.560
the. This is another kind of a fine tuning that you will be

00:48:42.560 --> 00:48:46.320
able to adopt. Now the seven one is basically embedding fine

00:48:46.320 --> 00:48:46.720
tuning.

00:48:51.700 --> 00:48:54.960
Embedding fine tuning.

00:48:57.030 --> 00:49:00.530
So we all are using embedding model right. So many times we

00:49:00.530 --> 00:49:03.110
have used embedding model even in my live classes, even in

00:49:03.110 --> 00:49:05.590
my generative AI or all the classes I would say. So we have

00:49:05.590 --> 00:49:09.090
been using embedding models whenever we have to convert our

00:49:09.090 --> 00:49:11.430
data set into a vector right in our numerical

00:49:11.430 --> 00:49:15.230
representation. So embedding models we are going to use. So

00:49:15.230 --> 00:49:17.570
obviously if we are using a models. Model will be having a

00:49:17.570 --> 00:49:22.750
limited knowledge or maybe like a model is old. It's been

00:49:22.750 --> 00:49:26.990
trained on the old data not on my data. So if I have to you

00:49:26.990 --> 00:49:31.230
know build a better embedding model. So again private better

00:49:31.230 --> 00:49:33.830
embedding model I would say. So in general we try to go

00:49:33.830 --> 00:49:37.050
ahead with the embedding fine tuning. So where we try to

00:49:37.050 --> 00:49:42.350
like train some of the layers and which will be able to give

00:49:42.350 --> 00:49:47.550
me embedding or vector for a particular data set. Now there

00:49:47.550 --> 00:49:50.930
is another approach that we try to follow. Which is called

00:49:50.930 --> 00:49:53.590
as reranker, which is the second layer of embedding by the

00:49:53.590 --> 00:49:58.070
way reranker. You must have heard about the Kohira reranker

00:49:58.070 --> 00:50:00.630
which is API based reranking.

00:50:03.900 --> 00:50:09.420
Re-ranker fine tuning. So basically what is the reranker. So

00:50:09.420 --> 00:50:12.300
in case of embedding as we all know, the definition. And we

00:50:12.300 --> 00:50:15.380
have seen a lot of use cases. So basically you try to you

00:50:15.380 --> 00:50:18.240
know, match a vector based on the co-science similarity or

00:50:18.240 --> 00:50:21.360
made. including distances may be based on the dot product,

00:50:21.580 --> 00:50:25.740
it will try to match two vectors means a query vector and

00:50:25.740 --> 00:50:28.560
all the vector which is available inside my vector database

00:50:28.560 --> 00:50:32.880
and then it will try to fetch a top k result. This is what

00:50:32.880 --> 00:50:37.520
embedding does. Now on top of that, just to make my entire

00:50:37.520 --> 00:50:42.400
rag a fine grain one, a fine grain one. So what we do? So we

00:50:42.400 --> 00:50:47.060
try to attach a re-ranker. What re-ranker does by the way?

00:50:47.060 --> 00:50:50.780
So re-ranker is nothing but another layer of the model I

00:50:50.780 --> 00:50:54.720
would say. So whatever top k result embedding system will

00:50:54.720 --> 00:50:58.800
try to fetch from your like millions of record, like maybe

00:50:58.800 --> 00:51:03.880
I'm trying to like fetch top 100, top 200. So what re-ranker

00:51:03.880 --> 00:51:07.860
will do is that it will try to re-rank because this

00:51:07.860 --> 00:51:10.860
embedding is going to like fetch the data based on maybe

00:51:10.860 --> 00:51:13.960
cosine similarity, based on the distances. Again this is

00:51:13.960 --> 00:51:17.040
just a double check model. So which will try to again re

00:51:17.040 --> 00:51:19.080
-rank. It will try to rearrange and give a ranking to the

00:51:19.080 --> 00:51:22.360
same data which I'm trying to fetch from my embedding with

00:51:22.360 --> 00:51:25.340
respect to my query. So this is where re-ranker comes into a

00:51:25.340 --> 00:51:28.520
picture. Like I said, just an additional approach we try to

00:51:28.520 --> 00:51:34.520
add on top of any rag application so that my rag will be a

00:51:34.520 --> 00:51:37.540
refined one. So again we are using a model inside this one

00:51:37.540 --> 00:51:41.800
and we try to fine tune it. And again Kohira is one of the

00:51:41.800 --> 00:51:46.240
platform. So where you can try to consume a re-ranker as an

00:51:46.240 --> 00:51:50.320
API. But technically you are using a model. That's it. You

00:51:50.320 --> 00:51:53.400
are using a model. So which try to give a ranks and based on

00:51:53.400 --> 00:51:57.540
the ranks it will try to produce the data. So re-ranker is

00:51:57.540 --> 00:52:03.740
one. Now the ninth one is something called as multi-model

00:52:03.740 --> 00:52:05.720
fine tuning. You must have heard about a multi-model,

00:52:13.910 --> 00:52:17.730
multi-model fine tuning. So you must have seen that

00:52:17.730 --> 00:52:21.330
basically there are some models which will be able to

00:52:21.330 --> 00:52:25.390
understand your text. And which will be able to understand

00:52:25.390 --> 00:52:29.630
your images or maybe like a text plus images or maybe audio

00:52:29.630 --> 00:52:35.710
and video. It will be able to understand. So like in many

00:52:35.710 --> 00:52:39.510
cases we are using it. So obviously that that's a model and

00:52:39.510 --> 00:52:42.350
if there is a model so we try to fine tune it by providing a

00:52:42.350 --> 00:52:45.950
input as images input as a text that okay fine for this text

00:52:45.950 --> 00:52:48.690
this image or for this image this is the text right so that

00:52:48.690 --> 00:52:51.730
it will be able to learn the relation between text based on

00:52:51.730 --> 00:52:55.990
the new data set. Or based on my own private data set.

00:52:56.550 --> 00:52:59.810
Majorly you will be able to find out a very very huge use

00:52:59.810 --> 00:53:03.250
cases in a safety industry. So let's suppose if I'm trying

00:53:03.250 --> 00:53:06.750
to build a camera so which will be able to identify the

00:53:06.750 --> 00:53:09.730
intrusion or let's suppose if I'm trying to like build

00:53:09.730 --> 00:53:12.810
something for a surveillance. So in a safety and

00:53:12.810 --> 00:53:14.870
surveillance industry you will be able to find out this

00:53:14.870 --> 00:53:17.450
multi-model and again you have to train it. You have to tune

00:53:17.450 --> 00:53:20.130
it on a different different instances because instances will

00:53:20.130 --> 00:53:22.570
keep on changing. Everyone will try to break it. They will

00:53:22.570 --> 00:53:26.330
take the system in their own way and again so you will be

00:53:26.330 --> 00:53:29.650
able to find a lot of use cases in case of a medical right

00:53:29.650 --> 00:53:33.590
because we do a lot of discoveries. We keep on like

00:53:33.590 --> 00:53:38.050
enhancing our equipments so our data collection is growing

00:53:38.050 --> 00:53:41.210
or I can say it's enhancing day by day the kind of images

00:53:41.210 --> 00:53:45.450
that we were able to take 10 years back. Now the image and

00:53:45.450 --> 00:53:48.410
scanning is on another label and again maybe in a couple of

00:53:48.410 --> 00:53:50.950
years it will be again on another label. And obviously. I

00:53:50.950 --> 00:53:55.330
need all of those information all of those data to make my

00:53:55.330 --> 00:53:57.770
medical system or whatever robot that we are building or

00:53:57.770 --> 00:54:01.410
decision making system that we are building more powerful.

00:54:01.590 --> 00:54:04.730
So obviously this multi-model fine tuning is required over

00:54:04.730 --> 00:54:06.810
there which can understand the images which can understand

00:54:06.810 --> 00:54:11.270
images video text or maybe a audio and it will be able to

00:54:11.270 --> 00:54:13.850
correlate establish a correlation in between and then it

00:54:13.850 --> 00:54:19.170
will be able to produce it right. So here these are the in

00:54:19.170 --> 00:54:21.730
general. Again. There will be. A couple of more different

00:54:21.730 --> 00:54:24.670
type of fine tuning but in general these are the type of the

00:54:24.670 --> 00:54:28.970
fine tuning that exist across the industry. Now so in your

00:54:28.970 --> 00:54:33.730
resume what you can do you can try to attach at least two or

00:54:33.730 --> 00:54:36.530
three different different kind of a fine tuning that OK I

00:54:36.530 --> 00:54:39.450
have worked in this kind of fine tuning with respect to your

00:54:39.450 --> 00:54:41.710
domain maybe you are working in a sales domain maybe into a

00:54:41.710 --> 00:54:44.390
marketing domain maybe into a finance domain maybe into a

00:54:44.390 --> 00:54:46.470
health care domain supply chain domain wherever you are

00:54:46.470 --> 00:54:49.130
working. Two three models obviously are two three

00:54:49.130 --> 00:54:52.430
approaches. You for sure you will be able to find out where

00:54:52.430 --> 00:54:55.010
you will be able to you know prove that that OK I have been

00:54:55.010 --> 00:54:59.330
using it. Yeah. And plus you can try to create your own DPR

00:54:59.330 --> 00:55:04.950
detail project report. I believe last to last week I have

00:55:04.950 --> 00:55:08.690
asked you to fine tune your resume. Anyone who has done that

00:55:08.690 --> 00:55:09.150
by the way.

00:55:18.930 --> 00:55:19.990
Yeah. Any anyone.

00:55:28.190 --> 00:55:31.930
And last week when I was talking about a project so I have

00:55:31.930 --> 00:55:36.090
just asked you to you know take a note. Take a note and then

00:55:36.090 --> 00:55:39.250
based on that try to create your own summary. So anyone who

00:55:39.250 --> 00:55:41.910
have created a summary of the project that I have discussed

00:55:41.910 --> 00:55:46.310
last Monday Tuesday I have not taken the class. But yeah I'm

00:55:46.310 --> 00:55:49.370
talking about a last Monday by the way. So anyone who has

00:55:49.370 --> 00:55:49.730
done that.

00:56:06.260 --> 00:56:11.200
OK so waiting for an answer by the way well

00:57:01.510 --> 00:57:05.590
this is a student progress. OK fine. OK. Right. So basically

00:57:05.590 --> 00:57:09.730
see guys for a long term and even for a short term it's

00:57:09.730 --> 00:57:12.350
advisable for all of you. To create your own project report.

00:57:12.690 --> 00:57:15.290
Right. Let's suppose if you are mentioning like three to

00:57:15.290 --> 00:57:17.910
four project always always make sure that complete detail

00:57:17.910 --> 00:57:20.650
project report is starting from the use cases client

00:57:20.650 --> 00:57:25.210
requirement data collection strategy maybe a team like size

00:57:25.210 --> 00:57:28.070
of the team then time taken. What was your roles and

00:57:28.070 --> 00:57:31.210
responsibility. Always try to keep it handy and write it by

00:57:31.210 --> 00:57:34.410
yourself. Don't like you know take a help of chat GPT

00:57:34.410 --> 00:57:37.990
obviously. Right. But try to write it once in your lifetime

00:57:37.990 --> 00:57:41.370
I believe so that you know every time whenever you go for

00:57:41.370 --> 00:57:45.570
the interview. Right. You have to reiterate a same version.

00:57:45.830 --> 00:57:48.710
Right. Because obviously interviewer are going to ask you a

00:57:48.710 --> 00:57:51.430
very first question that tell me about your project and your

00:57:51.430 --> 00:57:54.310
story should start from your DPR a detailed project report

00:57:54.310 --> 00:57:58.590
and just write down everything in such a way that nothing

00:57:58.590 --> 00:58:00.850
can be asked beyond that. Like like I have already discussed

00:58:00.850 --> 00:58:03.850
my previous class last Monday. Right. That try to keep

00:58:03.850 --> 00:58:08.310
everything into a single one into a single DPR project

00:58:08.310 --> 00:58:10.530
report and let's suppose if I mentioned three project then I

00:58:10.530 --> 00:58:13.630
should have a three. Right. And believe me it will help you

00:58:13.630 --> 00:58:16.830
out even in the future. Right. Not just like this particular

00:58:16.830 --> 00:58:20.570
switch or maybe an opportunity but even the future

00:58:20.570 --> 00:58:23.270
opportunity that you all will be able to get it is going to

00:58:23.270 --> 00:58:31.270
help you out. OK. So now moving ahead. OK. Great. LinkedIn

00:58:31.270 --> 00:58:33.570
post I believe we all should do it on a regular basis. I

00:58:33.570 --> 00:58:38.470
don't know why people are not doing it. Yeah. Don't tag me

00:58:38.470 --> 00:58:43.030
but yeah for you. Like. Start like you know doing a LinkedIn

00:58:43.030 --> 00:58:45.970
post start building a network because this is something that

00:58:45.970 --> 00:58:49.170
you will not be able to build it on a single day and you

00:58:49.170 --> 00:58:52.390
can't even buy it by the way. You have to do it on a regular

00:58:52.390 --> 00:58:56.370
basis and then only you will be able to like achieve these

00:58:56.370 --> 00:59:00.690
these things a network a solid network which will be able to

00:59:00.690 --> 00:59:04.090
help you out in a short term or in a long term.

00:59:08.780 --> 00:59:14.200
OK. So now let me talk about like a little bit of data type.

00:59:14.360 --> 00:59:17.780
So what kind of a data in general we try to prepare for some

00:59:17.780 --> 00:59:22.640
of these fine tuning approach. Yeah. So let's move ahead and

00:59:22.640 --> 00:59:25.900
let's talk about the data type in general we are going to

00:59:25.900 --> 00:59:28.560
use. And again I will show you my practical data set as well

00:59:28.560 --> 00:59:30.880
because I have already like done this fine tuning. So

00:59:30.880 --> 00:59:33.680
obviously I have a data set for that one. So I have for I

00:59:33.680 --> 00:59:36.040
have a data set for Laura. I have a data set for Cora. I

00:59:36.040 --> 00:59:39.880
have a data set for embeddings. And I have a data set for

00:59:39.880 --> 00:59:43.880
basically re-ranking because these four things I have

00:59:43.880 --> 00:59:46.840
already built it. And beyond that if you would like to

00:59:46.840 --> 00:59:49.580
explore so I'll even tell you like a way where you can go

00:59:49.580 --> 00:59:51.680
and you can try to explore all the other approaches and the

00:59:51.680 --> 00:59:57.980
data type in general we try to prepare. So here if I'll talk

00:59:57.980 --> 01:00:02.700
about a fine tuning like in general. So what is or what kind

01:00:02.700 --> 01:00:06.560
of a data type data set we prepare because this is one of

01:00:06.560 --> 01:00:11.980
the most I would say a hectic job. We can say a big job

01:00:11.980 --> 01:00:15.960
which big which consume a big real resources in terms of

01:00:15.960 --> 01:00:18.960
human in terms of writing a logic and everything and in

01:00:18.960 --> 01:00:21.420
terms of building a pipeline because when we are talking

01:00:21.420 --> 01:00:23.940
about data so obviously we are talking about the huge amount

01:00:23.940 --> 01:00:27.560
of the data 10 20 million kind of a token we are talking

01:00:27.560 --> 01:00:31.460
about. So here when we talk about a data preparation. So

01:00:31.460 --> 01:00:34.920
what is a kind of a format in general. Again these formats

01:00:34.920 --> 01:00:37.400
will not be applicable all the time. But yeah. In general

01:00:37.400 --> 01:00:40.280
most of the time in 90 percent cases you will be able to

01:00:40.280 --> 01:00:41.540
find out these formats only.

01:00:44.400 --> 01:00:51.650
Data format or fine tuning.

01:00:56.990 --> 01:01:02.730
Now so so the very first type of the data which generally we

01:01:02.730 --> 01:01:05.050
try to use it is called as pairwise format.

01:01:09.670 --> 01:01:14.730
Pairwise format. Yeah. Most common type of the data set that

01:01:14.730 --> 01:01:19.370
we try to use. Like use it in general. So what how we

01:01:19.370 --> 01:01:23.130
prepare this data or what will be there inside this data. So

01:01:23.130 --> 01:01:26.150
inside this data you can say there will be a key value pair.

01:01:26.550 --> 01:01:32.450
So let's suppose my key is basically query. Right. Let's

01:01:32.450 --> 01:01:35.010
suppose I'm writing what

01:01:36.800 --> 01:01:43.050
is your own right. So let's suppose this is my query. Then

01:01:43.050 --> 01:01:46.210
there will be a positive positive

01:01:47.940 --> 01:01:55.020
passage. As a value. And then there will be a negative

01:01:55.020 --> 01:01:55.500
passage.

01:01:58.990 --> 01:02:04.370
Negative passage you will be able to find out. So positive

01:02:04.370 --> 01:02:07.590
means. So let's suppose I'm writing a relevant one. Right.

01:02:07.610 --> 01:02:09.570
So with respect to this question. So there will be a

01:02:09.570 --> 01:02:12.090
relevant answer. So I'm writing a relevant answer such that

01:02:12.090 --> 01:02:15.850
that Euron is a

01:02:22.410 --> 01:02:27.090
BLE affordable upskilling

01:02:34.020 --> 01:02:37.000
platform. Right. So this is my positive passage. Let's

01:02:37.000 --> 01:02:39.200
suppose what could be a negative passage. So maybe something

01:02:39.200 --> 01:02:42.780
which is not even relevant to this one. Right. So basically

01:02:42.780 --> 01:02:48.480
I can I can try to write over here that Amazon is amazing.

01:02:50.680 --> 01:02:52.640
Amazon is amazing.

01:02:54.990 --> 01:02:57.890
Right. So let's suppose I'm writing this one. Amazon is

01:02:57.890 --> 01:03:00.810
amazing. So it will try to learn from a positive as well as

01:03:00.810 --> 01:03:02.970
it will try to learn from the negative that. Okay fine. I

01:03:02.970 --> 01:03:06.170
should do this and I should not do this one. And likewise

01:03:06.170 --> 01:03:09.130
let's suppose we have a millions of data. Right. We have a

01:03:09.130 --> 01:03:12.230
millions of data. 20 million 30 million record if we have a

01:03:12.230 --> 01:03:15.210
billions of record it will if we have. So obviously

01:03:15.210 --> 01:03:17.950
everywhere there will be a negative pattern and everywhere

01:03:17.950 --> 01:03:20.410
there will be a positive pattern positive passage or

01:03:20.410 --> 01:03:23.210
negative passage with respect to a question it will be able

01:03:23.210 --> 01:03:25.910
to learn with the accuracy and if we are preparing our data

01:03:25.910 --> 01:03:29.250
set with this in this manner. So it's called as pairwise

01:03:29.250 --> 01:03:32.770
format. So it's not like let's suppose if I'm reading a book

01:03:32.770 --> 01:03:35.710
so I'll try to provide the entire books to the system and

01:03:35.710 --> 01:03:38.930
then I'll say okay fine like learn it. No. First of all we

01:03:38.930 --> 01:03:42.850
have to prepare the data. Let's suppose I'm trying to scrap

01:03:42.850 --> 01:03:48.250
a website or maybe like a any any like a book library. Let's

01:03:48.250 --> 01:03:50.990
suppose I have a thousands of book libraries thousands of

01:03:50.990 --> 01:03:53.230
book in time inside my libraries and in every book there are

01:03:53.230 --> 01:03:56.710
thousands of pages and every pages there are maybe 200 words

01:03:56.710 --> 01:04:00.590
or 2000 words depends right size of the books. So it's not

01:04:00.590 --> 01:04:03.390
like I will just take those books or scrap those books and

01:04:03.390 --> 01:04:08.510
then send those books to the model. No. We are supposed to

01:04:08.510 --> 01:04:14.090
do a data preparation first positive negative. So lot of

01:04:14.090 --> 01:04:19.690
human labelers are required. It needs a lot of effort. It

01:04:19.690 --> 01:04:23.090
needs a lot of understanding and just imagine like just

01:04:23.090 --> 01:04:26.310
imagine I'm talking about just English in our entire world.

01:04:26.410 --> 01:04:29.330
There are thousands of languages which exist that that's the

01:04:29.330 --> 01:04:31.450
reason right. That's the reason so many models are not good

01:04:31.450 --> 01:04:34.050
at all good I would say with respect to the regional

01:04:34.050 --> 01:04:37.130
languages. And there are many companies who are just working

01:04:37.130 --> 01:04:40.490
on that. So see model training everyone knows right. If I

01:04:40.490 --> 01:04:43.190
see at the end of the day machine understand 0 1 0 1 0 1

01:04:43.190 --> 01:04:45.750
that's it right. So machine don't understand any other

01:04:45.750 --> 01:04:49.470
languages except 0 1 0 1 just understand binary. We as a

01:04:49.470 --> 01:04:52.910
human have to understand those languages. We as a human

01:04:52.910 --> 01:04:56.410
being has to interpret that and then convert that into a 0 1

01:04:56.410 --> 01:04:58.670
and I have to give to the machine. Machine will just learn a

01:04:58.670 --> 01:05:02.370
pattern between 0 1 at the end of the day right. So this is

01:05:02.370 --> 01:05:04.650
the reason like this is one of the. Again very very huge

01:05:04.650 --> 01:05:08.830
task even image labeling is a very huge task in a like many

01:05:08.830 --> 01:05:12.030
companies right. Many companies which I know and I know them

01:05:12.030 --> 01:05:16.630
like since last like seven eight year I would say right. And

01:05:16.630 --> 01:05:19.690
they just do one thing they do image labeling they just do a

01:05:19.690 --> 01:05:23.190
data labeling all this like NLP labeling. So there are so

01:05:23.190 --> 01:05:25.170
many companies you will be able to find out in the world. So

01:05:25.170 --> 01:05:28.330
where there are like hundreds of employees and they just do

01:05:28.330 --> 01:05:31.410
one thing they just label the data right. It's a huge

01:05:31.410 --> 01:05:35.190
industry for AI. If you felt. Huge I can say a derived

01:05:35.190 --> 01:05:38.430
industry for AI because you can't use your data set

01:05:38.430 --> 01:05:42.630
directly. You have to label you have to vet those data. You

01:05:42.630 --> 01:05:45.490
have to validate those data. There will be a human

01:05:45.490 --> 01:05:47.990
intervention a lot of human intervention because you can't

01:05:47.990 --> 01:05:51.310
just write a code break the data into a chunks and then send

01:05:51.310 --> 01:05:53.990
it to a model. No. How model will be able to understand that

01:05:53.990 --> 01:05:57.230
what is the question what should be the answer. So just

01:05:57.230 --> 01:06:00.930
imagine how much of data has been labeled because at the end

01:06:00.930 --> 01:06:03.650
of the day sending those data for a training. What should be

01:06:03.650 --> 01:06:07.070
the answer. I will need maybe like one script which I will

01:06:07.070 --> 01:06:10.710
write for once my entire team will write for once and then

01:06:10.710 --> 01:06:15.010
infrastructure a big one obviously. Right. I'll send for the

01:06:15.010 --> 01:06:19.330
training. So where where the effort actually what is

01:06:19.330 --> 01:06:21.550
required so actually what is required in terms of building a

01:06:21.550 --> 01:06:25.390
data pipeline right. And there are like a lot of approaches

01:06:25.390 --> 01:06:29.670
we try to follow. And again ultimately I will be looking for

01:06:29.670 --> 01:06:33.290
data in this particular format. So this is something called.

01:06:33.290 --> 01:06:36.990
As pairwise format one of the data format which we can try

01:06:36.990 --> 01:06:41.890
to adopt for certain tasks not for all the tasks. So this is

01:06:41.890 --> 01:06:45.390
not universal format I'm talking about by the way. There is

01:06:45.390 --> 01:06:49.390
something called as list wise format because

01:06:51.020 --> 01:06:53.860
at the end of the day my model is just a mathematics right.

01:06:54.000 --> 01:06:58.380
I have to set the input x1 x2 x3 x4. So that will go as a

01:06:58.380 --> 01:07:03.740
input layer. So I have to arrange the data. It's not like I

01:07:03.740 --> 01:07:06.480
will just send any kind of data. Any kind of a scrap and my

01:07:06.480 --> 01:07:08.860
model will be able to learn any kind of things. No not going

01:07:08.860 --> 01:07:11.800
to happen in that way. So there is something called as list

01:07:11.800 --> 01:07:16.240
wise. So this is also a huge task which takes not just one

01:07:16.240 --> 01:07:18.600
or two days. It takes a month and month or month of time.

01:07:22.410 --> 01:07:26.970
List wise format. Now what it does. So again it will help

01:07:26.970 --> 01:07:29.810
you out. It's a data you know preparation technique. So

01:07:29.810 --> 01:07:33.570
where you will be having a query right. You will be having a

01:07:33.570 --> 01:07:38.410
query as a key. And let's suppose again I'm writing. Who is

01:07:38.410 --> 01:07:39.910
Sudhanshu

01:07:43.460 --> 01:07:47.100
right. Let's suppose I have written. So this is my query.

01:07:47.220 --> 01:07:51.100
Now system is supposed to give the answer. So here like as

01:07:51.100 --> 01:07:53.480
it suggests it's a list wise right. So there will be a

01:07:53.480 --> 01:07:56.400
multiple like data inside one single list. This is what it

01:07:56.400 --> 01:07:59.580
means. As simple as that. So here the candidate for this

01:07:59.580 --> 01:08:02.460
particular query like candidate, candidate

01:08:05.150 --> 01:08:08.630
I can try to keep it inside the list. There can be 3, 4, 5,

01:08:08.730 --> 01:08:12.150
8, 10 whatever it is right. So again inside that. This

01:08:12.150 --> 01:08:16.410
candidate. There will be text right. So text which is

01:08:16.410 --> 01:08:19.710
eventually the answer of this particular query right.

01:08:19.790 --> 01:08:24.190
Labeled by a human again. Not auto labeling. Labeled by a

01:08:24.190 --> 01:08:29.230
human by the way. So here who is Sudhanshu. So Sudhanshu is

01:08:31.530 --> 01:08:35.130
a teacher.

01:08:39.330 --> 01:08:44.230
Let's suppose right. So we have a text and then for this

01:08:44.230 --> 01:08:49.390
text human are going to label it right. Label it. That okay

01:08:49.390 --> 01:08:52.190
this answer is correct. So if this answer is correct let me

01:08:52.190 --> 01:08:56.290
give a label is equals to 1. Hypothetically like for example

01:08:56.290 --> 01:09:00.490
I have given a label is equals to 1. So now this is one of

01:09:00.490 --> 01:09:05.150
the sorry. This is one of the record. Now another record.

01:09:06.110 --> 01:09:08.510
It's a list wise pair right. So this is one of the record

01:09:08.510 --> 01:09:11.630
inside this particular list. I can maintain a multiple

01:09:11.630 --> 01:09:17.930
records over here. So the another record is. Again. Text and

01:09:17.930 --> 01:09:23.050
answer to this question may be against this question. So who

01:09:23.050 --> 01:09:23.550
is Sudhanshu.

01:09:27.950 --> 01:09:30.970
So Akash is student

01:09:33.690 --> 01:09:39.530
right. And then I am going to maintain label. Now label is

01:09:39.530 --> 01:09:43.370
equal to 0 basically because this is not the relevant answer

01:09:43.370 --> 01:09:46.210
with respect to my question right. So it will try to learn

01:09:46.210 --> 01:09:48.810
from non relevancy as well not just from the relevancy. So

01:09:48.810 --> 01:09:52.010
that it will learn what to avoid. Then again. I can try to

01:09:52.010 --> 01:09:57.800
maintain another variance of the text inside my list. Text

01:09:57.800 --> 01:10:00.560
over here which is relevant which may be not relevant

01:10:00.560 --> 01:10:03.880
depends right. So h

01:10:07.440 --> 01:10:15.800
a n s h u Sudhanshu used to teach data let's suppose right.

01:10:16.140 --> 01:10:22.080
So here label is equals to 1 I am maintaining. So this is

01:10:22.080 --> 01:10:25.160
called as list wise format. So we are in a list. We are

01:10:25.160 --> 01:10:27.540
going to provide the data. You can take a note of these

01:10:27.540 --> 01:10:32.120
things guys. My writing is not that great. So I tried a lot

01:10:32.120 --> 01:10:35.380
to improvise it since childhood. But yeah. So here my

01:10:35.380 --> 01:10:40.840
teacher used to give me like a task to write one page daily.

01:10:40.980 --> 01:10:46.680
But again. So I think it's a childhood habit because of that

01:10:46.680 --> 01:10:49.520
I am not able to improvise my handwriting. If you are not

01:10:49.520 --> 01:10:51.860
comfortable I can start typing it. I am good with typing by

01:10:51.860 --> 01:10:55.760
the way. Good with the keyboard. But yeah. Not that great

01:10:55.760 --> 01:11:01.520
with the handwriting. And again as a person who uses a

01:11:01.520 --> 01:11:05.820
keyboard right. So it's very tough to write. So hope all of

01:11:05.820 --> 01:11:06.660
you are able to understand.

01:11:10.260 --> 01:11:14.060
Hand is not coincide with the mind as fast. No that is not a

01:11:14.060 --> 01:11:17.340
reason. Handwriting is actually bad. That is the actual

01:11:17.340 --> 01:11:22.540
reason. So no excuses at all for me. Okay so this is called

01:11:22.540 --> 01:11:25.880
as list wise format. So in which we are going to prepare the

01:11:25.880 --> 01:11:27.560
data set. This is called list wise format. This is again one

01:11:27.560 --> 01:11:31.640
of the way by which you will be able to prepare the data set

01:11:31.640 --> 01:11:34.620
in some of the training approaches or fine tuning

01:11:34.620 --> 01:11:37.360
approaches. Right. Training fine tuning both are same

01:11:37.360 --> 01:11:40.300
because data format is going to be same. Again if you are

01:11:40.300 --> 01:11:42.560
doing a training from the scratch or you are going out with

01:11:42.560 --> 01:11:44.320
the fine tuning. It doesn't matter at all. Data preparation

01:11:44.320 --> 01:11:50.400
strategy is going to be same by the way. Then like in

01:11:50.400 --> 01:11:54.720
general hugging face. Right. Hugging face has defined its

01:11:54.720 --> 01:12:01.100
own format you can say. So let's suppose that's called as

01:12:01.100 --> 01:12:02.820
ranking data

01:12:06.280 --> 01:12:13.280
set with hugging face. Yeah with hugging face. So in TSB or

01:12:13.280 --> 01:12:16.260
CSB style we try to prepare this data. This data we try to

01:12:16.260 --> 01:12:18.900
prepare it in JSON format or JSONL format. So there are like

01:12:18.900 --> 01:12:22.600
JSONL format generally you will be able to find out these

01:12:22.600 --> 01:12:26.200
formats inside open AI models. So whenever you will try to

01:12:26.200 --> 01:12:28.980
use open AI format model. So you have to go ahead with that

01:12:28.980 --> 01:12:32.560
one. If you would like to see a practical like how we do a

01:12:32.560 --> 01:12:37.200
fine tuning. So maybe you can even watch my generative AI

01:12:37.200 --> 01:12:41.400
with NLP and fine tuning lectures which I have given as a

01:12:41.400 --> 01:12:44.480
live classes over there even in a practical manner with

01:12:44.480 --> 01:12:48.020
respect to open AI platform with respect to hugging face I

01:12:48.020 --> 01:12:50.220
have already shown the fine tuning and most of the fine

01:12:50.220 --> 01:12:53.440
tuning approach that I have discussed right above which I

01:12:53.440 --> 01:12:56.560
have listed down mostly I have not discussed 8 line but yeah

01:12:56.560 --> 01:12:58.880
I have already discussed I believe like five or six over

01:12:58.880 --> 01:13:03.260
there. Practical manner. And again so more in depth in terms

01:13:03.260 --> 01:13:07.580
of mathematics I have discussed over there. So if you have a

01:13:07.580 --> 01:13:10.620
time and if you have an interest maybe you can watch those

01:13:10.620 --> 01:13:14.500
lectures. You will have to spend like three four hours for

01:13:14.500 --> 01:13:21.380
that. So ranking data set with hugging faces. So again this

01:13:21.380 --> 01:13:24.680
is just one of the format. So where we try to maintain a

01:13:24.680 --> 01:13:31.310
query and then we try to give a tab, tab means three spaces

01:13:31.310 --> 01:13:34.570
by the way. Then we try to maintain a document,

01:13:37.900 --> 01:13:42.240
maintain a document. Then again three space you can say tab

01:13:42.240 --> 01:13:48.100
and then we try to maintain a label yeah. So for example

01:13:48.100 --> 01:13:57.050
query who is Sudhanshu, three space or you can say tab. Then

01:13:57.050 --> 01:14:03.250
there will be like a you know a document right. So Sudhanshu

01:14:05.910 --> 01:14:09.890
is teacher. Then again three space. Then again three space

01:14:09.890 --> 01:14:13.790
you can say tab and then maybe a label. One zero whatever

01:14:13.790 --> 01:14:17.450
you want or whatever is your labeling strategies. So this is

01:14:17.450 --> 01:14:20.170
called as ranking data set with hugging face. So generally

01:14:20.170 --> 01:14:23.990
hugging face requires this kind of approach which is more

01:14:23.990 --> 01:14:27.830
like similar to like the previous one. You know in a maybe

01:14:27.830 --> 01:14:30.970
if I am trying to maintain these things in a CSV comma

01:14:30.970 --> 01:14:34.590
separated values. So separation by comma so we can try to

01:14:34.590 --> 01:14:39.050
prepare this one right. So again it's not a big deal to find

01:14:39.050 --> 01:14:40.210
out what kind of data set we are using. What kind of you

01:14:40.210 --> 01:14:43.810
know data set format that we have to like send. Once my data

01:14:43.810 --> 01:14:46.970
set is ready let's suppose my data set is ready even in this

01:14:46.970 --> 01:14:51.390
format or in this format right. In this format for some of

01:14:51.390 --> 01:14:54.810
the tasks I can convert one to another that that's like a

01:14:54.810 --> 01:14:57.990
very calm. I can do it with the help of code itself that's

01:14:57.990 --> 01:15:02.450
possible yeah that's that is something which is a possible.

01:15:02.770 --> 01:15:04.930
So there are multiple different different kind of approaches

01:15:04.930 --> 01:15:08.290
you will be able to find out with respect to a data set.

01:15:08.370 --> 01:15:10.830
Again based on the model. You can even search so you can

01:15:10.830 --> 01:15:13.930
write a model name I can say that okay what should be the

01:15:13.930 --> 01:15:18.350
like an input data set format and you will be able to find

01:15:18.350 --> 01:15:20.870
out in its documentation that okay I am taking this kind of

01:15:20.870 --> 01:15:23.750
an input format. So you have to give always input in that

01:15:23.750 --> 01:15:27.390
particular format otherwise it will crash. Your entire code

01:15:27.390 --> 01:15:30.550
base is going to crash if like a data set is not available

01:15:30.550 --> 01:15:33.750
in a particular format which model is going to accept yeah

01:15:33.750 --> 01:15:37.670
model is going to take by the way. So you should you should

01:15:37.670 --> 01:15:42.810
be making that compatible. Is it fine guys everyone. Which

01:15:42.810 --> 01:15:46.010
class sir? So it was basically if you will go and search in

01:15:46.010 --> 01:15:51.330
your own generative AI with I can show you that batch maybe.

01:15:55.440 --> 01:16:00.660
So it was generative

01:16:02.780 --> 01:16:05.800
AI with NLP NLP

01:16:09.220 --> 01:16:12.200
and fine tuning. That was a batch name. Yeah this one this

01:16:12.200 --> 01:16:15.680
one this was also a live class which I have started on March

01:16:15.680 --> 01:16:20.380
and like a company. So I have completed it. Okay let me log

01:16:20.380 --> 01:16:21.700
in this

01:16:34.310 --> 01:16:40.150
one. So here you will be able to find out yeah June live

01:16:40.150 --> 01:16:42.850
fine tuning. So here if you watch this lecture I think it's

01:16:42.850 --> 01:16:49.830
a two hour three hour class basically. So it was basically a

01:16:49.830 --> 01:16:53.290
three hour class. So where you will be able to see fine

01:16:53.290 --> 01:16:55.910
tuning LoRa base fine tuning, Quora based fine tuning, fine

01:16:55.910 --> 01:17:00.930
tuning with the help of like open AI by the way. Right. So

01:17:00.930 --> 01:17:03.450
fine tuning with the help of open AI and many many more

01:17:03.450 --> 01:17:07.390
thing you will be able to find out there. Everything with

01:17:07.390 --> 01:17:09.930
respect to fine tuning by the way you will be able to find

01:17:09.930 --> 01:17:10.690
out. And

01:18:40.830 --> 01:18:43.510
if you would like to know more about the data set with

01:18:43.510 --> 01:18:47.090
respect to the model. So like I told you right. So for every

01:18:47.090 --> 01:18:50.910
model for example if I'm talking about these kind of a fine

01:18:50.910 --> 01:18:55.510
tuning supervised fine tuning SFT vision fine tuning DPO

01:18:55.510 --> 01:18:59.270
which I talked about reinforcement fine tuning. Now. So even

01:18:59.270 --> 01:19:02.670
in open AI so they have given you like this one so where you

01:19:02.670 --> 01:19:06.010
can go and you can try to like practice these things. And if

01:19:06.010 --> 01:19:10.050
I would like to know that how we are supposed to prepare a

01:19:10.050 --> 01:19:12.670
data set for supervised fine tuning and only these models

01:19:12.670 --> 01:19:16.050
you will be able to fine tune. So here you will be able to

01:19:16.050 --> 01:19:19.430
see the data set type that in whenever you are preparing the

01:19:19.430 --> 01:19:21.970
data you are supposed to prepare a data set in this

01:19:21.970 --> 01:19:25.170
particular format itself. If you are not preparing the data

01:19:25.170 --> 01:19:27.970
set in this format and if you are not uploading those data

01:19:27.970 --> 01:19:31.030
set into a JSON L format you will not be able to do the fine

01:19:31.030 --> 01:19:34.130
tuning. Not just that. So basically if you are going ahead

01:19:34.130 --> 01:19:37.650
with the open AI based fine tuning. So inside a dashboard

01:19:37.650 --> 01:19:40.390
itself. So let me show you that dashboard which I have

01:19:40.390 --> 01:19:44.630
already shown in a practical manner. And even I have like a

01:19:44.630 --> 01:19:47.950
fine tuned couple of model in my live classes I have shown

01:19:47.950 --> 01:19:50.670
them like how fine tuning actually works the live classes

01:19:50.670 --> 01:19:53.430
which I was talking about right. So here I have shown this

01:19:53.430 --> 01:19:55.810
example plus I have shown this example in a local system

01:19:55.810 --> 01:19:59.130
plus in a hugging phase. All three places I have shown this

01:19:59.130 --> 01:20:02.190
particular example in that particular class. If you would

01:20:02.190 --> 01:20:04.870
like to learn like how fine tuning actually happens. So

01:20:04.870 --> 01:20:09.550
basically like here in case of open AI they have already

01:20:09.550 --> 01:20:13.290
given you a portal and through a portal itself you will be

01:20:13.290 --> 01:20:17.030
able to you know do supervised fine tuning a DPO fine tuning

01:20:17.030 --> 01:20:19.910
a reinforcement fine tuning. So again inside all these

01:20:19.910 --> 01:20:23.410
categories if I talk about supervised these are my models by

01:20:23.410 --> 01:20:25.890
the way these are my fine tuned models which I have created

01:20:25.890 --> 01:20:29.190
this big names that you are able to. See the small name that

01:20:29.190 --> 01:20:31.370
you are able to see. So yeah these are the models given by

01:20:31.370 --> 01:20:34.210
open AI right. So these are the models that you will be able

01:20:34.210 --> 01:20:37.070
to fine tune. Even I can find you in my own models

01:20:37.070 --> 01:20:39.230
everywhere you can see engaged by technology right which is

01:20:39.230 --> 01:20:42.750
like a parent of your own. So these are the custom fine

01:20:42.750 --> 01:20:47.370
tuned model on 3.5 3.5 4.1 nano and then turbo I have

01:20:47.370 --> 01:20:52.270
created and this is basically a fine tuning that basically

01:20:52.270 --> 01:20:56.850
like done by open AI and then in open AI obviously they're

01:20:56.850 --> 01:20:59.350
going to charge you for the fine tuning. A small amount not

01:20:59.350 --> 01:21:02.530
a very big one again depends upon the data set that you are

01:21:02.530 --> 01:21:05.870
going to opt for it is going to charge because they charge

01:21:05.870 --> 01:21:09.310
per token once fine tuning will be done. So directly from

01:21:09.310 --> 01:21:12.610
the open AI portal itself you will be able to consume it

01:21:12.610 --> 01:21:16.050
just like any other APIs right just like any other APIs but

01:21:16.050 --> 01:21:19.410
again so there will be no security if you trust open AI then

01:21:19.410 --> 01:21:22.270
only you can use it if you don't trust it you can't use it

01:21:22.270 --> 01:21:26.430
very simple right if you don't trust it you can't use it but

01:21:26.430 --> 01:21:28.690
yeah they have given you everything in a UI interface itself

01:21:28.690 --> 01:21:31.650
so here you just have to do only one thing your data

01:21:31.650 --> 01:21:34.870
preparation if your data set is not prepared then again open

01:21:34.870 --> 01:21:38.510
AI can't help you in that way right. So for every model so

01:21:38.510 --> 01:21:41.050
they have already given you like how you are supposed to do

01:21:41.050 --> 01:21:44.370
a data preparation sample is already given to you right

01:21:44.370 --> 01:21:47.410
sample is already given to you everywhere for every model

01:21:47.410 --> 01:21:50.970
you will be able to find out that by default yeah by default

01:21:50.970 --> 01:21:53.850
you will be able to find out that particular thing so you

01:21:53.850 --> 01:21:56.690
can you can try this out maybe like once or twice for a

01:21:56.690 --> 01:21:59.590
learning purposes. Maybe they are giving you $5 of credit

01:21:59.590 --> 01:22:04.270
I'm not sure now previously they were giving but yeah as of

01:22:04.270 --> 01:22:07.390
now I'm not sure about it about the free credit but yeah you

01:22:07.390 --> 01:22:10.510
can you can try this out and you can try this out even a

01:22:10.510 --> 01:22:13.030
hugging phase right hugging phase space where you can try to

01:22:13.030 --> 01:22:18.510
do the fine tuning over there again just just watch this one

01:22:18.510 --> 01:22:23.310
this lecture which I'm talking about right the 8th June

01:22:23.310 --> 01:22:29.170
lecture where you will be able to see the fine tuning. In

01:22:29.170 --> 01:22:32.890
practical if someone is not aware about it because if you

01:22:32.890 --> 01:22:35.530
are going for the interview it's a strong recommendation to

01:22:35.530 --> 01:22:40.470
all of you that practically you should do at least once if

01:22:40.470 --> 01:22:42.990
you have not done that and before writing something into

01:22:42.990 --> 01:22:45.950
your resume that I have done this you should at least once

01:22:45.950 --> 01:22:49.010
experience it so that you can elaborate and then you can you

01:22:49.010 --> 01:22:56.060
will be able to build the story around it okay. So these are

01:22:56.060 --> 01:22:58.700
the data set by the way that you all will be able to

01:22:58.700 --> 01:23:04.940
prepare. Now so coming to now a holistic approach end to end

01:23:04.940 --> 01:23:08.800
approach that we try to follow coming to the costing part

01:23:08.800 --> 01:23:14.040
timelines basically so what will be the timelines and GPU

01:23:14.040 --> 01:23:19.260
hours so how many GPU hours is required so what is a formula

01:23:19.260 --> 01:23:23.560
of doing that GPU hours calculation because many people many

01:23:23.560 --> 01:23:28.720
companies many industry many teams is having a worry about a

01:23:28.720 --> 01:23:33.500
GPU hours that what is a GPU hours this model is going to

01:23:33.500 --> 01:23:37.060
need or what is the GPU hours that model is going to need.

01:23:37.420 --> 01:23:41.780
Now keeping that in a mind right let's talk about a GPU

01:23:41.780 --> 01:23:45.100
hours it's a simple mathematical calculation by which you

01:23:45.100 --> 01:23:50.040
will be able to do it. So here let me write down the GPU

01:23:50.040 --> 01:23:53.000
hours calculation it's a very small formula anyone who is

01:23:53.000 --> 01:23:57.040
aware about it guys GPU hours rough estimation formulas

01:23:57.040 --> 01:23:58.200
yeah. Yes

01:24:08.240 --> 01:24:12.440
so basically a GPU hours is

01:24:15.770 --> 01:24:18.390
a calculation approximately again this is just approximation

01:24:18.390 --> 01:24:22.450
formula it's not a hard and fast formula. So it's equal to

01:24:22.450 --> 01:24:27.550
total training token total

01:24:31.900 --> 01:24:33.980
training token divided

01:24:36.610 --> 01:24:38.370
by tokens

01:24:41.020 --> 01:24:45.680
per GPU

01:24:47.120 --> 01:24:54.100
into number of you can say has means number of GPU hours.

01:25:01.480 --> 01:25:08.760
So this is basically a GPU hours so how many GPU hours is

01:25:08.760 --> 01:25:11.800
required for any kind of a model training so it depends upon

01:25:11.800 --> 01:25:14.600
the total training token means total input token that we are

01:25:14.600 --> 01:25:18.520
going to give and then how many tokens we are going to fit

01:25:18.520 --> 01:25:23.220
into one GPU in per second right. So basically we try to

01:25:23.220 --> 01:25:27.500
like a. Talk about a batch we try to talk about basically a

01:25:27.500 --> 01:25:30.980
step size all those things right. So obviously tokens per

01:25:30.980 --> 01:25:33.820
GPU that you will be able to fit you all will be able to

01:25:33.820 --> 01:25:36.500
find out but this is just a rough estimation again network

01:25:36.500 --> 01:25:40.180
latency matters a lot read write matters a lot. This is just

01:25:40.180 --> 01:25:43.440
a roughly estimation that you will be able to find out in

01:25:43.440 --> 01:25:46.300
general with respect to GPU hours. Okay fine so if I'm

01:25:46.300 --> 01:25:48.520
training this much of token and this much of token I can fit

01:25:48.520 --> 01:25:52.460
into one GPU and my total model is like having this many

01:25:52.460 --> 01:25:55.780
number of tokens. And I have this many number of total GPU

01:25:55.780 --> 01:26:00.980
then approximately these things like so approximately these

01:26:00.980 --> 01:26:04.720
many number of hours will be required right approximately

01:26:04.720 --> 01:26:07.520
these many number of hours will be required. So token per

01:26:07.520 --> 01:26:11.340
second GPU will be able to process because if model size is

01:26:11.340 --> 01:26:16.860
very very big so again latency will be high. So token per

01:26:16.860 --> 01:26:20.240
second GPU will be able to per tokens per GPU it will be

01:26:20.240 --> 01:26:24.120
able to process per second or per hours basically. So this

01:26:24.120 --> 01:26:28.980
is just a rough estimation about the GPU hours. Now so in

01:26:28.980 --> 01:26:34.620
general right so in general what kind of like GPUs we are

01:26:34.620 --> 01:26:37.600
going to use so people are going to ask you in an interview

01:26:37.600 --> 01:26:41.280
that okay so which GPU you have used by the way. So you can

01:26:41.280 --> 01:26:46.120
say that that I have been using A100 or H100 series simple

01:26:46.120 --> 01:26:52.140
A100 or H100 I have been using with respect to my model

01:26:52.140 --> 01:26:58.080
training. Now if I will talk about A100 right A100 in

01:26:58.080 --> 01:27:02.480
general it will be able to process 2 to 4k thousand k means

01:27:02.480 --> 01:27:07.540
thousand by the way. So tokens per second tokens per second.

01:27:07.640 --> 01:27:11.280
So this is the capacity of A100 in general. If I will talk

01:27:11.280 --> 01:27:14.940
about H100 again A100 series you will be able to find out a

01:27:14.940 --> 01:27:19.200
variance. H100 ATGV if I will talk about. So it will be able

01:27:19.200 --> 01:27:23.940
to process approximately 3 to 6GV. This token thousand token

01:27:23.940 --> 01:27:30.960
k means thousand. So tokens per second per second it will be

01:27:30.960 --> 01:27:36.580
able to process in general. If again given a condition right

01:27:36.580 --> 01:27:41.540
given a condition that if my model size is approximately 7

01:27:41.540 --> 01:27:44.800
billion it's not like you will try to load 120 billion model

01:27:44.800 --> 01:27:49.200
and then it's going to process 2 to 4k token. No it will be

01:27:49.200 --> 01:27:52.580
less if my model size is increasing. For example. As per the

01:27:52.580 --> 01:27:58.700
benchmark right if I have a 13 billion parameter model. So

01:27:58.700 --> 01:28:02.540
in that case number of token that it will be able to process

01:28:02.540 --> 01:28:08.880
is going to be 1.2 to 2.5k which is less right. So here my

01:28:08.880 --> 01:28:12.780
model parameter was less. My number of token processed by a

01:28:12.780 --> 01:28:17.520
GPU was more per second. As my token size as my model size

01:28:17.520 --> 01:28:20.180
has increased right so obviously your inferencing will slow.

01:28:20.340 --> 01:28:24.380
So. You will be able to process 1.2k to 2.5k now based on

01:28:24.380 --> 01:28:27.720
this you can do the exact estimation not approximate

01:28:27.720 --> 01:28:30.240
approximate approximate means obviously there will be a

01:28:30.240 --> 01:28:34.160
delta of 5 percent 7 percent error rate but you will be able

01:28:34.160 --> 01:28:37.760
to do the estimation with respect to all the GPUs that okay

01:28:37.760 --> 01:28:40.560
if I'm using this GPU if I have this many number of users

01:28:40.560 --> 01:28:43.200
and if this GPU will be able to process this many tokens

01:28:43.200 --> 01:28:46.900
then how many such GPUs I need. This is how we conclude the

01:28:46.900 --> 01:28:50.500
like entire like a topics when we say and when we try to

01:28:50.500 --> 01:28:53.840
like you know. Do a infra estimation not just during a

01:28:53.840 --> 01:28:56.980
training even after training so we have to host it right. So

01:28:56.980 --> 01:28:59.860
based on the user. So this is how we try to like set the

01:28:59.860 --> 01:29:02.680
entire infrastructure even for a scaling it's not like I

01:29:02.680 --> 01:29:04.640
will spin all the hundred machine or thousand machine

01:29:04.640 --> 01:29:08.060
altogether because every machine comes with the heavy cost

01:29:08.060 --> 01:29:12.460
right. Now let's suppose if I'm talking about a 70 billion

01:29:12.460 --> 01:29:15.200
parameter model right 70 billion parameter models obviously

01:29:15.200 --> 01:29:18.880
number of token which it can process will be less right as

01:29:18.880 --> 01:29:22.100
my model size is increasing. Right. Models weight is

01:29:22.100 --> 01:29:25.920
increasing so it will be able to process approximately 0.5

01:29:25.920 --> 01:29:32.320
to I can say 1k token 1000 500 to 1000 token it will be able

01:29:32.320 --> 01:29:37.640
to process per second right in a 100 estimation wise right.

01:29:37.760 --> 01:29:42.540
So as you can see my model size is increasing and number of

01:29:42.540 --> 01:29:47.920
token process will decrease in the same GPU so it very much

01:29:47.920 --> 01:29:50.380
depends upon the total weight of the model. So that's the

01:29:50.380 --> 01:29:52.760
reason I was talking about the quantization. So if I have to

01:29:52.760 --> 01:29:56.600
run something in my mobile devices or maybe Raspberry Pi or

01:29:56.600 --> 01:30:00.320
maybe Google Corel or Nvidia Jetson Nano kind of a or JVR

01:30:00.320 --> 01:30:03.280
kind of a devices. So obviously which is called as edge

01:30:03.280 --> 01:30:06.740
device right edge device or mobile devices. So over there I

01:30:06.740 --> 01:30:12.960
need a very very like a less model size right. This is where

01:30:12.960 --> 01:30:15.260
this quantization comes into picture it will try to convert

01:30:15.260 --> 01:30:19.460
maybe this like parameter is going to be same. But it will

01:30:19.460 --> 01:30:21.260
try to. It will try to decrease the size because it is

01:30:21.260 --> 01:30:25.220
increasing the representation of the number bits itself 16

01:30:25.220 --> 01:30:28.780
bit to 4 bit. So obviously more bits means more space less

01:30:28.780 --> 01:30:32.300
bits means less space inside a memory and eventually that

01:30:32.300 --> 01:30:36.040
will decrease the size of the model. Now so all of these

01:30:36.040 --> 01:30:38.000
things are standard 16 bit I'm talking about let's suppose.

01:30:38.240 --> 01:30:42.520
Now if I'll talk about the H100 so if there will be a 13

01:30:42.520 --> 01:30:47.020
billion parameter approximately right approximately H100

01:30:47.020 --> 01:30:51.320
will be able to process 2 to 4k token. So in case of 7

01:30:51.320 --> 01:30:55.100
billion parameter model 3 to 6k token and in case of this

01:30:55.100 --> 01:30:58.580
one 2 to 4k token 1000 token it will be able to process per

01:30:58.580 --> 01:31:02.700
sec second by the way yeah. And in case of a 70 billion

01:31:02.700 --> 01:31:06.160
parameter so it will be able to process how many tokens so 1

01:31:06.160 --> 01:31:11.100
to 2k token per sec by the way if I'm using a H100. So

01:31:11.100 --> 01:31:14.680
obviously comparison wise H100 performance will be like

01:31:14.680 --> 01:31:18.960
better as compared to my A100. Now so if I'm talking about

01:31:18.960 --> 01:31:24.640
my system. My machine which is having a RTX 4090 yeah. So my

01:31:24.640 --> 01:31:28.960
machine is having like a RTX 4090 which is having a 24 GB of

01:31:28.960 --> 01:31:33.060
GPU 96 GB of a RAM. If I'll talk about my system

01:31:33.060 --> 01:31:37.240
benchmarking so my system can process 70 billion parameter

01:31:37.240 --> 01:31:42.620
model sorry my bad. So if model is 7 billion parameter

01:31:42.620 --> 01:31:46.580
approximately so in that case it is going to process 0.6k

01:31:46.580 --> 01:31:53.560
token to 1.2k token. Ok. Right. And if I have a 13 billion

01:31:53.560 --> 01:32:00.640
parameter model, then it will be able to process 0.3 to 0.7k

01:32:00.640 --> 01:32:04.800
like a token per second. And if I'm using a 70 billion

01:32:04.800 --> 01:32:09.320
parameter model, maybe my entire system will go for a toss.

01:32:09.740 --> 01:32:12.460
Yeah, my system will hang. So I'm not writing a benchmarking

01:32:12.460 --> 01:32:15.580
for 70 billion parameter for my system because practically

01:32:15.580 --> 01:32:21.080
it will not work. Because that's an upper limit like 10

01:32:21.080 --> 01:32:24.400
billion, 15 billion I can process, but 70 billion will be

01:32:24.400 --> 01:32:27.140
like 20 billion is also like I have seen that like 20

01:32:27.140 --> 01:32:30.220
billion is working well in my system for a single user. But

01:32:30.220 --> 01:32:33.260
when two users will start hitting means my token processing

01:32:33.260 --> 01:32:36.800
should be double. And in that case, again, it will choke. It

01:32:36.800 --> 01:32:40.620
will not be able to process it. Right. So that that's a

01:32:40.620 --> 01:32:42.740
capacity of my system, by the way.

01:32:46.140 --> 01:32:50.420
Yeah. How do you calculate a parameter? Parameter by default

01:32:50.420 --> 01:32:54.320
is nothing but this lines, right? This weights, trainable

01:32:54.320 --> 01:32:57.160
weights is a parameter. So I believe whenever you are going

01:32:57.160 --> 01:33:00.440
to create a model, just try to call a model dot summary. It

01:33:00.440 --> 01:33:03.080
will give you a parameter for everything. If you have built

01:33:03.080 --> 01:33:06.740
a model once in your life, deep learning, forget about like

01:33:06.740 --> 01:33:10.620
LLM models. Even if you have built a maybe like a computer

01:33:10.620 --> 01:33:14.060
vision model. Right. We call a model summary. I believe you

01:33:14.060 --> 01:33:17.340
remember that part and model summary gives you the number of

01:33:17.340 --> 01:33:19.360
parameter. Eventually that parameters are nothing. But these

01:33:19.360 --> 01:33:21.920
lines, right, are weight lines, which is connecting the

01:33:21.920 --> 01:33:25.600
neurons, weights, basically. So that is a total number of

01:33:25.600 --> 01:33:28.720
the parameter. Parameter is a trainable parameter. And this

01:33:28.720 --> 01:33:33.600
is where this 7B or this 70B or this one comes into picture.

01:33:33.860 --> 01:33:38.380
So these parameters are like coming from where the 70B or 7B

01:33:38.380 --> 01:33:41.440
or 13B. All of these things are coming from where coming

01:33:41.440 --> 01:33:43.920
from that trainable parameter, trained parameter. That is

01:33:43.920 --> 01:33:46.080
the number of parameter which has been trained on this

01:33:46.080 --> 01:33:48.580
parameter will be eventually give you the final output.

01:33:49.020 --> 01:33:51.800
Again, there will be some model creation. So you will be

01:33:51.800 --> 01:33:53.860
able to find out some data parameter. You will be able to

01:33:53.860 --> 01:33:57.060
find out some non-trainable parameter, which is just like

01:33:57.060 --> 01:34:01.080
behave as a delta on top of like an entire model of weights.

01:34:01.520 --> 01:34:05.580
So this is called a parameter. Now, number of tokens. Tokens

01:34:05.580 --> 01:34:09.780
definition is a bit different. You can't say that one word

01:34:09.780 --> 01:34:15.100
is equal to one token because in general, like a definition

01:34:15.100 --> 01:34:18.840
wise, like token definition wise. Point seven five or point.

01:34:19.020 --> 01:34:22.720
Eight zero on an average, right? Let's suppose average word

01:34:22.720 --> 01:34:27.120
is having like a seven or eight character. Let's imagine,

01:34:27.380 --> 01:34:32.220
right? Let's think in that way. So 70.75 or maybe like point

01:34:32.220 --> 01:34:37.020
eight of that's like a eight eight character combination,

01:34:37.340 --> 01:34:40.040
which is representing a single word is called as a token.

01:34:40.180 --> 01:34:43.460
That's the actual calculation for the token. And every

01:34:43.460 --> 01:34:47.320
platform, you know, try to tweak tweak it like just to maybe

01:34:47.320 --> 01:34:49.120
charge you more because people are charging. You're going to

01:34:49.120 --> 01:34:52.100
token, right? But yeah, that's the definition of the token.

01:34:52.200 --> 01:34:55.560
So you can try to check it platform wise. Maybe you can try

01:34:55.560 --> 01:34:59.200
to check the token like a definition for open AI, then for a

01:34:59.200 --> 01:35:03.680
public city. And then you can try to check it for a cloud

01:35:03.680 --> 01:35:06.980
era. Sorry, cloud. I'm talking about cloud a basically,

01:35:07.180 --> 01:35:10.920
right? Anthropic, basically. So anthropic cloud is on it

01:35:10.920 --> 01:35:14.520
model. You can try to check basically. So that is something

01:35:14.520 --> 01:35:18.840
called as tokens. So generally we as a people or we as a.

01:35:18.840 --> 01:35:21.940
Initial learners, we think that one word is equal to one

01:35:21.940 --> 01:35:25.360
token, but that is not an actual definition, by the way.

01:35:25.520 --> 01:35:28.040
Yeah, that is not an actual definition. According to open

01:35:28.040 --> 01:35:31.820
AI, it's a point seven five, I believe, which I remember

01:35:31.820 --> 01:35:35.200
when I checked it last time. Yeah. So that's that's actual

01:35:35.200 --> 01:35:38.620
definition. Maybe you can check it now. Every platform will

01:35:38.620 --> 01:35:41.780
be able to give you some like a different kind of a margin.

01:35:42.800 --> 01:35:46.100
Scroll down a little bit. OK, there is nothing down, by the

01:35:46.100 --> 01:35:50.720
way. Yeah. So this is the benchmarking. So this is for a one

01:35:50.720 --> 01:35:53.800
hundred eight one hundred. And then this is for my system,

01:35:53.900 --> 01:35:57.860
by the way. RTX 4090, which I'm using in my machine, which

01:35:57.860 --> 01:36:02.040
I'm using for a teaching and my project basically. So

01:36:02.040 --> 01:36:05.280
accordingly, you can do now the complete math that how many

01:36:05.280 --> 01:36:07.980
number of users, what will be their chat frequency, what

01:36:07.980 --> 01:36:10.620
will be the average chat they will do, what will the average

01:36:10.620 --> 01:36:12.920
token they will try to give as a query, what will be the

01:36:12.920 --> 01:36:15.780
average token they will be able to get as an output. So when

01:36:15.780 --> 01:36:18.180
you will do all the average. Raging, you will reach out to

01:36:18.180 --> 01:36:20.820
the actual calculation so you can prepare a beautiful Excel

01:36:20.820 --> 01:36:24.180
sheet just for doing this calculation. And then based on

01:36:24.180 --> 01:36:27.280
that, you can decide, OK, so if this is the number of users,

01:36:27.400 --> 01:36:30.120
this is the average chat they are going to do. And this is

01:36:30.120 --> 01:36:33.660
the average token per chat they are trying to use and per

01:36:33.660 --> 01:36:36.840
second. Right. Because everything has been calculated per

01:36:36.840 --> 01:36:39.100
second because machine will be up twenty four by seven. So

01:36:39.100 --> 01:36:41.400
obviously you have to it's not like, you know, you know,

01:36:41.400 --> 01:36:45.280
like in a twenty four hour if hundred user is coming. Right.

01:36:45.580 --> 01:36:49.500
And in one second. Twenty four hundred user is coming in

01:36:49.500 --> 01:36:51.760
both the places. Calculation will be different. A GPU

01:36:51.760 --> 01:36:54.180
calculation will be different. Right. Because hundred user

01:36:54.180 --> 01:36:57.760
twenty four hour. Right. Hundred user single second means

01:36:57.760 --> 01:37:00.600
like in a single hit. So calculation is going to be

01:37:00.600 --> 01:37:03.260
different. Your GPU load is going to be different even for

01:37:03.260 --> 01:37:06.400
one single second. That hundred second. Sorry, just that

01:37:06.400 --> 01:37:08.700
particular one second. Hundred people are coming all

01:37:08.700 --> 01:37:12.020
together means you have to process that many number of

01:37:12.020 --> 01:37:15.320
tokens input output both. So you should do the math

01:37:15.320 --> 01:37:17.580
accordingly. So based on the traffic, by the way. And this

01:37:17.580 --> 01:37:19.640
is something that we do all the time, load testing, even

01:37:19.640 --> 01:37:22.600
with respect to website, right? We do the same thing. So

01:37:22.600 --> 01:37:25.140
this is again nothing new, by the way. So server cost

01:37:25.140 --> 01:37:30.100
calculation. We all as a, you know, product developers used

01:37:30.100 --> 01:37:33.920
to do. Yeah. Or maybe like infra costing. So we all try to

01:37:33.920 --> 01:37:37.140
estimate in that way. It's not like I will just go and

01:37:37.140 --> 01:37:40.040
blindly invest, start investing on the infrastructure.

01:37:40.720 --> 01:37:45.580
Right. So this is the estimation. Again, training, how much

01:37:45.580 --> 01:37:49.760
time like my data. My system is going to take what will be

01:37:49.760 --> 01:37:53.240
the training cost. Again, I will be able to estimate it from

01:37:53.240 --> 01:37:57.240
this sort of estimation itself. Right. If I'm no machine

01:37:57.240 --> 01:37:59.600
level calculation that, OK, fine, this machine is this much

01:37:59.600 --> 01:38:02.600
fast, this much of token it will be able to process. So

01:38:02.600 --> 01:38:05.880
roughly I will be able to estimate that, OK, to, you know,

01:38:05.900 --> 01:38:09.900
train a model, how much cost is required or for how many

01:38:09.900 --> 01:38:14.040
hours is required. As simple as that. So hope this this

01:38:14.040 --> 01:38:18.240
particular part is clear to all of you. Now coming to the

01:38:18.240 --> 01:38:22.680
data part. So how much data is required? Compute. We are

01:38:22.680 --> 01:38:25.400
like we are able to do the calculation for training as well

01:38:25.400 --> 01:38:29.760
as for inferencing. Now, how much data in general is

01:38:29.760 --> 01:38:33.140
required? Right. So is it like a 10 million? Is it like a

01:38:33.140 --> 01:38:35.820
hundred million? Is it like a billion of data required for

01:38:35.820 --> 01:38:38.740
like, you know, all of these kind of a training that we have

01:38:38.740 --> 01:38:43.640
discussed, by the way. So supervised fine tuning, right. SFT

01:38:43.640 --> 01:38:46.560
in general, we used to say a supervised fine tuning. So

01:38:46.560 --> 01:38:49.260
where we are trying to give like a supervised means what?

01:38:49.380 --> 01:38:51.740
Supervised means we have like a question, we have an answer.

01:38:51.880 --> 01:38:53.800
That is something called a supervised fine tuning. Right.

01:38:54.060 --> 01:38:57.200
That is a definition of supervised even from the concept of

01:38:57.200 --> 01:39:02.040
machine learning. So if I'm doing a supervised fine tuning,

01:39:02.180 --> 01:39:06.680
SFT, let's say, right, inside supervised. So maybe I'm doing

01:39:06.680 --> 01:39:11.600
a LoRa, I'm doing a Cora or I'm trying to do maybe like a

01:39:11.600 --> 01:39:18.060
DPO, something like that. Right. DPO is basically RLHF. So

01:39:18.060 --> 01:39:23.180
DPO is not supervised, by the way, my bad. DPO is RLHF. So

01:39:23.180 --> 01:39:26.760
supervised fine tuning, if I'm trying to do so, if I'm

01:39:26.760 --> 01:39:30.160
trying to train the model for a particular domain, maybe

01:39:30.160 --> 01:39:35.380
like a finance domain or something like that. So in general,

01:39:35.700 --> 01:39:43.400
three to 20 million token is required, means this much of

01:39:43.400 --> 01:39:45.720
data you have to prepare. Again, it's a basic thing. Biggest

01:39:45.720 --> 01:39:50.120
scale I know, but yeah, minimum is 3 million and somewhere

01:39:50.120 --> 01:39:53.720
mid 20 million. Bigger is always better, right? Bigger is

01:39:53.720 --> 01:39:55.460
always better. But yeah, this is something which is

01:39:55.460 --> 01:40:01.220
required. Right. So basically if I'm trying to maybe, so if

01:40:01.220 --> 01:40:05.700
I'm trying to train just for the domain, a small domain

01:40:05.700 --> 01:40:06.960
adoption

01:40:11.150 --> 01:40:17.010
means for a small client, if I'm training it. But 20 to 100

01:40:18.290 --> 01:40:21.230
million token is required. So 20 to 100 million token is

01:40:21.230 --> 01:40:25.150
required in general. So if I'm trying to train the

01:40:25.150 --> 01:40:29.430
enterprise assistant, right, so which is going to take a lot

01:40:29.430 --> 01:40:32.750
of entire enterprise, maybe a big one, right? Fortune 500.

01:40:33.230 --> 01:40:40.460
So in that case, enterprise. So 20 to 100 million token is

01:40:40.460 --> 01:40:44.500
required in general. And let's suppose if I'm like building

01:40:44.500 --> 01:40:48.120
a model, which is a generic model for a big, big audience.

01:40:48.300 --> 01:40:52.220
Right. In a particular segment. Maybe for a health care. Or

01:40:52.220 --> 01:40:56.780
maybe I will say for a K-12 education. Or maybe just for

01:40:56.780 --> 01:40:59.380
coding. Right. Just for coding. That's for that particular

01:40:59.380 --> 01:41:03.980
task. So then I will need somewhere between 100 million

01:41:03.980 --> 01:41:09.340
token to 500 million token. Now from here, you will be able

01:41:09.340 --> 01:41:12.240
to get an answer. So generic, let's suppose. Generic for a

01:41:12.240 --> 01:41:20.520
particular domain. A big one. Generic for big domain.

01:41:24.570 --> 01:41:27.130
Domain. So now if someone is going to ask you a question

01:41:27.130 --> 01:41:30.370
that, okay, how much data you have prepared. Right. How we

01:41:30.370 --> 01:41:33.230
have prepared. What format of the data we have prepared. I

01:41:33.230 --> 01:41:36.370
think we have an answer for that. Right. How much data you

01:41:36.370 --> 01:41:38.950
have prepared. We have an answer. So whatever story that you

01:41:38.950 --> 01:41:41.810
are writing, whatever DPR or detailed project report that

01:41:41.810 --> 01:41:44.410
you are writing. So you can try to choose based on the

01:41:44.410 --> 01:41:47.070
application for which you are writing. Right. Maybe for a

01:41:47.070 --> 01:41:50.630
sales domain. Maybe for just a tone adaption. Maybe just for

01:41:50.630 --> 01:41:54.250
like, you know, putting up some sort of guardrails. RLHF.

01:41:54.250 --> 01:41:58.410
Based on that, you can try to, you know, say that, okay,

01:41:58.450 --> 01:42:01.370
fine, this is my tokens. Calculation and compute. So again,

01:42:01.450 --> 01:42:04.670
you can try to choose out of this one in your DPR, detailed

01:42:04.670 --> 01:42:10.030
project report, by the way. Now, so if I'm trying to do a

01:42:10.030 --> 01:42:13.430
DPO or RLHF, which is nothing but a feedback based training.

01:42:13.630 --> 01:42:18.750
Right. Direct preference or optimization or maybe RLHF. This

01:42:18.750 --> 01:42:22.870
is nothing but feedback based training. So here in general.

01:42:22.870 --> 01:42:26.590
If I'm just trying to build something like an initial model.

01:42:26.750 --> 01:42:31.070
Right. With respect to this one. So not much is required

01:42:31.070 --> 01:42:34.550
because model will already be trained on a big data. So we

01:42:34.550 --> 01:42:37.730
are just trying to change or tune our nature of the model.

01:42:38.050 --> 01:42:42.190
So here 100K tokens are required. So somewhere around 30,000

01:42:42.190 --> 01:42:46.910
to 1 lakh tokens are required. Pairs are required. Right. So

01:42:46.910 --> 01:42:50.030
that I will be able to, you know, teach a model in terms of

01:42:50.030 --> 01:42:53.730
a tone, in terms of avoiding some sort of a harmful contents

01:42:53.730 --> 01:42:56.970
and everything. Right. If I'll talk about a mature system,

01:42:57.230 --> 01:43:00.630
obviously. So where it should be like a super solid, it

01:43:00.630 --> 01:43:04.350
should not give any kind of a ambiguous answer. So in that

01:43:04.350 --> 01:43:11.430
situation, I need somewhere 100K to 300K. So 1 lakh to 3

01:43:11.430 --> 01:43:15.870
lakh minimum token is required for building this RLHF.

01:43:15.970 --> 01:43:18.070
Because this is nothing but a fine tuning. You can say fine.

01:43:18.290 --> 01:43:21.430
Everything, even SFT is a fine tuning. I'm not denying that.

01:43:21.430 --> 01:43:24.790
I'm denying that fact. But again, in SFT, we are trying to

01:43:24.790 --> 01:43:27.130
train with a new data. But here, so model is already

01:43:27.130 --> 01:43:29.790
trained. We are just trying to tune the behavior of the

01:43:29.790 --> 01:43:34.770
model. That's it. This is what we do with respect to a RLHF.

01:43:35.810 --> 01:43:39.370
So this is in general that you should know with respect to

01:43:39.370 --> 01:43:41.910
the number of token. And based on that, you can try to

01:43:41.910 --> 01:43:47.310
decide that, okay, this is like data. This is the model. And

01:43:47.310 --> 01:43:52.110
then this is generally a time it is going to take. Now

01:44:00.400 --> 01:44:03.560
let's come to the people part. People part.

01:44:07.040 --> 01:44:12.060
Or you can say a team part. So I'm involved into any of

01:44:12.060 --> 01:44:14.240
these tasks, fine tuning tasks. So obviously like I told

01:44:14.240 --> 01:44:19.020
you, so fine tuning task always starts from a data

01:44:19.020 --> 01:44:22.960
acquisition or you can say data gathering.

01:44:27.330 --> 01:44:32.450
Data gathering, this is going to be the very first step. Now

01:44:32.450 --> 01:44:34.830
when I talk about a data gathering, so I'm talking about a

01:44:34.830 --> 01:44:38.470
private data. Or maybe your client is going to acquire some

01:44:38.470 --> 01:44:43.530
of the data set from a website. Or maybe for example, right,

01:44:43.610 --> 01:44:47.190
I know people who used to sell the books, right? And they

01:44:47.190 --> 01:44:49.830
have a digital print of the books. So obviously those books

01:44:49.830 --> 01:44:52.410
are available publicly. But it's not like even as a PDF,

01:44:52.430 --> 01:44:55.190
those books are available publicly. But let's suppose if I

01:44:55.190 --> 01:44:58.110
have to train my model, I can't just go and scrap their PDF

01:44:58.110 --> 01:45:02.950
or read their PDF. No. That is like they may file a lawsuit.

01:45:04.290 --> 01:45:06.730
Like maybe. Maybe today, tomorrow when they will be able to

01:45:06.730 --> 01:45:09.650
find out. And yeah, eventually they will be able to find

01:45:09.650 --> 01:45:14.250
out. So basically what we do, that we try to obtain a

01:45:14.250 --> 01:45:18.110
license. Again, licensing will give you a huge, huge amount

01:45:18.110 --> 01:45:22.550
of the cost if you are trying to take a license or like just

01:45:22.550 --> 01:45:25.130
a read license, right? If you're trying to take the data

01:45:25.130 --> 01:45:28.970
licenses, again, there will be a multiple different kind of

01:45:28.970 --> 01:45:32.330
licenses. It's not like you own the data. It's like you have

01:45:32.330 --> 01:45:35.910
leased a data. You have leased the data. So there are so

01:45:35.910 --> 01:45:38.510
many like companies I know personally, right? I know

01:45:38.510 --> 01:45:41.250
personally and I know a lot of people inside those

01:45:41.250 --> 01:45:44.830
companies. So who have a lot of data, right? And they try to

01:45:44.830 --> 01:45:48.090
lease out those data to other people. Best example I can

01:45:48.090 --> 01:45:50.510
give you like book publishers because they have a lot of

01:45:50.510 --> 01:45:53.310
books, right? You can just name any of the book publishers

01:45:53.310 --> 01:45:56.550
and everyone is selling the data, right? Everyone is selling

01:45:56.550 --> 01:46:00.190
the data because they have like books since last 50 years,

01:46:00.270 --> 01:46:02.890
maybe like 80 years. Some publishers are 100 years old.

01:46:02.890 --> 01:46:06.350
Right? And they have like a lot of books, a lot of data, and

01:46:06.350 --> 01:46:09.950
they have their books over internet in a PDF format. A lot

01:46:09.950 --> 01:46:12.730
of books are available in hard copy. I need all those data

01:46:12.730 --> 01:46:16.070
because this is a big because you can't go to library and

01:46:16.070 --> 01:46:19.010
then take out those data set, by the way, right? All the

01:46:19.010 --> 01:46:21.330
books are available in the library. But if you're trying to

01:46:21.330 --> 01:46:23.890
train the model, you will not be able to use it because

01:46:23.890 --> 01:46:25.810
that's not your data. If you're using it, you're stealing

01:46:25.810 --> 01:46:29.790
it, by the way. Yeah. Many platforms have already done that

01:46:29.790 --> 01:46:32.130
and they are facing a lawsuit. I think we all know the

01:46:32.130 --> 01:46:35.770
story. But whenever you are training a data, we have to go

01:46:35.770 --> 01:46:38.750
in a legal way. So data gathering or data acquisition,

01:46:39.090 --> 01:46:46.270
right? So acquisition is one of the big deal and one of the

01:46:46.270 --> 01:46:53.250
costly deal that we all use us to do because we are not

01:46:53.250 --> 01:46:55.930
Google, right? So we are not having all kind of a data set

01:46:55.930 --> 01:46:59.010
and whatever data set that again. So if I'll talk about like

01:46:59.010 --> 01:47:02.190
a medical data, so obviously patient record data, right? So

01:47:02.190 --> 01:47:04.390
you have to go to all the hospitals and then you have to

01:47:04.390 --> 01:47:07.270
acquire the data set in a public domain. All those data set

01:47:07.270 --> 01:47:09.730
will not be available if you have to obtain a research data

01:47:09.730 --> 01:47:14.290
set, right? If you have to obtain maybe like, you know, drug

01:47:14.290 --> 01:47:16.870
synthesis, if you're doing a drug synthesis or if you're

01:47:16.870 --> 01:47:19.310
building something for that synthesis. So you have to

01:47:19.310 --> 01:47:22.530
acquire those data set which has been used for a drug

01:47:22.530 --> 01:47:27.590
discovery or testing or any other things. So data is very,

01:47:27.750 --> 01:47:31.770
very specific things. And data acquisition itself is a big

01:47:31.770 --> 01:47:36.390
deal. Right? Inside a company. And again, so before, you

01:47:36.390 --> 01:47:38.890
know, acquiring a data before making a payment because

01:47:38.890 --> 01:47:41.310
before taking a license and every company sells a data,

01:47:41.390 --> 01:47:44.210
every company, whoever is having because every company is

01:47:44.210 --> 01:47:46.790
sitting on a bomb of data, especially the old one, the big

01:47:46.790 --> 01:47:49.590
one, right? Old one and the big one. Like I said, a book

01:47:49.590 --> 01:47:51.650
publisher, right? A hundred year old. They have a lot of

01:47:51.650 --> 01:47:53.950
data, right? They have a lot of data. They have not acquired

01:47:53.950 --> 01:47:57.370
data from you basically, right? So generally we think that

01:47:57.370 --> 01:48:01.050
that companies are using all the data which has been

01:48:01.050 --> 01:48:05.670
acquired. Or, you know, through a Google or maybe through

01:48:05.670 --> 01:48:08.510
some platform like a social media platform. But no, on

01:48:08.510 --> 01:48:10.490
social media platform, all those data set is not available.

01:48:10.850 --> 01:48:13.450
Right? For example, medical research data set where you will

01:48:13.450 --> 01:48:16.490
find out on social media platform, right? Actual finance

01:48:16.490 --> 01:48:18.390
data set where you will be able to find out on a social

01:48:18.390 --> 01:48:22.090
media platform. So, yes, on a social media platform, we

01:48:22.090 --> 01:48:25.410
generally find out a normal conversation based data set.

01:48:25.550 --> 01:48:30.550
Right? So even social media is a very small piece. We as a

01:48:30.550 --> 01:48:33.710
general public. We see that that, okay, these companies are

01:48:33.710 --> 01:48:36.710
collecting a huge amount of data set for me. They are

01:48:36.710 --> 01:48:39.590
collecting not just a data set. They are collecting your

01:48:39.590 --> 01:48:43.550
behaviors more than a data set because they have to run the

01:48:43.550 --> 01:48:46.550
ads. Basically, obviously they are collecting the data set,

01:48:46.550 --> 01:48:50.770
a data set of your behavior, right? Your clicks, your swaps,

01:48:50.790 --> 01:48:53.190
everything, right? Everything they are collecting. But

01:48:53.190 --> 01:48:58.050
majority of like, you know, authentic data set, you will be

01:48:58.050 --> 01:49:01.410
able to find out from someone who have already worked,

01:49:01.490 --> 01:49:04.530
researched and vetted. And those data sets will be available

01:49:04.530 --> 01:49:09.270
in a real places. For example, like knowledge means books,

01:49:09.410 --> 01:49:13.230
right? Because we as a human being are reading books since

01:49:13.230 --> 01:49:16.590
hundreds and hundreds of years. Right? And we have written

01:49:16.590 --> 01:49:19.330
everything into our books. Right? We have written everything

01:49:19.330 --> 01:49:22.090
into our books. Almost everything is available inside the

01:49:22.090 --> 01:49:26.590
books. Right? Even till date. So it's a vetted source,

01:49:26.790 --> 01:49:28.930
right? It's a vetted source. It's an authentic source, by

01:49:28.930 --> 01:49:33.090
the way. Right? It's a true source. So books, again, are

01:49:33.090 --> 01:49:35.430
research. So everyone write a research paper. There are like

01:49:35.430 --> 01:49:39.570
not just one or two. There are like millions of fields

01:49:39.570 --> 01:49:42.450
itself. And every field, there are millions and billions of

01:49:42.450 --> 01:49:44.910
like a research which is happening in last hundred year,

01:49:45.050 --> 01:49:48.230
let's suppose. Right? Every country, every researcher, every

01:49:48.230 --> 01:49:51.270
lab, every institute, every like, you know, premium

01:49:51.270 --> 01:49:54.950
institutes, everyone is doing that. So, and that too in a

01:49:54.950 --> 01:49:56.930
different field. Right? In a completely different field.

01:49:57.090 --> 01:50:01.830
Right? So from those research papers, because again, that

01:50:01.830 --> 01:50:04.510
will be the authentic one, that will be the vetted one. So

01:50:04.510 --> 01:50:06.950
there are data which is distributed across all of these

01:50:06.950 --> 01:50:09.430
things and every company try to acquire. They try to spend a

01:50:09.430 --> 01:50:14.390
lot in a millions, you can say. Right? So 1 million is equal

01:50:14.390 --> 01:50:18.370
to 8 to 9 crores in INR. So that is like a cost they have to

01:50:18.370 --> 01:50:21.270
like a pay just to take a license. Again, they don't own

01:50:21.270 --> 01:50:25.050
those data, but so data gathering is one of the biggest

01:50:25.050 --> 01:50:27.790
thing. It's not like every time when you like need a data,

01:50:27.890 --> 01:50:31.050
you just go to the website and scrap the data. Not at all.

01:50:31.190 --> 01:50:34.270
That is not that. Yeah. We scrap the website for maybe a

01:50:34.270 --> 01:50:39.610
real latest or news based data. But again, that is not a

01:50:39.610 --> 01:50:42.630
thumb rule. So don't go and say that into an interview. Say

01:50:42.630 --> 01:50:45.690
that, okay, fine. So we had a requirement and then my

01:50:45.690 --> 01:50:48.610
company has a lot of vendors who was providing this kind of

01:50:48.610 --> 01:50:52.550
data. Just name some XYG vendors. Right? Some random name.

01:50:52.610 --> 01:50:54.650
That's completely fine. And we have acquired this data, this

01:50:54.650 --> 01:50:57.590
data, this data from them. Basically we have taken a license

01:50:57.590 --> 01:51:00.150
of those data. So this was the volume of the data that they

01:51:00.150 --> 01:51:03.550
have given to me. And before even acquiring the data, before

01:51:03.550 --> 01:51:07.390
taking a license, so we have basically vet out those data.

01:51:07.510 --> 01:51:10.790
How much percentage of this data I will be able to use.

01:51:11.010 --> 01:51:13.490
Right? Because accordingly company is going to pay. Again,

01:51:13.530 --> 01:51:16.050
there are like a lot of licensing agreement, but yeah, data

01:51:16.050 --> 01:51:20.310
gathering is one of the biggest thing that we try to like

01:51:20.310 --> 01:51:25.410
do. Now. Once we are able to gather the data, maybe digital,

01:51:25.630 --> 01:51:28.450
maybe physical, right? Depends. Let's suppose someone is

01:51:28.450 --> 01:51:31.250
giving you a physical copy of it because we all know that

01:51:31.250 --> 01:51:35.610
all the old system was working in a physical copy, right?

01:51:35.730 --> 01:51:40.630
Even our entire like a judicial system, if I'll talk about,

01:51:40.730 --> 01:51:45.190
right? Or even if I'll talk about maybe a land acquisition

01:51:45.190 --> 01:51:48.430
or transfer or buy and sale, everything was happening on a

01:51:48.430 --> 01:51:51.710
basically like a physical paper, right? Physical paper and a

01:51:51.710 --> 01:51:55.790
stamp. So for some of the use cases you will be able to get,

01:51:55.970 --> 01:51:58.870
uh, because distillation has started happening in last four

01:51:58.870 --> 01:52:01.970
to five years, right? You can say 10 year max to max, but

01:52:01.970 --> 01:52:05.230
again, before 10 year, we have just started the actual data

01:52:05.230 --> 01:52:07.410
conversion started maybe last seven to eight years itself,

01:52:07.510 --> 01:52:10.510
right? Before that majority of things was happening on a pen

01:52:10.510 --> 01:52:12.830
and paper by pen and paper itself, right? Majority,

01:52:12.870 --> 01:52:15.430
especially these transactions, right? So where you see the

01:52:15.430 --> 01:52:18.310
actual record and actual data, it's still, it's happening,

01:52:18.430 --> 01:52:22.350
right? It's still, we use paper. Okay. So we can get, uh,

01:52:22.550 --> 01:52:25.590
like, uh, you know, uh, and there is a company basically. So

01:52:25.590 --> 01:52:28.410
I don't know whether you are aware about this or not, but

01:52:28.410 --> 01:52:31.250
yeah, there are so many companies who just does this data

01:52:31.250 --> 01:52:35.790
processing and there are like a 10, 20 year old company who

01:52:35.790 --> 01:52:38.390
just does this one. They just write a code, they build a

01:52:38.390 --> 01:52:41.110
system just for doing a data processing itself because it's

01:52:41.110 --> 01:52:43.250
a huge task, right? Data can come in a different, different

01:52:43.250 --> 01:52:47.370
formats. So data gathering and then data obviously vetting

01:52:47.370 --> 01:52:50.330
we used to do and then data processing. Now when I said data

01:52:50.330 --> 01:52:55.040
processing, it's a very big task because there will be a

01:52:55.040 --> 01:52:57.320
different, different kind of a physical file, distal file

01:52:57.320 --> 01:53:00.420
again in a physical file, different formats. Some will be

01:53:00.420 --> 01:53:03.340
blurred, some will be like a half vetted, many things, many

01:53:03.340 --> 01:53:05.800
problems we used to face some with the images, some with the

01:53:05.800 --> 01:53:08.260
line, some with the signatures, a lot of like a variations

01:53:08.260 --> 01:53:10.140
you will be able to find out when you are especially

01:53:10.140 --> 01:53:13.040
processing the old documents. New one is still fine, but

01:53:13.040 --> 01:53:15.780
yeah, old one will be like a messy one and you have to

01:53:15.780 --> 01:53:18.480
process it. So basically data processing, or you can say a

01:53:18.480 --> 01:53:21.120
pre-processing, which is itself is a habit. You are having a

01:53:21.120 --> 01:53:26.060
very big layer and lines, right? And then data alignment,

01:53:26.200 --> 01:53:29.320
I'm just writing you the overall like layers. So data

01:53:29.320 --> 01:53:31.140
alignment, alignment

01:53:33.540 --> 01:53:40.140
with respect to model. So whatever task that you are trying

01:53:40.140 --> 01:53:43.180
to do, so maybe embedding task, maybe like a re-ranking

01:53:43.180 --> 01:53:46.440
task, maybe SFTs, supervised fine tuning, RLHF, DPO,

01:53:46.620 --> 01:53:49.080
whatever task you are trying to do. So obviously you have to

01:53:49.080 --> 01:53:53.180
align the data accordingly. Now once that will be done. This

01:53:53.180 --> 01:54:00.880
entire like a process will be done. Then you try to build a

01:54:00.880 --> 01:54:02.920
pipeline for a build

01:54:04.940 --> 01:54:13.320
pipeline for model, right? Now when this will be done,

01:54:17.660 --> 01:54:20.360
setup infra.

01:54:21.780 --> 01:54:29.260
Then when this will be done, start training.

01:54:30.040 --> 01:54:31.660
Then when this will be done,

01:54:34.440 --> 01:54:37.700
monitor. Some will take hours, some will take days, some

01:54:37.700 --> 01:54:42.040
will take month. So monitor, training. Generally we try to

01:54:42.040 --> 01:54:44.800
like do a checkpointing and from there we try to like

01:54:44.800 --> 01:54:48.700
observe it. And we try to even do a callbacks. So early

01:54:48.700 --> 01:54:51.700
stopping basically, training.

01:54:53.520 --> 01:54:58.720
And when you are doing a monitoring training, then once your

01:54:58.720 --> 01:55:02.600
model will stop changing the behavior, means it will start

01:55:02.600 --> 01:55:04.880
decreasing the loss. That is something called as stop

01:55:04.880 --> 01:55:10.400
changing the behavior. In AI. So then you take a frozen

01:55:10.400 --> 01:55:15.980
wait, frozen wait, which is nothing but a fixed wait, which

01:55:15.980 --> 01:55:22.200
is not changing at all. And then you try to do a testing. If

01:55:22.200 --> 01:55:27.280
result is up to you, it's fine. If result is not up to you,

01:55:27.340 --> 01:55:31.600
so the result that you are receiving and the result like

01:55:31.600 --> 01:55:35.000
that your client or maybe you were looking for. If that is

01:55:35.000 --> 01:55:38.040
not working. If that is not good, then sometime we try to

01:55:38.040 --> 01:55:41.460
start again from here, data pre-processing augmentation or

01:55:41.460 --> 01:55:45.760
maybe and after that, so we try to again fall back here and

01:55:45.760 --> 01:55:48.520
we start from here. So start training and again same loop

01:55:48.520 --> 01:55:52.180
after doing all the corrections and parameter again plays an

01:55:52.180 --> 01:55:56.120
important role over there, testing. And once this will be

01:55:56.120 --> 01:56:03.320
done, then maybe we can go ahead with RLHF so that we can

01:56:03.320 --> 01:56:09.780
try to tweak the model. Or tune the model based on the

01:56:09.780 --> 01:56:12.640
behavior which I am looking for, based on the tone which I

01:56:12.640 --> 01:56:16.260
am looking for and stop all the harmful things out of the

01:56:16.260 --> 01:56:22.700
model or maybe DPO. And then if I am happy with the result,

01:56:22.840 --> 01:56:25.840
then what I will do, so I will try to go ahead with the

01:56:25.840 --> 01:56:26.380
distribution.

01:56:29.780 --> 01:56:32.300
Distribution means a final deployment pipeline. So again

01:56:32.300 --> 01:56:35.560
deployment pipeline will be different. So this is the

01:56:35.560 --> 01:56:38.620
overall cycle. In general again, I am not saying that it is

01:56:38.620 --> 01:56:41.180
a thumb rule, but yeah, in general cycle you will be able to

01:56:41.180 --> 01:56:45.060
follow. And this is not just a month long cycle. This is 6

01:56:45.060 --> 01:56:49.060
to 8 months or sometime years long cycle for all of us. And

01:56:49.060 --> 01:56:51.940
for some of us, it is going to be a repetitive cycle. We

01:56:51.940 --> 01:56:55.420
just keep on getting into the loop again and again and again

01:56:55.420 --> 01:56:58.000
and again. Depends upon the use cases for which I am

01:56:58.000 --> 01:57:00.080
working. So is this fine guys?

01:57:04.310 --> 01:57:07.090
So will there be data engineering team? Obviously. So data

01:57:07.090 --> 01:57:09.070
preparation, data engineering team, data science team,

01:57:09.150 --> 01:57:14.210
everyone will be involved all over there. Yeah. So hope this

01:57:14.210 --> 01:57:14.850
is making sense.

01:57:20.940 --> 01:57:25.060
So problem framing, data mining, cleaning, formatting and

01:57:25.060 --> 01:57:29.040
again annotation, then like a evaluation, packaging,

01:57:29.240 --> 01:57:33.240
serving, regression testing, documentation again. So

01:57:33.240 --> 01:57:37.180
distribution, then after that again documentation comes into

01:57:37.180 --> 01:57:41.900
a picture, handover comes into a picture, right? So then you

01:57:41.900 --> 01:57:48.480
are going to do a handover. Yeah. Yeah. Handover. So many

01:57:48.480 --> 01:57:51.820
things. So just a process that we try to follow across the

01:57:51.820 --> 01:57:54.700
industry. But yeah, this is something that we try to follow

01:57:54.700 --> 01:57:59.800
all the time, even in a real time. Yeah. Even in a real

01:57:59.800 --> 01:58:05.600
time. So hope this is going to give you a good idea. This is

01:58:05.600 --> 01:58:08.760
going to give you the very, very good idea. Now if I'll talk

01:58:08.760 --> 01:58:13.680
about a team, by the way, so if I'll talk about the team, so

01:58:13.680 --> 01:58:16.480
who will be the part of this particular team? The team of

01:58:16.480 --> 01:58:19.340
like this entire system that you are going to train or

01:58:19.340 --> 01:58:25.340
build. So there will be a lead MLE or you can say MLE

01:58:27.070 --> 01:58:31.390
lead, machine learning engineer lead you can say. Nowadays,

01:58:31.470 --> 01:58:34.990
generally I engineer also, right? So there will be a lead.

01:58:35.170 --> 01:58:38.530
Maybe there is only one lead. Above this lead, there will be

01:58:38.530 --> 01:58:40.870
a manager. Above this manager, there will be a director. So

01:58:40.870 --> 01:58:43.750
I'm not writing that hierarchy, by the way, right? So there

01:58:43.750 --> 01:58:47.190
will be one lead and there will be a DE, let's suppose.

01:58:47.250 --> 01:58:51.430
Okay. So there will be, let's suppose a three DE I have. So

01:58:51.430 --> 01:58:54.110
DE wise. Okay. I have written here, but yeah, let's write it

01:58:54.110 --> 01:58:56.730
on here. So three D, DML data engineer, by the way, right?

01:58:56.950 --> 01:59:00.410
So there will be a data engineer and then there will be a

01:59:00.410 --> 01:59:02.270
domain expert, domain

01:59:06.050 --> 01:59:08.170
expert or, and

01:59:12.140 --> 01:59:16.760
annotator who can annotate the data, basically a person who

01:59:16.760 --> 01:59:20.220
understands the, like, you know, the data set. Because let's

01:59:20.220 --> 01:59:21.780
suppose if I'm trying to build something in Sanskrit,

01:59:21.960 --> 01:59:24.900
something in Tamil, Telugu language. So obviously I need a

01:59:24.900 --> 01:59:27.200
person who understands that part. And again, in that

01:59:27.200 --> 01:59:29.720
category, I need a person who understands a particular

01:59:29.720 --> 01:59:34.220
domain, right? So I need that kind of a people by the way,

01:59:34.280 --> 01:59:38.720
right? So here I need a huge team, right? I need a huge

01:59:38.720 --> 01:59:41.580
team. Generally we try to outsource, but yeah. So let me

01:59:41.580 --> 01:59:45.480
write down as a four to five people, right? So who is

01:59:45.480 --> 01:59:48.220
working day and night, day and night, day and night. Then

01:59:48.220 --> 01:59:52.120
there will be a project manager, right? So there will be,

01:59:52.160 --> 01:59:56.040
let's suppose. One project manager, I'm going to write down,

01:59:56.180 --> 02:00:02.900
and then you can say there will be a gen AI engineer, right?

02:00:02.960 --> 02:00:06.260
Who understands the actual architecture of the model, the

02:00:06.260 --> 02:00:08.600
elements that you are going to use. They understand the

02:00:08.600 --> 02:00:11.060
differences in between the understands even a domain, some

02:00:11.060 --> 02:00:13.960
subject matter expert as well. That particular person is a

02:00:13.960 --> 02:00:16.980
person who have already trained such kind of a model for

02:00:16.980 --> 02:00:20.400
that kind of a scale. So maybe I can say there will be three

02:00:20.400 --> 02:00:24.340
gen AI engineer in general. You will be able to find out,

02:00:24.440 --> 02:00:28.460
there will be a ops guy. So who can maintain and manage. So

02:00:28.460 --> 02:00:33.100
maybe one to two ops guy will be there. Yeah. One, two or

02:00:33.100 --> 02:00:37.940
two ops guy will be there. There will be a QA guy, which

02:00:37.940 --> 02:00:41.000
will be a part of team. So again, one to two QA person, you

02:00:41.000 --> 02:00:44.280
can try to like a use it. If you are building the full stack

02:00:44.280 --> 02:00:48.020
application, website and app and everything. So maybe a full

02:00:48.020 --> 02:00:51.820
stack person who understands build development. By the way,

02:00:51.860 --> 02:00:57.680
more mean or less. So maybe I can keep three to four people

02:00:57.680 --> 02:01:02.780
over there, right again. So we keep on increasing and

02:01:02.780 --> 02:01:05.680
decreasing person in this one. This is a big task. This is

02:01:05.680 --> 02:01:08.520
like a little bit of boring task. You have to go through all

02:01:08.520 --> 02:01:11.000
the data and then you have to label it. So we keep on

02:01:11.000 --> 02:01:13.520
increasing and decreasing based on sometimes we try to

02:01:13.520 --> 02:01:15.440
outsource it. That okay, fine. So they are the domain

02:01:15.440 --> 02:01:20.220
expert. Let them do their own work. So this is the people

02:01:20.220 --> 02:01:23.420
distribution. You can try to fit in yourself. Over here.

02:01:23.720 --> 02:01:27.720
Yeah, over here. So this is all about a fine tuning guys.

02:01:27.840 --> 02:01:31.680
Now I have already built a demo. It will be better if you

02:01:31.680 --> 02:01:36.060
can run it in your own system and then test it. So here I

02:01:36.060 --> 02:01:38.860
have already like a created a master control panel and plus

02:01:38.860 --> 02:01:42.500
inside a code you will be able to find out a readme file. So

02:01:42.500 --> 02:01:46.500
I have properly maintained a readme file. So just do like a

02:01:46.500 --> 02:01:50.100
pip install requirement.txt and then in a readme file, it's

02:01:50.100 --> 02:01:53.060
already written then execute the code. Streamlit, it's a

02:01:53.060 --> 02:01:55.340
streamlit code so streamlit run and the file name which I

02:01:55.340 --> 02:01:57.820
have given even command is given inside the readme file. So

02:01:57.820 --> 02:02:01.560
you can try to execute it and here the demo which I'm giving

02:02:01.560 --> 02:02:05.000
you which you can keep it inside your GitHub as a proof of

02:02:05.000 --> 02:02:08.520
concept that you have done something is this one. So LoRa

02:02:08.520 --> 02:02:12.020
fine tuning. So LoRa rank adoption fine tuning basically,

02:02:12.060 --> 02:02:14.320
which is a part of prefit, parameter efficient fine tuning.

02:02:14.520 --> 02:02:18.460
So here you can try to select any of these models and then

02:02:18.460 --> 02:02:22.600
you can try to increase or decrease a rank. This rank is

02:02:22.600 --> 02:02:24.800
nothing but a metric size that we try to run in parallel.

02:02:25.680 --> 02:02:30.660
LoRa alpha again, it's a parameter right and dropout. So how

02:02:30.660 --> 02:02:34.540
much we should like choose how much we should lose. So these

02:02:34.540 --> 02:02:36.860
are the parameter which is required. So whenever you are

02:02:36.860 --> 02:02:39.820
trying to do a LoRa training and then you know, training

02:02:39.820 --> 02:02:42.320
epoch for how many epoch you are training. I believe you all

02:02:42.320 --> 02:02:44.620
are aware about these parameters because this is something

02:02:44.620 --> 02:02:47.420
that we have been using with all neural networks, right? Bad

02:02:47.420 --> 02:02:50.420
size, your learning rate, your warm up ratio, your weight

02:02:50.420 --> 02:02:53.320
decay, maximum gradient, normal. So all of these things you

02:02:53.320 --> 02:02:55.520
can try to increase and decrease from here itself. It will

02:02:55.520 --> 02:02:58.340
increase and decrease in a parameter and then you can try to

02:02:58.340 --> 02:03:01.400
click on retrain. As of now, the folder which I'm going to

02:03:01.400 --> 02:03:04.480
share with all of you over there, you will be able to find

02:03:04.480 --> 02:03:07.820
out a weights, a default weights by the way. Similarly it

02:03:07.820 --> 02:03:10.060
goes for a Quora. So I have given you a multiple model

02:03:10.060 --> 02:03:14.220
option, right? Then inference testing and for like a, you

02:03:14.220 --> 02:03:16.640
can get, try to like choose the model. So whatever model

02:03:16.640 --> 02:03:18.360
that you are going to build, you can choose the model from

02:03:18.360 --> 02:03:21.120
here, which is coming from a directory itself. Which is

02:03:21.120 --> 02:03:23.060
already, I have fine-tuned and which is coming from the

02:03:23.060 --> 02:03:25.240
directory and then you can test it by generating the

02:03:25.240 --> 02:03:28.800
response, yeah, generating the response. So this is one of

02:03:28.800 --> 02:03:30.880
the example and it's a big code based, which I have

02:03:30.880 --> 02:03:33.440
prepared. So once you will see it, you will get to know

02:03:33.440 --> 02:03:36.340
about it. So you can try to use it and you will be able to

02:03:36.340 --> 02:03:39.600
like find out the relevance with respect to like entire

02:03:39.600 --> 02:03:42.780
things, which I taught you. Plus embedding and re-ranking

02:03:42.780 --> 02:03:46.100
another like a, it's the two project, but I have just, this

02:03:46.100 --> 02:03:48.300
is again two project, Laura and Quora, different, different,

02:03:48.340 --> 02:03:51.740
but I have just merged it in one interface again for this

02:03:51.740 --> 02:03:54.900
one. So you can do a data preparation. So here the data

02:03:54.900 --> 02:03:56.940
provision, which I was talking about query, positive,

02:03:56.980 --> 02:03:59.800
negative data preparation, you will be able to find out even

02:03:59.800 --> 02:04:02.360
in this one, I have already prepared the data and in a

02:04:02.360 --> 02:04:05.160
similar manner, the manner which I taught you in today's

02:04:05.160 --> 02:04:07.400
class. So you will be able to see the data as well. And

02:04:07.400 --> 02:04:10.380
technically I have used water data. So actually I have used

02:04:10.380 --> 02:04:14.680
my own class data. So basically I, I'm giving like a lot of

02:04:14.680 --> 02:04:17.340
classes, right? Saturday, Sunday and other days. So

02:04:17.340 --> 02:04:20.540
basically what, what I have done. So I have taken the, like

02:04:20.540 --> 02:04:26.600
basically captions of my own class and then I have converted

02:04:26.600 --> 02:04:29.540
that into a training data and then that data you will be

02:04:29.540 --> 02:04:32.500
able to find out by the way. Yeah. So dataset wise, I have

02:04:32.500 --> 02:04:34.360
not downloaded any kind of a data from any GitHub

02:04:34.360 --> 02:04:38.240
repository. It's my own data by the way. Yeah. Although you

02:04:38.240 --> 02:04:40.660
will be able to find out a lot of discrepancy with respect

02:04:40.660 --> 02:04:44.720
to a labels because I have not spent time on labeling. It's

02:04:44.720 --> 02:04:48.340
a very like a lengthy task, right? To read all those

02:04:48.340 --> 02:04:51.720
records. And I had to prepare like a thousands of records to

02:04:51.720 --> 02:04:56.440
build this example. So I, but yeah, dataset is mine, my

02:04:56.440 --> 02:05:01.080
class data by the way, means this like a classes which I'm

02:05:01.080 --> 02:05:04.800
giving. So it's a, like a caption of that data. If you'll

02:05:04.800 --> 02:05:06.780
read out the data, you will be able to understand that part

02:05:06.780 --> 02:05:10.340
as well. So here embedding, you can try to come, you can try

02:05:10.340 --> 02:05:13.080
to, you don't even like evaluate it. You can try to generate

02:05:13.080 --> 02:05:17.200
the embeddings by the way, by selecting the models. So just

02:05:17.200 --> 02:05:19.480
like a embedding. Using this numbers, you will be able to

02:05:19.480 --> 02:05:22.120
generate vectors. You will be able to generate, and then

02:05:22.120 --> 02:05:25.840
these are the models. So model wise, all mini LM L6 V2 I'm

02:05:25.840 --> 02:05:29.060
using for embedding and for re-ranker. So I'm using this

02:05:29.060 --> 02:05:32.080
one. And again, you will be able to generate all those

02:05:32.080 --> 02:05:35.140
analytics. So this is analytics based on my training, by the

02:05:35.140 --> 02:05:38.100
way, plus I have created a logging system as well. So where

02:05:38.100 --> 02:05:40.820
you can check all the label of log, all info, success,

02:05:40.920 --> 02:05:43.620
warning, error, whatever log system will give you all of

02:05:43.620 --> 02:05:46.660
these log, it is capturing and I'm showing it in a UI. Yeah.

02:05:46.760 --> 02:05:49.320
Yeah. I'm showing it everything into the UI. So this is one

02:05:49.320 --> 02:05:51.260
of the beautiful system. You can try to take this snapshot

02:05:51.260 --> 02:05:53.820
and nowhere I have used your own, by the way, if you would

02:05:53.820 --> 02:05:57.400
like to tag me, tag me, if you don't, it's again, fine with

02:05:57.400 --> 02:06:00.140
me at the end of the day, you should get the benefit as a

02:06:00.140 --> 02:06:03.740
student. So you can just try to like, you know, prepare the

02:06:03.740 --> 02:06:08.920
best, you know, like a read me file, attach it, maintain it

02:06:08.920 --> 02:06:12.020
inside of GitHub, share with everyone that, okay, if anyone

02:06:12.020 --> 02:06:14.480
would like to like use my code and then you can flaunt it, I

02:06:14.480 --> 02:06:15.980
don't have any issue. Yeah.

02:06:19.870 --> 02:06:19.890
So there's a lot of things that you can do. And again, it

02:06:19.890 --> 02:06:20.770
depends on your organization, from where you took the data.

02:06:20.890 --> 02:06:24.350
So like I said, caption data, right? My own videos, caption

02:06:24.350 --> 02:06:25.490
our

02:06:29.360 --> 02:06:31.800
list starts from which one that depends upon your year of

02:06:31.800 --> 02:06:34.440
experience. Right? If you are fresher, people will just ask

02:06:34.440 --> 02:06:37.640
you to go and sit for a rotation or maybe a, you know, do

02:06:37.640 --> 02:06:39.760
the vetting for orientation or sit with the rotation team

02:06:39.760 --> 02:06:42.620
day and night and then like annotate thousands of data. This

02:06:42.620 --> 02:06:48.120
is what we do with the fresher or, you know, uh, interns, we

02:06:48.120 --> 02:06:50.080
don't allow them to write a code. We don't have a trust on

02:06:50.080 --> 02:06:55.400
them. So basically we'll send you for the rotation. Okay. so

02:06:55.400 --> 02:06:58.200
if you are fresher you will fit in that place if you are

02:06:58.200 --> 02:07:00.660
like a little bit of experience even six month or one year

02:07:00.660 --> 02:07:03.600
of experience if you have shown uh like a skills basically

02:07:03.600 --> 02:07:06.220
right if you have proven yourself in previous company or

02:07:06.220 --> 02:07:08.800
maybe in a current company itself then obviously you will be

02:07:08.800 --> 02:07:12.900
a part of other blocks which i have created yeah if you are

02:07:12.900 --> 02:07:15.140
good with talking if you are good with explanation maybe you

02:07:15.140 --> 02:07:17.380
will be a part of your like a client discussions as well

02:07:17.380 --> 02:07:20.640
even as a junior right team loves it who knows technical who

02:07:20.640 --> 02:07:23.040
can understand the business i have seen many people even

02:07:23.040 --> 02:07:26.000
when i was like working so i was a part of that team and

02:07:26.000 --> 02:07:29.220
then i used to do the same thing right uh even when i was

02:07:29.220 --> 02:07:31.280
junior like i'm talking about these things like nine or ten

02:07:31.280 --> 02:07:35.820
years back so yeah everything depends upon your skill set

02:07:35.820 --> 02:07:40.020
like how much you are able to get freshers freshers freshers

02:08:15.260 --> 02:08:21.160
i have seen freshers right who is contributing across the

02:08:21.160 --> 02:08:25.380
project and i have seen freshers freshers means two three

02:08:25.380 --> 02:08:27.480
year experience people i have seen right even i have seen

02:08:27.480 --> 02:08:29.940
six year experience people seven experience people they're

02:08:29.940 --> 02:08:33.140
literally they're surviving i don't know how right and

02:08:33.140 --> 02:08:35.380
especially you will be able to find out these come but these

02:08:35.380 --> 02:08:40.220
kind of people in like i should not say but i think we all

02:08:40.220 --> 02:08:43.460
know the reality so it's not a hidden fact which just i'm

02:08:43.460 --> 02:08:47.320
talking about majorly in tcs enforces b pro all those ways

02:08:47.320 --> 02:08:49.740
so that's the reason that those companies never gives a and

02:08:49.740 --> 02:08:52.200
those who is having a talent uh they will leave a talent

02:08:52.220 --> 02:08:54.800
even though even like i was in b pro right but i left the

02:08:54.800 --> 02:08:57.300
company after like one and a half year of experience right

02:08:57.300 --> 02:09:00.960
actually my reason was different when i was like uh i helped

02:09:00.960 --> 02:09:03.280
the company because they were giving me on site and i was

02:09:03.280 --> 02:09:05.540
not interested in on site i was looking for like a bangalore

02:09:05.540 --> 02:09:08.900
location i was in chennai no i was in hyderabad i think at

02:09:08.900 --> 02:09:11.180
that point of time yeah i was looking for bangalore location

02:09:11.180 --> 02:09:16.140
so and they were giving me like a london there was a project

02:09:16.140 --> 02:09:19.040
called a southern water project which was uh like a british

02:09:19.040 --> 02:09:22.700
project london uh based project and they were sending me

02:09:22.700 --> 02:09:25.280
there for three year and i said no i have to do something

02:09:25.280 --> 02:09:29.420
here so put down the paper same thing i did in a deloitte

02:09:29.420 --> 02:09:33.440
they were sending me dublin ireland so i said okay five

02:09:35.680 --> 02:09:39.900
year lalaj to company

02:09:39.900 --> 02:09:42.480
degi nahi degi wo to apke lene ke par depend karta hai na

02:09:42.480 --> 02:09:45.640
agar apka aspiration kuch haur hai if you are working uh

02:09:45.640 --> 02:09:50.480
hard and if you have your own goal then ap company ek goal

02:09:50.480 --> 02:09:53.940
pura nahi karoge then apna goal pura karoge apke baas ke koi

02:09:53.940 --> 02:09:55.840
goal hi nahi karoge hai toh company toh goal degi company

02:09:55.840 --> 02:09:59.860
manager director apke junior apke senior sab apko goal degi

02:09:59.860 --> 02:10:03.220
agar apka khud ka goal degi hai toh agar apka khud ka goal

02:10:03.220 --> 02:10:07.240
hai toh aap apne goal par kaam kuroge baat ke nahi hai toh

02:10:07.240 --> 02:10:10.360
koi deta nahi hai depends upon ki how we take it right

02:10:11.640 --> 02:10:16.020
situation paida nahi hota hai it's our choice to live in

02:10:16.020 --> 02:10:18.180
that situation or leave that situation

02:10:22.030 --> 02:10:26.450
yeah depends upon you so that's all i will uh share this

02:10:26.450 --> 02:10:31.130
code base inside your uh uh dashboard by the way so once i

02:10:31.130 --> 02:10:33.070
will upload the recording so whenever recording will be

02:10:33.070 --> 02:10:36.690
available i'll approve this code base uh and then maybe you

02:10:36.690 --> 02:10:39.410
can just go through the code base once to run this code with

02:10:39.410 --> 02:10:41.770
very easy stream it run and then the file name which i have

02:10:41.770 --> 02:10:44.810
given inside the readme file and you will be able to run it

02:10:44.810 --> 02:10:48.470
so that's all about today's class guys if you have any

02:10:48.470 --> 02:10:50.910
question you can raise your hand and if you don't have a

02:10:50.910 --> 02:10:54.710
question so we can conclude the class now yeah so any

02:10:54.710 --> 02:10:56.990
question yeah we've been good if you have any question

02:10:56.990 --> 02:11:00.300
please do not hesitate to ask any question in the chat box

02:11:00.300 --> 02:11:02.280
yeah you have a question we've been or no one is having any

02:11:02.280 --> 02:11:04.700
question class was great yeah yeah sorry i forgot to ask how

02:11:04.700 --> 02:11:07.880
was the class guys yeah

02:11:10.480 --> 02:11:15.860
okay thank you class was great awesome thanks yeah uh vipin

02:11:15.860 --> 02:11:21.720
you have question i think you are on mute okay so nares go

02:11:21.720 --> 02:11:25.260
ahead with the question yeah hi sudhanshu hi go ahead so

02:11:25.260 --> 02:11:30.220
coming to this uh fine tuning right so when i when i uh

02:11:30.220 --> 02:11:31.880
browsing this hugging field i'm going to go ahead and i'm

02:11:31.880 --> 02:11:31.880
going to go ahead and i'm going to go ahead and i'm

02:11:36.280 --> 02:11:38.080
going to go ahead and i'm going to go ahead and touch it is

02:11:38.080 --> 02:11:43.800
that

02:11:43.800 --> 02:11:45.560
um i actually wanted to demand that one but that was just on

02:11:45.560 --> 02:11:46.080
the paper like one answer that was given in the previous

02:11:46.080 --> 02:11:48.380
class like it has been three questions that have with

02:11:48.380 --> 02:11:49.800
respect to the columns so i want to add to that uh the one

02:11:49.800 --> 02:11:51.000
part of the question is that in most tests in the days which

02:11:51.000 --> 02:11:51.080
you go througharn uh can um teach or can say that is

02:11:51.080 --> 02:11:51.320
actually difficult to get you need to you are already

02:11:51.320 --> 02:11:51.320
engaged in what it is which is ropes which means you have

02:11:51.320 --> 02:11:51.720
gets ctg right cell there is um you know man that you can

02:11:51.720 --> 02:11:56.700
fix your vipin text or something which i i i think it's

02:11:56.700 --> 02:12:00.660
a factual problem so you can attach on this cl moment or

02:12:00.660 --> 02:12:04.200
kick on the Again, Lama model, if I'll talk about, so there

02:12:04.200 --> 02:12:07.900
is a base model, for example, GPT. So GPT is basically GPT

02:12:07.900 --> 02:12:10.100
two or one was basically a base model. And then we have

02:12:10.100 --> 02:12:13.980
started, they have started using the series of the model. So

02:12:13.980 --> 02:12:16.560
for example, I'll talk about like a, just a recent example.

02:12:16.760 --> 02:12:20.340
So you must have heard about this GPT OSS 20B, right? And

02:12:20.340 --> 02:12:24.680
then GPT OSS 120B. Yeah. So basically we have a GPT even

02:12:24.680 --> 02:12:28.440
with some one beam parameter model. And then variant of that

02:12:28.440 --> 02:12:32.540
is 20B, 120B. Yes. So basically my base model is that one.

02:12:32.620 --> 02:12:34.860
And then that is the instruction based model, which where I

02:12:34.860 --> 02:12:37.780
have given like a more data and then size is big, parameter

02:12:37.780 --> 02:12:41.440
is big, and we have trained it. That's our definition, by

02:12:41.440 --> 02:12:45.440
the way. But yeah, that's just a definition. And Sudhanshu,

02:12:45.560 --> 02:12:48.580
I'm not coming from a data science background. So basic

02:12:48.580 --> 02:12:53.700
question, like what is model extensions? Like ONNX, I have

02:12:53.700 --> 02:12:57.340
seen. So basically it depends upon the library. Most of the

02:12:57.340 --> 02:13:02.280
time, for example, if I'm using maybe a, like a, let's

02:13:02.280 --> 02:13:04.940
suppose a tensor flow based libraries, tensor based

02:13:04.940 --> 02:13:07.460
libraries. So maybe I'll end up creating a PV model prod

02:13:07.460 --> 02:13:11.080
buff, basically. Or maybe I'll end up creating a dot Keras

02:13:11.080 --> 02:13:16.100
model, or I will creating maybe a MD5 model or P5 model or

02:13:16.100 --> 02:13:21.280
H5 model. Or if I'm using a ONG NX library, which is again a

02:13:21.280 --> 02:13:23.600
deep learning based neural network library. So in that case,

02:13:23.640 --> 02:13:26.800
I will end up building a ONX based model. Basically it's a

02:13:26.800 --> 02:13:30.720
library and then model format in which we are storing. The

02:13:30.720 --> 02:13:36.960
weights. Oh, got it. Thanks. Yeah. Okay. Yeah. Ranji, please

02:13:36.960 --> 02:13:41.640
go ahead. Yeah. Yeah. I was, it's very interesting. Like I

02:13:41.640 --> 02:13:46.260
had no idea what is fine tuning will involve. And this is

02:13:46.260 --> 02:13:50.960
very detailed, uh, no, sir. So you will be able to find out,

02:13:50.960 --> 02:13:54.020
you will be able to find out more detail in my, uh, like a

02:13:54.020 --> 02:13:57.620
practical lectures here. I'm just covering everything with,

02:13:57.620 --> 02:14:00.220
with respect to revision. I'm assuming that people are.

02:14:00.220 --> 02:14:02.580
Already aware about it. Right. It's an interview patch by

02:14:02.580 --> 02:14:05.700
the way. Right. So I'm not focusing on core because, uh,

02:14:05.820 --> 02:14:09.260
when I talked about these, uh, like things. So I have also

02:14:09.260 --> 02:14:12.120
talked about like a mathematics, uh, preferred, uh, Cora,

02:14:12.160 --> 02:14:14.960
Laura, all those mathematics behind it in my previous. Yeah,

02:14:15.020 --> 02:14:18.560
please go ahead. Yeah, I will go over those and see, because

02:14:18.560 --> 02:14:23.160
it interests me. So, uh, how much data have you used for

02:14:23.160 --> 02:14:26.100
this application? I mean, the size of the data, which one

02:14:26.100 --> 02:14:30.400
for this, which I'm sharing with you. Yes. Approximately. I

02:14:30.400 --> 02:14:33.360
think for my embedding, there are 1600 records, which I have

02:14:33.360 --> 02:14:37.420
created. Uh, if I remember correctly, I can go and I can

02:14:37.420 --> 02:14:40.320
like, uh, you know, uh, count it, but yeah, approximately

02:14:40.320 --> 02:14:46.060
1600, I believe, which I remember. Right. But again, 1600 is

02:14:46.060 --> 02:14:49.720
not enough. Uh, like I said, I'm training these things in a

02:14:49.720 --> 02:14:52.260
local, so the benchmarking, which I have given you with

02:14:52.260 --> 02:14:55.680
respect to data, go with that, right. For this one, 1600, I

02:14:55.680 --> 02:14:58.880
have used and, uh, for this one embedding and, uh, re

02:14:58.880 --> 02:15:04.900
-ranking and for. This one, uh, for my fine tuning, uh, I

02:15:04.900 --> 02:15:13.520
have used around yeah. 1,692 data, data samples, the size 1

02:15:13.520 --> 02:15:19.080
,692. It's a, it's a very big effort. No, it's, it's not a

02:15:19.080 --> 02:15:21.860
big record. It's a very small record. No, no. I mean, uh,

02:15:21.900 --> 02:15:26.200
not from a size. I mean, your size, but to basically go over

02:15:26.200 --> 02:15:29.640
this complete fine tuning from end to end for any

02:15:29.640 --> 02:15:33.140
enterprise. Right. It's the big efforts. Um, yeah, I'm like,

02:15:33.180 --> 02:15:34.880
you just had to increase the data and then you can follow

02:15:34.880 --> 02:15:37.860
the same pipeline, then do it the way I have done that. Like

02:15:37.860 --> 02:15:40.020
you don't have to do anything. Actually. You just have to

02:15:40.020 --> 02:15:43.500
increase the number of data, uh, format also have given, uh,

02:15:43.560 --> 02:15:45.680
even I have given the script inside this project that how to

02:15:45.680 --> 02:15:49.040
prepare in that format basically. But yeah, on rotation, you

02:15:49.040 --> 02:15:51.620
have to do, uh, by the way, on rotation is a manual task.

02:15:51.840 --> 02:15:54.680
You can't like a skip that particular task. So on rotation,

02:15:54.820 --> 02:15:58.080
you have to do, and then like use the same thing, your model

02:15:58.080 --> 02:16:01.060
will be trained. And again, you have. Uh, if you have to use

02:16:01.060 --> 02:16:05.060
it on some GPUs, right, pick the same code, put it on a GPUs

02:16:05.060 --> 02:16:06.980
and you will be able to train. So you don't have to do

02:16:06.980 --> 02:16:11.320
anything. So, so basically, uh, like in the, in your other

02:16:11.320 --> 02:16:15.740
class that you showed us the GPUs, uh, though we may, we can

02:16:15.740 --> 02:16:19.040
use that and have some, a little bit of experience in that.

02:16:19.120 --> 02:16:24.020
Right. I have also, you will be able to find out some in, I

02:16:24.020 --> 02:16:27.760
think, uh, if I remember correctly, I have also mentioned H

02:16:27.760 --> 02:16:31.560
100 Python file. Uh, in embedding, I believe. Yeah. In

02:16:31.560 --> 02:16:34.460
embedding a step. So I have also created in finding out not

02:16:34.460 --> 02:16:37.100
good. Both are fine tuning by the way, but yeah, in one of

02:16:37.100 --> 02:16:41.080
the approach, I have also given you, uh, embedding, uh,

02:16:41.320 --> 02:16:45.340
running this embedding on H 100. So just one Python script,

02:16:45.560 --> 02:16:50.700
put the data around the model. Say the question on the

02:16:50.700 --> 02:16:54.820
application that we, for example, the rag applications, we

02:16:54.820 --> 02:16:58.140
put the data, for example, if that data is a proprietary

02:16:58.140 --> 02:17:02.140
data somewhere, then how can we use those data without

02:17:02.140 --> 02:17:08.040
licensing just even for our trial? No, you can't, we cannot,

02:17:08.080 --> 02:17:12.620
right? No, no, not at all. So, so, uh, so that means even if

02:17:12.620 --> 02:17:17.500
we are doing this education and, uh, but we, we can use

02:17:17.500 --> 02:17:20.680
maybe just for the education, but not for the business

02:17:20.680 --> 02:17:23.400
purpose. Right. No, you can, you can, if you are looking for

02:17:23.400 --> 02:17:26.740
like an entire data science, uh, training based data, uh,

02:17:26.900 --> 02:17:29.820
you can use my entire lecture because I have. Okay. Like,

02:17:29.820 --> 02:17:33.900
uh, more than I believe in our, like a video system. So we

02:17:33.900 --> 02:17:39.840
have more than, uh, 3,500, uh, lectures so far, it's a big

02:17:39.840 --> 02:17:43.260
in itself and covering all the topic end to end. And if

02:17:43.260 --> 02:17:45.800
you're going to convert those data into a transcript, it's a

02:17:45.800 --> 02:17:51.080
huge data. We have, even

02:17:51.080 --> 02:17:55.000
for rag, even for rag, even for rag in a specific one, this

02:17:55.000 --> 02:17:59.560
AI ML only right topic that we have covered. So even for.

02:17:59.720 --> 02:18:03.680
Um, even for anything, rag is not the one case for anything.

02:18:04.060 --> 02:18:07.560
Uh, I have like, I have that data, so I have my own data.

02:18:07.660 --> 02:18:10.200
That's the reason. So even in this example, I have used my

02:18:10.200 --> 02:18:12.880
own data. I have not used like any other data from internet.

02:18:13.760 --> 02:18:18.140
Mm-hmm. Okay. Uh, uh, for example if we are looking for the

02:18:18.140 --> 02:18:21.920
hotel industry or industry where, for example, a restaurant,

02:18:22.120 --> 02:18:26.860
okay? And we have got so many recipes outside in the books.

02:18:26.940 --> 02:18:32.320
We cannot use that. Uh, uh. without getting a license. Which

02:18:32.320 --> 02:18:38.300
one? For example, we are doing POC for a restaurant

02:18:38.300 --> 02:18:46.600
industry, food. And suppose there are recipes available on

02:18:46.600 --> 02:18:50.660
the internet and we cannot just download and do rag on that.

02:18:51.500 --> 02:18:54.620
Yeah, obviously. See, whatever data is public, you can use

02:18:54.620 --> 02:18:56.720
it. Whatever data is not public, you should not use it.

02:18:56.720 --> 02:18:59.520
Although, even though if you are able to access those data,

02:18:59.600 --> 02:19:01.940
but still you should not use it, someday people will come,

02:19:02.060 --> 02:19:05.220
they will file a lawsuit on you. You will be in trouble. As

02:19:05.220 --> 02:19:11.340
simple as that. Right. So, I think that's the answer. Thank

02:19:11.340 --> 02:19:17.180
you. Okay. So, PK, go ahead with your question. Sir, a lot

02:19:17.180 --> 02:19:22.480
of companies... Oh, sorry. I think I put you on... Can you

02:19:22.480 --> 02:19:28.320
please raise your hand once again? Yeah. Go ahead, PK. Yeah.

02:19:28.380 --> 02:19:32.820
A lot of companies do not use LLM of its own and they do not

02:19:32.820 --> 02:19:35.640
build LLMs also. But when it comes to the interview, how

02:19:35.640 --> 02:19:39.160
would an interviewer would justify that you have actually

02:19:39.160 --> 02:19:44.840
worked on an LLM or building an LLM? No one is going to

02:19:44.840 --> 02:19:47.300
check with your manager, your director, or your team member.

02:19:49.550 --> 02:19:55.390
But sir, in healthcare, it becomes very clear. Healthcare is

02:19:55.390 --> 02:19:57.930
a very big industry in itself. That's the first thing. Okay?

02:19:58.250 --> 02:20:00.930
You have to verify what is written in the resume through the

02:20:00.930 --> 02:20:06.070
story. Because only you can verify that story. Your team

02:20:06.070 --> 02:20:08.290
member will not know about the interview. Even your manager

02:20:08.290 --> 02:20:11.410
will not know about it in such a big world. Right? So,

02:20:11.430 --> 02:20:13.350
whatever story you tell, the other person will listen to it

02:20:13.350 --> 02:20:16.370
and will ask a cross-question on it. And look, in courses

02:20:16.370 --> 02:20:20.190
and interviews, basically, if you are lying or telling the

02:20:20.190 --> 02:20:22.810
truth, it does not matter. Okay? Whether you are able to

02:20:22.810 --> 02:20:26.890
prove it or not, it is an interview for knowledge check. So,

02:20:26.990 --> 02:20:29.410
whether you have worked on it and you are telling the story

02:20:29.410 --> 02:20:32.770
or not, it is up to you. Everyone tells it in the interview.

02:20:33.070 --> 02:20:36.290
Like, we do the same thing. We tell about the hospital's

02:20:36.290 --> 02:20:39.630
work in the interview. But sir, if we look at the LLM,

02:20:40.210 --> 02:20:43.690
specifically, if we look at the LLM, not more than 50-60

02:20:43.690 --> 02:20:47.250
LLMs will be available as of today in the market. What are

02:20:47.250 --> 02:20:51.230
you talking about? We will have to learn. LLMs are available

02:20:51.230 --> 02:20:55.630
above 10,000. We are talking about open source. Okay. We are

02:20:55.630 --> 02:20:57.370
talking about open source, again. You will never hear about

02:20:57.370 --> 02:20:58.250
the proprietary LLMs.

02:21:00.950 --> 02:21:03.830
Okay. The proprietary LLMs in the company, the client

02:21:03.830 --> 02:21:05.770
-specific. What we talked about today, we will talk about

02:21:05.770 --> 02:21:10.350
it. Client-specific, product-specific, right? You will never

02:21:10.350 --> 02:21:12.430
hear about those LLMs in the market. You will have to go to

02:21:12.430 --> 02:21:19.690
the team and ask them what their name was. Okay. Now, your

02:21:19.690 --> 02:21:22.150
team has made LLMs, let's suppose, for a particular use

02:21:22.150 --> 02:21:24.730
case, which works for the same client. How would I know the

02:21:24.730 --> 02:21:25.070
name of that LLM?

02:21:28.010 --> 02:21:30.530
Okay. Then, sir, there is no need for a RAG. I can make my

02:21:30.530 --> 02:21:34.550
own LLM in my company and it will work. No. We had said

02:21:34.550 --> 02:21:37.470
earlier, like when we started the lecture, we had said that

02:21:37.470 --> 02:21:39.950
RAG is an approach. Then, there is a fine-tuning approach.

02:21:40.250 --> 02:21:43.230
But why are we using RAG and why fine-tuning? So, RAG is

02:21:43.230 --> 02:21:46.790
where data is frequently changing. Fine-tuning is where data

02:21:46.790 --> 02:21:49.890
is not changing quickly. Okay. Where you have huge knowledge

02:21:49.890 --> 02:21:52.510
based on the last 20 years, 10 years, or the last year.

02:21:52.510 --> 02:21:56.450
Okay. There is Hughes. Obviously, the RAG will be a costly

02:21:56.450 --> 02:22:00.090
operation. But majority of places, we also do hybrid. I

02:22:00.090 --> 02:22:02.470
mean, fine-tuning. I mean, till a particular date on Hughes

02:22:02.470 --> 02:22:04.750
data, train it. And then, the continuous data that I have,

02:22:04.950 --> 02:22:09.670
which is frequently coming to me, I run RAG on it. So,

02:22:09.670 --> 02:22:11.710
basically, combination of both, which is a hybrid approach.

02:22:13.700 --> 02:22:17.160
Okay. Sir, you have increased the confidence. Exactly. To

02:22:17.160 --> 02:22:20.480
lie. No. So, this is what happens in the interview. So, did

02:22:20.480 --> 02:22:21.840
you tell the truth in the last interview? The whole thing.

02:22:23.640 --> 02:22:25.960
Did you tell the truth? No, sir. I told the truth. Exactly.

02:22:26.400 --> 02:22:28.440
See, the interview is the same. The interview is all about

02:22:28.440 --> 02:22:30.160
knowledge testing. Right? There is no fight between truth

02:22:30.160 --> 02:22:33.080
and lie. No one wants to become Haribansaraya Bachchan. I

02:22:33.080 --> 02:22:36.440
mean, not Haribansaraya Bachchan. Raja. Harishchand. Sorry.

02:22:38.640 --> 02:22:42.200
Okay. So, that's a fact. Right? The person in front of you

02:22:42.200 --> 02:22:43.760
will check the knowledge. You are giving your knowledge

02:22:43.760 --> 02:22:45.680
check. That's it. How much you know, how much you don't

02:22:45.680 --> 02:22:47.400
know. Whether you are able to handle cross-question or not.

02:22:47.960 --> 02:22:51.020
Whether you have seen the scenario or not. That is the only

02:22:51.020 --> 02:22:53.180
thing. Okay. That is the only thing. This gives me enough

02:22:53.180 --> 02:22:57.240
motivation to learn all the concepts of LLM building from

02:22:57.240 --> 02:23:00.460
the scratch till the, as far as I am able to justify.

02:23:00.660 --> 02:23:03.020
Tomorrow, when you come to class, bring the DPR of this

02:23:03.020 --> 02:23:06.860
project. The previous one. And everyone. Yes, sir. Yeah.

02:23:07.080 --> 02:23:11.080
See, you will enjoy joining the class. But until you sit and

02:23:11.080 --> 02:23:13.600
work a little, believe me, it becomes difficult to tell that

02:23:13.600 --> 02:23:16.580
story. Okay. But once you write the DPR yourself. That's why

02:23:16.580 --> 02:23:17.940
we keep saying it. By the way, people don't believe it.

02:23:19.040 --> 02:23:21.400
Those who believe it, they take out the job. So, that's it.

02:23:21.420 --> 02:23:25.120
So, basically, once you write the DPR, I think you will be

02:23:25.120 --> 02:23:28.300
like having an edge. You don't have to do revision at the

02:23:28.300 --> 02:23:31.480
last moment. And it will give you a lot of thought actually.

02:23:32.880 --> 02:23:37.300
Okay. Sir, one last question. Sir, all the classes don't

02:23:37.300 --> 02:23:40.700
have transcripts. Sir, I mean, I can directly save a lot of

02:23:40.700 --> 02:23:43.220
time just by going into a specific topic which I want to

02:23:43.220 --> 02:23:45.300
learn. So, then you can ask the chat GPT. Then you won't

02:23:45.300 --> 02:23:47.320
have to join the live class. What is the need? We have

02:23:47.320 --> 02:23:52.190
already given the topic name. Sir, your. You live. Yes, sir.

02:23:52.290 --> 02:23:53.070
That's what I am talking about. Sir, you can ask the chat

02:23:53.070 --> 02:23:55.370
GPT. Then the topic name has already been given in the

02:23:55.370 --> 02:23:58.130
syllabus. Right. You don't have to study the batch. Okay.

02:23:58.130 --> 02:24:00.450
You don't have to ask us. You can ask the chat GPT. Or you

02:24:00.450 --> 02:24:03.270
can ask the URI. That give me 10 examples out of this. 10

02:24:03.270 --> 02:24:04.510
things. If people would have studied that much by

02:24:04.510 --> 02:24:08.650
themselves, then what is the need for our semester? Sir, if

02:24:08.650 --> 02:24:10.450
they don't see your face, they don't understand. Sir.

02:24:10.570 --> 02:24:12.850
Exactly. So, that's it. If they don't see your face, they

02:24:12.850 --> 02:24:13.790
don't understand. So, we take the fee to show our face.

02:24:16.390 --> 02:24:19.650
Okay. Okay, sir. Thank you. Jokes are just a part, guys.

02:24:19.890 --> 02:24:23.690
Don't feel offended anyone. So, we. We are just having a

02:24:23.690 --> 02:24:29.110
very, very casual conversation. Okay, take next question, by

02:24:29.110 --> 02:24:33.070
the way. Nice conversation with PK. Okay. Sir, is Madhuri

02:24:33.070 --> 02:24:37.270
here? Yeah, Madhuri. Please, go ahead. Sir, actually, I had

02:24:37.270 --> 02:24:43.510
attended the sessions of JNAI from outside. I mean, still,

02:24:43.670 --> 02:24:48.130
it is going on. But I am not getting confidence to write the

02:24:48.130 --> 02:24:51.730
projects or to. I mean, I am not getting confidence for

02:24:51.730 --> 02:24:54.910
JNAI. Exactly. Even machine learning ka toh mujhe experience

02:24:54.910 --> 02:24:59.270
nahi hai, but I have learned that. So, if liye waha pe thora

02:24:59.270 --> 02:25:03.010
bho confusion ho raha hai. I am so same. Saturday to Sunday

02:25:03.010 --> 02:25:05.590
wala you JNI class bhai karo na, hum logon hai, I think, 8

02:25:05.590 --> 02:25:08.010
or 10 week hi hua hai. So, we all are able to build a big,

02:25:08.030 --> 02:25:10.350
big application. Kali hai ka application khatam. Yesterday

02:25:10.350 --> 02:25:12.890
itself, we have finished one big application. Literally big

02:25:12.890 --> 02:25:15.150
one. Which you can write directly into your resume. You can

02:25:15.150 --> 02:25:18.230
ask anyone who is a part of my JNI batch. Not this one. This

02:25:18.230 --> 02:25:23.170
is the interview bootcamp, right? So, other one. So, like, I

02:25:23.170 --> 02:25:25.430
don't know where you are joining and why you are not able to

02:25:25.430 --> 02:25:27.870
get the confidence. But batch ke inda toh log kaam bhi kar

02:25:27.870 --> 02:25:29.510
raha hai. Yesterday, I have given them 10 different,

02:25:29.530 --> 02:25:32.030
different projects to work on. And people have already

02:25:32.030 --> 02:25:36.730
started working on that. So, yeah. Ache, kyunki sir, kya ho

02:25:36.730 --> 02:25:39.430
raha hai na, ki, matlab concept tabi raha bhi thore thore

02:25:39.430 --> 02:25:44.650
clear ho raha hai. But I think, concept clear, yeah, I think

02:25:44.650 --> 02:25:47.230
we should not learn for like just clearing a concept because

02:25:47.230 --> 02:25:51.570
our interview, mein phas jaoge. Okay. Interview, mein

02:25:51.570 --> 02:25:53.830
concept check toh nahi hota. Interview toh, aapka reality

02:25:53.830 --> 02:25:56.610
check hota hai ki how much you have worked and basically,

02:25:57.290 --> 02:25:59.850
like, that is not just a knowledge check. Basically, that is

02:25:59.850 --> 02:26:04.610
going to be your experience check. Right. So, sirf concept

02:26:04.610 --> 02:26:07.390
clear rahega, tab toh aapko first line, within like 2-3

02:26:07.390 --> 02:26:09.690
minute, anyone will be able to judge you ki like, you have

02:26:09.690 --> 02:26:12.230
like just learned something. Right. You are just a learner.

02:26:12.350 --> 02:26:14.810
You are not implementer. You are not a developer. You are

02:26:14.810 --> 02:26:18.330
not an actual engineer. Right. So, sirf concept agar aap ja

02:26:18.330 --> 02:26:23.990
rahe toh, I think, you should not do. So, sir, yeh kaunsa

02:26:23.990 --> 02:26:27.570
batch hai aur iska duration kya hai? Gen AI, yeh wala hai,

02:26:27.650 --> 02:26:31.070
aapka Gen AI certification bootcamp chal raha hai. Okay.

02:26:31.210 --> 02:26:33.570
Iska bhi alag se WhatsApp group hai. Isme bhi like, kaafi

02:26:33.570 --> 02:26:36.050
saal log hai. Toh, aap join kar sakte ho. Certification,

02:26:37.110 --> 02:26:39.650
aapko dik jaega, aap jaoge na live class ke andar. Toh,

02:26:39.690 --> 02:26:42.670
wohin par aapko. But, yeh kuch 4 to 6 months ka hai, right?

02:26:43.670 --> 02:26:46.750
4 month, 3 to 1 month already ho chuka hai, usme. 1-1.5

02:26:46.750 --> 02:26:50.370
month. So, I think, more like a 2-2.5 month will go, not

02:26:50.370 --> 02:26:53.310
more than that. But, again, so, we have already discussed a

02:26:53.310 --> 02:26:55.670
lot of project over there. A lot of, like, practical stuff

02:26:55.670 --> 02:26:59.110
we have already discussed. You know, just like a hovering

02:26:59.110 --> 02:27:02.350
over the theories and everything. Okay. And the small, small

02:27:02.350 --> 02:27:07.050
things. Yeah. That's the reason, so, we are selling Elon

02:27:07.050 --> 02:27:10.030
Plus, right? Like, for an entire year, you have, like, you

02:27:10.030 --> 02:27:12.630
don't have to worry about anything. So, yeah. And the

02:27:12.630 --> 02:27:15.870
projects which you are creating, right? So, like, for that,

02:27:15.930 --> 02:27:19.530
you are using the URI as a platform and URI, as an LLM.

02:27:20.250 --> 02:27:22.910
Yeah. So, basically, in that batch, in this one, LLM is not

02:27:22.910 --> 02:27:24.950
required. That project which I have shown you today, so, LLM

02:27:24.950 --> 02:27:27.910
is not required. So, basically, my data set is required. So,

02:27:27.910 --> 02:27:30.110
I have used my own data set from my own video transcript.

02:27:30.330 --> 02:27:33.810
But, yeah, over there in Gen AI, so, you need LLM. Without

02:27:33.810 --> 02:27:37.430
LLM's access, you can't do a Gen AI, by the way. I think you

02:27:37.430 --> 02:27:40.550
are aware about that fact, right? Yes, but we are generally

02:27:40.550 --> 02:27:45.790
using the Open AI or some LLM. You need, basically, Open AI,

02:27:45.830 --> 02:27:48.690
you need Gemini, you need Mistral, you need Lama, you need

02:27:48.690 --> 02:27:52.330
Embedding Model, you need a lot of other different speech

02:27:52.330 --> 02:27:56.810
-based model. In URI, we have all. We have Open AI Model as

02:27:56.810 --> 02:28:01.730
well, Open AI 4.1 Nano, and then we have Open AI 5 Model,

02:28:01.910 --> 02:28:06.490
Open AI 20B Model, Open AI 120B Model, Open AI Text

02:28:06.490 --> 02:28:10.410
Embedding Model. So, basically, URI is not a model itself.

02:28:10.490 --> 02:28:14.350
URI is a platform that we have created as a neuron where you

02:28:14.350 --> 02:28:17.470
can access everything without making a payment or hurdles of

02:28:17.470 --> 02:28:21.470
any other platform. Including Open AI. But then, actually,

02:28:21.530 --> 02:28:24.310
how can we showcase that into our projects? Because the

02:28:24.310 --> 02:28:27.590
companies will not use the URI, right? So, they will go with

02:28:27.590 --> 02:28:30.610
the... Company will not even use Open AI, by the way. Yeah?

02:28:31.350 --> 02:28:34.070
Company will not even use Open AI because of security reason

02:28:34.070 --> 02:28:36.790
and everything. So, basically, that's the reason I have

02:28:36.790 --> 02:28:39.750
taught this week and last week itself how to host your own

02:28:39.750 --> 02:28:44.010
model on H100 GPU. And we were using our own model in a

02:28:44.010 --> 02:28:47.310
secured way. We were doing the inferencing. So, you are

02:28:47.310 --> 02:28:52.190
creating your... own LLM model? No, ma'am. We are hosting

02:28:52.190 --> 02:28:54.110
our own LLM model. Creation is different. Hosting is

02:28:54.110 --> 02:28:58.510
different. Okay. Yeah. I think you should go through those

02:28:58.510 --> 02:29:01.410
like things. Your fundamentals is not clear, I believe.

02:29:01.550 --> 02:29:06.310
Yeah. Okay, ma'am. Thank you so much. Yeah. Thanks. Yeah.

02:29:06.330 --> 02:29:06.890
Next one, please.

02:29:10.270 --> 02:29:13.710
Yeah. Sorry. Go ahead, please. Yeah. Sorry. You're audible.

02:29:13.710 --> 02:29:16.750
You're audible. Sir, I'm saying that you said that

02:29:16.750 --> 02:29:22.190
publishers have book proprietorship. Hmm. So, if they have

02:29:22.190 --> 02:29:26.010
proprietorship and if there's a model that's trained in the

02:29:26.010 --> 02:29:30.590
back-end and they use all those books then there must be a

02:29:30.590 --> 02:29:37.190
tool that can verify that these books are used in the back

02:29:37.190 --> 02:29:40.030
-end. No, basically the information from the chat and the

02:29:40.030 --> 02:29:45.810
tone because some models are the same pattern that you've

02:29:45.810 --> 02:29:49.910
given the data. Right. So, somewhere when a lawsuit is filed

02:29:49.910 --> 02:29:53.070
then the disclosure is told to show me the data. The entire

02:29:53.070 --> 02:29:57.170
one. Right. A lot of whistleblowers are inside the team who

02:29:57.170 --> 02:30:00.810
go out and tell that their data has been used. You must have

02:30:00.810 --> 02:30:03.150
seen that there was a whistleblower in the open AI. Right.

02:30:03.470 --> 02:30:06.670
So, there were all the places. So, because it's not just a

02:30:06.670 --> 02:30:09.590
one person. The CEO won't train by himself. He needs a big

02:30:09.590 --> 02:30:12.550
team for this. Right. Now, in the team there are legal

02:30:12.550 --> 02:30:14.970
contracts and everyone's got it and it's shown a lot of

02:30:14.970 --> 02:30:18.470
things. A lot of people know which property we're using and

02:30:18.470 --> 02:30:20.790
which we're not using. Someone said something wrong in that

02:30:20.790 --> 02:30:23.310
and they got the money from there or they didn't and they

02:30:23.310 --> 02:30:27.390
got the name and fame. So, the publishers don't have such an

02:30:27.390 --> 02:30:33.950
engineer team. Why do publishers need an engineer team? The

02:30:33.950 --> 02:30:37.910
one who is giving the data needs it. Right.

02:30:40.850 --> 02:30:45.490
Yeah. Yeah. So, I'll Yeah. Salary. So, don't show. Yeah. Go

02:30:45.490 --> 02:30:49.070
ahead, please. Yeah. So, don't show me before I come to the

02:30:49.070 --> 02:30:54.850
question. So, don't show me my I just

02:30:54.850 --> 02:30:56.450
want to make sure I attended

02:31:01.550 --> 02:31:04.350
your class. I attended very other very expensive another

02:31:04.350 --> 02:31:07.190
course for six months. Okay. But I couldn't make a

02:31:07.190 --> 02:31:10.250
transition data science. Your class you helped me to make

02:31:10.250 --> 02:31:13.090
the transition. So, if my best audience go with that, I'll

02:31:13.090 --> 02:31:18.830
tell you a rest assured. I'm not paid customer but I'm a

02:31:18.830 --> 02:31:21.270
devotee of I'm not a devotee of I mean, I've covered a lot.

02:31:21.730 --> 02:31:24.530
I've covered a lot. And even Sudhanshu, what you taught

02:31:24.530 --> 02:31:27.610
there, even the industry and sometimes some companies are

02:31:27.610 --> 02:31:32.670
not there yet. They're going with the project. I was just

02:31:32.670 --> 02:31:35.830
understanding. Projects were very advanced. There were not

02:31:35.830 --> 02:31:40.530
small projects. I mean, all projects were there somewhere. I

02:31:40.530 --> 02:31:44.850
mean, 99% projects were those that we did ourselves in

02:31:44.850 --> 02:31:51.850
Deloitte, Verizon, ENY, Unilever. Actually, it was the same

02:31:51.850 --> 02:31:55.930
project. Sir, all of them were our projects. Because we

02:31:55.930 --> 02:32:00.350
became a lead and we were running with 10 projects. And when

02:32:00.710 --> 02:32:06.250
interest, your managers say, take 4 more teams. And I was

02:32:06.250 --> 02:32:10.410
okay with that because I was in the learning phase. So, I

02:32:10.410 --> 02:32:12.770
had to learn, I had to explore as much as possible. And

02:32:12.770 --> 02:32:16.090
anyway, I didn't have any work in life. So, I was like a

02:32:16.090 --> 02:32:19.450
kind of, you know, a kind of a free person back then, like

02:32:19.450 --> 02:32:23.370
five, six years back. I only had work to do, but I didn't

02:32:23.370 --> 02:32:26.770
have to do. Then, what happened was that all those projects

02:32:26.770 --> 02:32:28.910
came, I learned them, got them done, got them code-based and

02:32:28.910 --> 02:32:32.250
then I have just refactored everything and then started all

02:32:32.250 --> 02:32:36.230
the students. Yeah, that was very helpful, Sardanshi. Thanks

02:32:36.230 --> 02:32:38.710
for that. But yeah, I'll quickly come to my questions,

02:32:38.810 --> 02:32:42.050
Sardanshi. I have a good amount of experience in the

02:32:42.050 --> 02:32:47.290
industry as well. I work in a startup and then I moved to a

02:32:47.290 --> 02:32:52.230
startup. But now, I belong to a small two-tier city in

02:32:52.230 --> 02:32:58.650
Karnataka. So, my friends found out that I work in AI. Now,

02:32:58.710 --> 02:33:01.530
they are asking me to make a presentation on how to make a

02:33:01.530 --> 02:33:04.430
transition. They have invited me to a guest lecture in their

02:33:04.430 --> 02:33:08.590
college. But these people know that there is a huge gap in

02:33:08.590 --> 02:33:12.050
the syllabus they are covering and when they are making a

02:33:12.050 --> 02:33:17.190
transition to Gen AI. How do I structure a lecture in Gen

02:33:17.190 --> 02:33:21.150
AI? It is a diverse topic and the volume of information is

02:33:21.150 --> 02:33:26.010
more. I need a few anchor points where we can structure one

02:33:26.010 --> 02:33:29.010
or two talks. So, when you are giving a talk to this kind of

02:33:29.010 --> 02:33:33.590
audience, never go into a technical term. Your audience will

02:33:33.590 --> 02:33:36.250
not be able to understand. So, always try to go ahead with

02:33:36.250 --> 02:33:41.010
the example. First of all, start with their mobile. How are

02:33:41.010 --> 02:33:43.550
the applications of Gen AI happening there? How is it

02:33:43.550 --> 02:33:47.750
education? How is AI impacting the world? Everything just by

02:33:47.750 --> 02:33:49.530
example. Let's suppose you are using a pitch deck.

02:33:50.890 --> 02:33:53.250
Obviously, you will end up creating 15-20 pages of pitch

02:33:53.250 --> 02:33:55.570
deck. So, put different those 15-20 decks.

02:34:00.410 --> 02:34:03.970
For example, when I start giving ML AI lectures, I talk

02:34:03.970 --> 02:34:06.630
about Ola Uber a lot, I talk about Netflix, I talk about

02:34:06.630 --> 02:34:09.390
YouTube, YouTube recommendation system, I talk about Google

02:34:09.390 --> 02:34:13.190
map, how it is used there, how Ola is predicting the price.

02:34:13.530 --> 02:34:16.690
So, when you start giving this example, they will correlate

02:34:16.690 --> 02:34:21.250
themselves. If you start talking about models, LLMs, fine

02:34:21.250 --> 02:34:24.810
-tuning, nothing will happen Everything will go over the

02:34:24.810 --> 02:34:32.230
head. So, when you are giving a talk, you have to engage the

02:34:32.230 --> 02:34:37.130
audience to generate some sort of interest in them. Yes,

02:34:37.130 --> 02:34:41.250
towards AI, right? Towards AI, right? Yes, yes. So, interest

02:34:41.250 --> 02:34:43.310
will come only when there is an example. So, I will not say

02:34:43.310 --> 02:34:47.850
that take even a single technical name, right? There are so

02:34:47.850 --> 02:34:51.570
many LLMs, right? Yes. But, if you ask normal people, they

02:34:51.570 --> 02:34:54.990
just call it ChatGPT. Yes, absolutely correct. Whether you

02:34:55.510 --> 02:34:57.090
give them the interface of Gemini, whether you give them the

02:34:57.090 --> 02:34:59.050
interface of Publicity, or give them the interface of Cloud.

02:35:00.090 --> 02:35:03.750
For a general public, it's what? It's a ChatGPT, which they

02:35:03.750 --> 02:35:07.610
have, it's on their face, like, no, ChatGPT. ChatGPT, yes,

02:35:07.710 --> 02:35:10.610
correct. So, basically, all the LLMs, if you ask Google to

02:35:10.610 --> 02:35:13.610
use Gemini, it's ChatGPT, right? Publicity, no, it's

02:35:13.610 --> 02:35:16.110
ChatGPT, right? Cloud is a good code generator, no, it's

02:35:16.110 --> 02:35:20.150
ChatGPT. So, basically, it's the way they know things,

02:35:20.370 --> 02:35:22.450
right? Understood, understood. If you are giving some

02:35:22.450 --> 02:35:26.230
knowledge in don't take those heavy, or not even a simple

02:35:26.230 --> 02:35:32.170
name, take a simple example around their like education and

02:35:32.170 --> 02:35:34.530
the thing that they see on a mobile and everywhere, right?

02:35:35.370 --> 02:35:37.830
Okay, okay. People will understand, they will clap for you.

02:35:39.670 --> 02:35:42.330
Okay, okay, thank you so much. I think my message structure

02:35:42.330 --> 02:35:44.450
is okay, I will let you know once I'm done. Thank you so

02:35:44.450 --> 02:35:48.330
much. Thanks, thanks, take care. Yeah, thank you. Okay, so,

02:35:48.410 --> 02:35:51.230
fine, everyone, thank you so much. It was an amazing class,

02:35:51.390 --> 02:35:54.890
hope all of you have enjoyed it. Yeah, the post conversation

02:35:54.890 --> 02:35:58.370
is even most engaging and interesting, I believe, when we do

02:35:58.370 --> 02:36:01.810
a doubt clearing. So, that's the reason I have like opened

02:36:01.810 --> 02:36:04.930
this option, this option was not available before like when

02:36:04.930 --> 02:36:08.510
I was started giving classes in Iran. Well, yeah, with that,

02:36:08.530 --> 02:36:10.370
thank you so much, everyone, take care again, tomorrow,

02:36:10.390 --> 02:36:12.810
we'll have an interesting discussion. So, be ready for that.

02:36:13.830 --> 02:36:19.610
And yeah, see you tomorrow. Good night. And happy great day

02:36:19.610 --> 02:36:27.630
and night ahead. Okay, been hanging. Love you. Love you. You

02:36:27.630 --> 02:36:29.990
are so fine guys with that. Thank you so much. Take care.

