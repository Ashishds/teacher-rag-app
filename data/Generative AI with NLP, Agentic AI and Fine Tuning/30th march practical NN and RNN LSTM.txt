WEBVTT

00:01:19.020 --> 00:01:21.300
going to show you a same kind of a neural network with

00:01:21.300 --> 00:01:24.920
respect to a tensorflow keras library and then i'm going to

00:01:27.680 --> 00:01:32.000
show you an example of rn lstm so with the help of

00:01:33.560 --> 00:01:35.980
tensorflow library yeah so just wait

00:01:47.660 --> 00:01:49.460
for a couple of more minute guys then we are going to start

00:01:50.260 --> 00:01:56.000
okay so let's get started i'm sharing my screen and just try

00:01:56.000 --> 00:01:59.060
to open up your vs code guys so that you can see how to do

00:01:59.060 --> 00:01:59.060
it okay so now what is tcp, it is a msrp so we have the

00:01:59.060 --> 00:01:59.060
router so if we just open up the browser we are going to be

00:01:59.060 --> 00:02:02.140
able to see how used is our vstm so we are going to see how

00:02:02.140 --> 00:02:03.960
much we have used we are going to see what is our vstm and

00:02:03.960 --> 00:02:10.660
then we are going to have our telsa flow library and then we

00:02:10.660 --> 00:02:14.660
are going to see what is the vstm of our network so all

00:02:14.660 --> 00:02:17.620
these

00:02:19.670 --> 00:02:21.270
are the three things guys so we are going to

00:02:40.340 --> 00:02:47.340
see the

00:02:47.340 --> 00:02:51.920
open up your vs code guys so that we can write a code so vs

00:02:51.920 --> 00:02:58.100
code and a folder and you can try to open up any new folder

00:02:58.100 --> 00:03:08.940
so for example today is 30th so 30th march practical so here

00:03:08.940 --> 00:03:12.860
is a folder which i have opened up and inside this folder so

00:03:12.860 --> 00:03:19.460
try to create a file so 30th underscore march underscore nn

00:03:19.460 --> 00:03:31.060
and rnn lstm dot i p p y n b okay so this is the file and

00:03:31.060 --> 00:03:36.880
now we can start doing a code sir i was not able to find out

00:03:36.880 --> 00:03:39.360
the whatsapp link for this batch you will be able to find

00:03:39.360 --> 00:03:42.620
out deepu actually so if you'll check the very first class

00:03:42.620 --> 00:03:46.200
right so in the very first class not a in like introduction

00:03:46.200 --> 00:03:48.480
class the induction class that i have given you in so over

00:03:48.480 --> 00:03:51.900
there you will be able to find out a whatsapp group link in

00:03:51.900 --> 00:03:55.200
a resource section itself yeah yeah abhijit good afternoon

00:03:55.200 --> 00:03:59.800
okay so let's get started guys i believe our environment is

00:03:59.800 --> 00:04:02.980
ready so now we can start doing a coding so first of all

00:04:02.980 --> 00:04:05.640
guys what i'm going to show you is how we can try to create

00:04:05.640 --> 00:04:08.640
a neural network the similar kind of a concept that we have

00:04:08.640 --> 00:04:13.120
discussed with the help of just a raw core python library so

00:04:13.120 --> 00:04:18.600
just by writing some sort of a if else or for variable and

00:04:18.600 --> 00:04:22.060
exact same thing i will be able to create exact same thing i

00:04:22.060 --> 00:04:24.680
will be able to do it even with the help of tensorflow

00:04:24.680 --> 00:04:27.840
library with the help of uh pythos libraries and there are

00:04:27.840 --> 00:04:30.340
so many other libraries which is available in a market which

00:04:30.340 --> 00:04:33.440
can help me out to create a neural network at the end of the

00:04:33.440 --> 00:04:37.180
day so a raw python so which will give you a complete

00:04:37.180 --> 00:04:41.320
understanding about a concept that we have already discussed

00:04:41.320 --> 00:04:44.220
in a step-by-step manner you will be able to see that how

00:04:44.220 --> 00:04:47.880
things actually works internally and then you can try to

00:04:47.880 --> 00:04:49.400
create a neural network with the help of a library so which

00:04:49.400 --> 00:04:52.520
is technically a wrapper so you will not be able to know

00:04:52.520 --> 00:04:55.900
much what is going inside it but yeah it is it is going to

00:04:55.900 --> 00:04:58.220
make your life a bit easier so instead of writing a hundred

00:04:58.220 --> 00:05:01.260
line of a code you can try to call just one single line and

00:05:01.260 --> 00:05:03.600
then it is going to work for you this is how this entire

00:05:03.600 --> 00:05:08.240
library has been created fine so ipo and b and dot pi yeah

00:05:08.240 --> 00:05:10.600
so if you are going to create dot pi again it's completely

00:05:10.600 --> 00:05:13.340
fine there is no issue at all you will have to execute that

00:05:13.340 --> 00:05:19.180
dot pi every time so in case of ipo and b so you can try to

00:05:19.180 --> 00:05:21.540
execute it cell by cell that's only differences but yeah

00:05:21.540 --> 00:05:25.560
anything is good aman okay so first of all guys uh let's try

00:05:25.560 --> 00:05:28.900
to define our input variable so our input is going to be

00:05:28.900 --> 00:05:32.820
let's suppose 1 comma 2 comma 3 comma 4 a very simple input

00:05:32.820 --> 00:05:37.020
i'm going to consider and then output is basically going to

00:05:37.020 --> 00:05:42.980
be let's suppose uh 2 comma 4 comma 6 comma 8 now so what i

00:05:42.980 --> 00:05:46.300
want over here is i would like to create a neural network

00:05:46.300 --> 00:05:49.220
right i would like to create a neural network so which will

00:05:49.220 --> 00:05:53.840
be able to learn my input and my output and it will be able

00:05:53.840 --> 00:05:56.900
or it should be able to learn the pattern between input and

00:05:56.900 --> 00:06:01.380
output as you can see that my input is 1 2 3 4 and my output

00:06:01.380 --> 00:06:06.620
is 2x of a respective input so i just wanted to create a

00:06:06.620 --> 00:06:08.600
neural network which will be able to understand a

00:06:08.600 --> 00:06:11.940
relationship between input and output and whenever i'm going

00:06:11.940 --> 00:06:15.780
to give some sort of a input in a future it should be able

00:06:15.780 --> 00:06:18.640
to give me a output so that's a that's a whole idea over

00:06:18.640 --> 00:06:21.100
here so we are trying to go ahead with a supervised machine

00:06:21.100 --> 00:06:23.860
learning algorithm supervised learning over here not a

00:06:23.860 --> 00:06:26.540
machine learning obviously deep learning a neural network

00:06:26.540 --> 00:06:31.220
and here so we'll try to train our model to understand a

00:06:31.220 --> 00:06:34.720
relationship between input and output as a human being so

00:06:34.720 --> 00:06:38.620
yes it's clearly visible that input and output is having

00:06:38.620 --> 00:06:42.560
some sort of a relation so input is like a output is

00:06:42.560 --> 00:06:46.120
basically a 2x of a input but again so i have to make sure

00:06:46.120 --> 00:06:47.880
that my machine will be able to do that so that's a whole

00:06:47.880 --> 00:06:48.000
idea over here so we are trying to go ahead with a to

00:06:48.000 --> 00:06:50.500
understand my neural network will be able to understand the

00:06:50.500 --> 00:06:53.360
way we have gone through a theory the way we have learned

00:06:53.360 --> 00:06:56.460
the theory part. So let's get started. Fine guys, problem

00:06:56.460 --> 00:07:00.740
statement is clear to all of us. Yeah. So my input is ready,

00:07:00.860 --> 00:07:03.740
my output is ready, I have to place a neural network in

00:07:03.740 --> 00:07:06.300
between so that it will be able to understand a relationship

00:07:06.300 --> 00:07:09.760
between input and output. And I'm not going to use any kind

00:07:09.760 --> 00:07:13.300
of a library in this particular example. So I'm going to

00:07:13.300 --> 00:07:15.660
show you each and everything with the help of raw Python

00:07:15.660 --> 00:07:20.420
code. Okay, so let's get started. And let's try to see how

00:07:20.420 --> 00:07:23.760
we can try to build a neural network. So obviously, as we

00:07:23.760 --> 00:07:26.920
know from our previous class understanding that we can try

00:07:26.920 --> 00:07:30.160
to build a neural network over here. So where I'm going to

00:07:30.160 --> 00:07:33.980
pass an input, right? So maybe I can try to pass an input

00:07:33.980 --> 00:07:37.400
over here, there will be a hidden layer. So it is going to

00:07:37.400 --> 00:07:41.060
send a data into a hidden layer basically, inside a hidden

00:07:41.060 --> 00:07:43.840
layer. So we can try to place an activation function over

00:07:43.840 --> 00:07:43.840
here. So we can try to pass an activation function over

00:07:43.840 --> 00:07:44.820
here. So we can try to pass an output over here. Right, so

00:07:44.820 --> 00:07:47.520
activation function will try to either like give me the same

00:07:47.520 --> 00:07:49.980
data, maybe it will try to filter out some data, or maybe it

00:07:49.980 --> 00:07:53.500
will try to non linearize some of the data depends what kind

00:07:53.500 --> 00:07:56.340
of activation function which I'm trying to use. And then

00:07:56.340 --> 00:07:59.480
eventually, it is going to give me the output, right, I will

00:07:59.480 --> 00:08:03.060
be able to calculate the output over here. Now, once I will

00:08:03.060 --> 00:08:06.100
be able to get an output, I'll try to calculate an error.

00:08:06.220 --> 00:08:09.240
Now, once I will be able to calculate an error, so I'll try

00:08:09.240 --> 00:08:12.920
to do a backward propagation. And I'll try to update all of

00:08:12.920 --> 00:08:15.920
these relations. So relation means of weights, right bits

00:08:15.920 --> 00:08:18.920
and biases, I'll try to update. And then again, I'll try to

00:08:18.920 --> 00:08:21.180
send the data again, I'll try to send a data. So I'll try to

00:08:21.180 --> 00:08:24.360
send a data multiple times so that my network will be able

00:08:24.360 --> 00:08:27.480
to learn it. Now this is something that that we are going to

00:08:27.480 --> 00:08:31.280
perform. And to do that, so I have an input over here, and I

00:08:31.280 --> 00:08:34.260
have output over here. So I have to make sure that my

00:08:34.260 --> 00:08:37.320
network will be able to understand a relationship between

00:08:37.320 --> 00:08:39.580
input and output. So that's a whole idea. That's a whole

00:08:39.580 --> 00:08:42.460
objective guys that we are going to fulfill in a very first

00:08:42.460 --> 00:08:49.140
place. So let's start coding everyone. So here, what we can

00:08:49.140 --> 00:08:52.240
do is, so we can try to initialize a weight. So initially,

00:08:52.440 --> 00:08:56.180
let's suppose my weight is 0.0. So my weight is completely

00:08:56.180 --> 00:09:00.440
zero, and then biases is going to be 0.0 initially. So there

00:09:00.440 --> 00:09:03.140
is no value I'm trying to assign to my weight and to my

00:09:03.140 --> 00:09:05.960
biases. And we know that that we know another variable

00:09:05.960 --> 00:09:08.620
called as learning rate. So learning rate is going to be 0

00:09:08.620 --> 00:09:12.460
.01. So at the time of modification of my weight and biases.

00:09:12.680 --> 00:09:15.620
So obviously, I have, to like, you know, introduce a

00:09:15.620 --> 00:09:18.260
learning rate. So maybe I can try to go ahead with 0.01,

00:09:18.480 --> 00:09:22.860
maybe 0.001, maybe 0.1. Or it's a hyper parameter. So I can

00:09:22.860 --> 00:09:25.300
try to tune it, it's completely fine. So these are the

00:09:25.300 --> 00:09:28.360
variable which I'm going to create. Now what I can do is so

00:09:28.360 --> 00:09:31.660
maybe we can try to create a loop over here so that I will

00:09:31.660 --> 00:09:35.580
keep on sending a data again and again and again, right. So

00:09:35.580 --> 00:09:38.540
multiple times, I'll keep on sending a data my input and my

00:09:38.540 --> 00:09:43.980
output data, and eventually it will be able to send a loop

00:09:43.980 --> 00:09:46.520
over here. So I can try to write maybe a for loop over here

00:09:46.520 --> 00:09:49.700
so that it will try to send our data multiple times. So I

00:09:49.700 --> 00:09:52.280
would like to send our data maybe 100 times inside my

00:09:52.280 --> 00:09:57.180
network, right. So here I can write that epoch. So I can

00:09:57.180 --> 00:09:59.360
write any kind of variable name. But yeah, just for the

00:09:59.360 --> 00:10:02.920
understanding purposes, I'm going to write for epoch in a

00:10:02.920 --> 00:10:06.300
range of 100, it simply means that my for loop is going to

00:10:06.300 --> 00:10:09.300
run for 100 times, right. So we all will be able to

00:10:09.300 --> 00:10:12.700
understand and yeah, you can even do a coding with me. So

00:10:12.700 --> 00:10:17.520
here is a for loop. code. I'll keep on copying and pasting

00:10:17.520 --> 00:10:22.720
this code inside your chat box. Okay, so for epoch in a

00:10:22.720 --> 00:10:26.460
range of 100, what we can do is so we can try to say that

00:10:26.460 --> 00:10:30.180
okay, so total underscore loss because every time I have to

00:10:30.180 --> 00:10:33.100
calculate the loss or total loss is equal to zero for now, I

00:10:33.100 --> 00:10:36.680
don't have any kind of a loss as of now basically, right as

00:10:36.680 --> 00:10:40.780
of now, now what I can do is so I can try to like I'm trying

00:10:40.780 --> 00:10:43.920
to run this loop for 100 times now every time whenever I'm

00:10:43.920 --> 00:10:47.000
trying to run the loops, I'll have to send the data right? I

00:10:47.000 --> 00:10:51.140
have to send them data technically inside our network. So

00:10:51.140 --> 00:10:55.580
here I can try to run a loop once again just to send data

00:10:56.180 --> 00:10:59.800
inside my network. So what kind of data I'm going to send

00:10:59.800 --> 00:11:02.100
let's try to understand what kind of a data I'm going to

00:11:02.100 --> 00:11:05.020
send inside a network and how it is going to like follow

00:11:05.020 --> 00:11:09.360
along. So here I have an input and then I have output. So

00:11:09.360 --> 00:11:11.820
obviously, I'm going to send the input data and I'm going to

00:11:11.820 --> 00:11:15.200
send the output data now here i can write a loop over here

00:11:15.200 --> 00:11:19.760
that for x and y right so i'm going to send x and then y uh

00:11:19.760 --> 00:11:25.760
and then in i can keep this input data and my output data

00:11:25.760 --> 00:11:30.540
into a zip output data into a zip now what is the meaning of

00:11:30.540 --> 00:11:33.480
this zip by the way so nothing it's a basic pythonic

00:11:33.480 --> 00:11:37.640
function and if i'm going to call zip input and output so it

00:11:37.640 --> 00:11:41.340
is going to basically match one by one one by one index to

00:11:41.340 --> 00:11:44.060
index means one two two two two four three two six four two

00:11:44.060 --> 00:11:46.820
six this is what it is going to do so maybe i can try to

00:11:46.820 --> 00:11:50.100
call a list along and then i will be able to show you a data

00:11:50.100 --> 00:11:52.820
as well so what zip does internally but yeah i'm hoping that

00:11:52.820 --> 00:11:55.440
uh we all understand what is the meaning of a zip or

00:11:55.440 --> 00:11:58.440
enumeration or such kind of a functions inside a python

00:11:58.440 --> 00:12:01.920
because i'm hoping that uh we all understands a basic of

00:12:01.920 --> 00:12:04.840
python at least so zip is going to do nothing so pair it is

00:12:04.840 --> 00:12:07.180
going to create so one two two two four three two four three

00:12:07.180 --> 00:12:09.900
two six four two six so this is the pair this is the pair

00:12:09.900 --> 00:12:12.560
this is the pair this is the where so if input is one output

00:12:12.560 --> 00:12:15.780
should be two input is two output should be four if input is

00:12:15.780 --> 00:12:18.540
six output should be six if input is four output should be

00:12:18.540 --> 00:12:22.740
eight yeah simple right it is going to create a pair so this

00:12:22.740 --> 00:12:25.400
is what a zip is going to do as simple as that so zip is

00:12:25.400 --> 00:12:28.760
going to create a pair now i'm trying to iterate over to

00:12:28.760 --> 00:12:31.020
this pair as you can see so this is available into a

00:12:31.020 --> 00:12:34.420
iterator iterable variables so i'm trying to iterate over to

00:12:34.420 --> 00:12:37.300
it so i'll try to extract one pair so value of x is equal to

00:12:37.300 --> 00:12:41.120
one value of i is equal to two then i'll another like a pair

00:12:41.120 --> 00:12:45.500
so value of x is equal to 2 and then 4 then 3 and 6 then 4

00:12:45.500 --> 00:12:49.160
and 8 so value of x and y in every iteration it will keep on

00:12:49.160 --> 00:12:54.440
assigning as simple as that okay now so here basically x and

00:12:54.440 --> 00:12:58.880
y is completely fine now what about a y prediction right so

00:12:58.880 --> 00:13:04.760
y underscore prediction variable i am going to create so y

00:13:04.760 --> 00:13:09.520
prediction so y prediction is nothing but my weight into x

00:13:09.520 --> 00:13:13.320
plus b so this is what a y prediction is going to be so i

00:13:13.320 --> 00:13:17.700
have a y over here so like a expected value and this is the

00:13:17.700 --> 00:13:20.580
predicted value so how i will be able to do a prediction i

00:13:20.580 --> 00:13:22.740
will be able to do a prediction based on the weight and

00:13:22.740 --> 00:13:26.620
biases yep so maybe i'm trying to like establish a like a

00:13:26.620 --> 00:13:29.940
linear relation over here and this is the forward pass right

00:13:29.940 --> 00:13:32.760
this is the forward pass so i'm trying to pass a data and

00:13:32.760 --> 00:13:36.000
then it is going to create a weight and it will be having a

00:13:36.000 --> 00:13:39.360
weight and the biases and then it is going to give me y as a

00:13:39.360 --> 00:13:43.540
prediction i'm trying to like a take one linear model i will

00:13:43.540 --> 00:13:47.500
be able to take even a multiple hidden layer over here and

00:13:47.500 --> 00:13:50.260
then i will be able to show you but in a later stage in a

00:13:50.260 --> 00:13:53.020
later stage as of now so i'm just trying to consider a basic

00:13:53.020 --> 00:13:56.040
a basic and most basic like a neural network i'm going to

00:13:56.040 --> 00:13:59.880
create over here okay so once i'm able to get a y prediction

00:13:59.880 --> 00:14:03.200
then i will try to calculate what i will try to calculate a

00:14:03.200 --> 00:14:06.420
error e double r or error is a like a let's suppose variable

00:14:06.420 --> 00:14:09.460
i am going to create so i will try to calculate an error now

00:14:09.460 --> 00:14:11.720
what is the error basically so error is nothing but a

00:14:11.720 --> 00:14:15.160
difference between basically y prediction that we are trying

00:14:15.160 --> 00:14:19.420
to do minus y right for this particular iteration so it has

00:14:19.420 --> 00:14:21.880
predicted something and then we are able to get something so

00:14:21.880 --> 00:14:25.200
for example initially my weight is going to be zero and my

00:14:25.200 --> 00:14:28.000
biases is going to be zero so y prediction is going to be

00:14:28.000 --> 00:14:30.080
what y prediction is going to be for the very first

00:14:30.080 --> 00:14:33.180
iteration so y prediction is going to be zero but what is my

00:14:33.180 --> 00:14:36.980
actual y so my actual y is basically two i was expecting two

00:14:36.980 --> 00:14:40.600
basically right and it has given me uh like a zero so

00:14:40.600 --> 00:14:43.340
obviously i will be having error is equal to two now once i

00:14:43.340 --> 00:14:47.000
will be able to get an error right once i will be able to

00:14:47.000 --> 00:14:50.080
get an error so maybe we can try to take our error function

00:14:50.080 --> 00:14:52.620
over here so mean squared error let's suppose if i'm talking

00:14:52.620 --> 00:14:56.420
about so loss function right we generally try to place a

00:14:56.420 --> 00:14:58.900
loss function so loss function mean squared error is nothing

00:14:58.900 --> 00:15:02.980
but error square basically right so maybe i can try to take

00:15:02.980 --> 00:15:03.360
an error function over here so i will try to take a loss

00:15:03.360 --> 00:15:05.700
function i can even try to take my own custom loss function

00:15:05.700 --> 00:15:08.140
that's completely up to me right no one is going to force

00:15:08.140 --> 00:15:15.180
you so error and then maybe error maybe square again so i

00:15:15.180 --> 00:15:19.300
will be able to calculate a loss now total loss so total

00:15:19.300 --> 00:15:24.640
loss is nothing but total previous loss plus basically a

00:15:24.640 --> 00:15:26.680
current loss so this is the this is going to create

00:15:26.680 --> 00:15:29.800
calculate the total loss so initially my total loss was zero

00:15:29.800 --> 00:15:32.680
right for this particular iteration so row loss is equal to

00:15:32.680 --> 00:15:35.220
zero so zero plus loss so this is the loss it is going to

00:15:35.220 --> 00:15:39.600
calculate now once i will be having a loss right once i will

00:15:39.600 --> 00:15:43.760
be having a loss then what i'm supposed to do so once i'm

00:15:43.760 --> 00:15:48.060
having a loss i have to basically do a backward propagation

00:15:48.060 --> 00:15:50.940
now this is basically a forward propagation in a forward

00:15:50.940 --> 00:15:54.880
direction so it is like trying to calculate the loss then it

00:15:54.880 --> 00:15:57.560
is trying to calculate the total loss and then i have to do

00:15:57.560 --> 00:16:00.140
a backward propagation backward propagation means a

00:16:00.140 --> 00:16:03.680
propagation so where i will try to change the value of w and

00:16:03.680 --> 00:16:06.940
b so i will try to change the value of my weights and the

00:16:06.940 --> 00:16:09.360
biases this is what we do in case of a backup propagation

00:16:09.360 --> 00:16:12.620
making sense guys to all of us all of us are able to

00:16:12.620 --> 00:16:16.140
understand till this point are you able to visualize the

00:16:16.140 --> 00:16:19.100
complete architecture and are you able to like you know

00:16:19.100 --> 00:16:21.960
correlate the theory that we have discussed and the code

00:16:21.960 --> 00:16:27.400
that we are trying to write over here yes everyone or no you

00:16:27.400 --> 00:16:29.060
are not able to understand i

00:16:31.530 --> 00:16:34.070
don't think that i am writing some complex code over here so

00:16:34.070 --> 00:16:38.110
i think i'm writing one of the most like a easiest and basic

00:16:38.110 --> 00:16:41.750
code in this place just to correlate with the understanding

00:16:41.750 --> 00:16:45.530
theoretical understanding that you have yeah understood sir

00:16:45.530 --> 00:16:48.030
it's easily relatable yes exactly so just just try to

00:16:48.030 --> 00:16:50.170
visualize just try to visualize whatever we have discussed

00:16:50.170 --> 00:16:53.030
in my new network class exact same thing i'm trying to

00:16:53.030 --> 00:16:55.730
replicate inside my code line by line line by line step by

00:16:55.730 --> 00:16:58.890
step manner without jumping from anywhere to anywhere yeah

00:16:58.890 --> 00:17:02.510
just just try to focus and plus do it on your own, please

00:17:02.510 --> 00:17:04.690
try to do it on your own so that you will be able to get

00:17:04.690 --> 00:17:07.070
better understanding you can maybe try to change your loss

00:17:07.070 --> 00:17:09.450
function you can try to like play with anything so

00:17:09.450 --> 00:17:13.370
everything is under your control basically okay fine so now

00:17:13.370 --> 00:17:16.090
i'm able to calculate the loss now what i have to do i have

00:17:16.090 --> 00:17:18.430
to do a backward propagation now what we do generally into a

00:17:18.430 --> 00:17:22.550
backward propagation yes guys so what we do generally into a

00:17:22.550 --> 00:17:27.450
backward propagation any idea so we try to recalculate the

00:17:27.450 --> 00:17:31.150
weight right so weight new is nothing but weight old minus

00:17:31.150 --> 00:17:34.850
eta which is nothing but a learning rate and then loss with

00:17:34.850 --> 00:17:39.050
respect to w right loss with respect to w and then we try to

00:17:39.050 --> 00:17:41.870
change basically our biases so weight biases new is nothing

00:17:41.870 --> 00:17:47.090
but biases old minus eta and then loss with respect to the

00:17:47.090 --> 00:17:50.310
biases can i say that this is what we do right this is how

00:17:50.310 --> 00:17:54.870
we try to adjust the weight into a backward pass right into

00:17:54.870 --> 00:17:58.230
a backward pass this is what we try to do so what we can do

00:17:58.230 --> 00:18:01.150
is so we know the older weight we we are aware about the old

00:18:01.150 --> 00:18:03.950
way old weight is basically zero and my old bias is

00:18:03.950 --> 00:18:07.270
basically zero uh i'm aware about the learning rate so if

00:18:07.270 --> 00:18:11.190
i'm able to find out this uh dl of so loss function with

00:18:11.190 --> 00:18:14.110
respect to like a weight i think i will be able to like uh

00:18:14.110 --> 00:18:17.290
do the changes right i will be able to make the changes over

00:18:17.290 --> 00:18:20.750
here so fine so let's suppose if i'm trying to find out like

00:18:20.750 --> 00:18:23.910
a rate of change of loss with respect to w now what is my

00:18:23.910 --> 00:18:26.730
loss function by the way so my loss function is basically

00:18:26.730 --> 00:18:31.210
error of square yeah my loss function is basically a error

00:18:31.210 --> 00:18:31.610
of square right so my loss function is question is basically

00:18:31.610 --> 00:18:36.630
a error of is square so what will be right guys so what will

00:18:36.630 --> 00:18:45.130
be basically DL by DW any idea okay they want saying I mean

00:18:45.130 --> 00:18:47.430
it's mean the mean square error yeah technically we are

00:18:47.430 --> 00:18:50.170
talking about mean square error what will be DL by DW guys

00:18:50.170 --> 00:18:54.690
by the way rate of change of loss with respect to W what it

00:18:54.690 --> 00:18:56.110
will be yep

00:19:00.780 --> 00:19:04.340
I think we have derived that we have discussed about it with

00:19:14.960 --> 00:19:18.280
the help of like a chain rule so we have broken it down so

00:19:21.290 --> 00:19:24.790
I think we have done a derivation for DL by DW into DW by

00:19:24.790 --> 00:19:30.910
like a like a entire function so I believe we have gone

00:19:30.910 --> 00:19:33.010
through the gradient descent yeah gradient descent is fine

00:19:33.010 --> 00:19:35.450
Sachin so what we have done with respect to gradient descent

00:19:35.450 --> 00:19:38.690
what is the final formula that we were able to reach out

00:19:38.690 --> 00:19:44.010
cross and drop you know just just tell me like DL by DW

00:19:44.010 --> 00:19:46.890
that's it just just tell me the final result final value of

00:19:46.890 --> 00:19:47.430
DL by DW

00:19:55.040 --> 00:19:57.800
yeah I'm not looking for in terms of a derivative I'm

00:19:57.800 --> 00:20:00.900
looking for in terms of a chain rule of derivative obviously

00:20:00.900 --> 00:20:03.020
that is something that we have done so if you will go and

00:20:03.020 --> 00:20:06.400
check back your new network class you will be able to find

00:20:06.400 --> 00:20:09.520
out so can I say that it's nothing but two into error into X

00:20:09.520 --> 00:20:14.340
right and then DL by like a DB which is nothing but a bias

00:20:14.340 --> 00:20:16.800
is nothing but two into error so this is something that we

00:20:16.800 --> 00:20:20.420
have derived in my classes right this is what we have

00:20:20.420 --> 00:20:23.520
already derived in my classes so what we can do is that we

00:20:23.520 --> 00:20:26.660
can try to use the same formula over here and then we can

00:20:26.660 --> 00:20:29.860
try to write a code accordingly in this particular place so

00:20:29.860 --> 00:20:36.200
my DL by DW so basically I can make it a variable as a DW as

00:20:36.200 --> 00:20:39.160
of now right so basically it's a DL by DW I'm trying to

00:20:39.160 --> 00:20:42.240
write so just as a variable name wise I have written this so

00:20:42.240 --> 00:20:48.120
two into two into error a variable that I have taken into X

00:20:48.120 --> 00:20:54.440
simple and my DB DB so basically it's a DL by DB. So, I'm

00:20:54.440 --> 00:20:58.840
just writing as a variable DB so two into two into error

00:20:58.840 --> 00:21:03.240
okay so this is something that we can try to write it down

00:21:03.240 --> 00:21:11.340
now so I'm able to find out DL by DW and DL by DB so now

00:21:11.340 --> 00:21:14.760
it's time to write or it's time to update the value of W so

00:21:14.760 --> 00:21:18.680
W new is nothing but W old into eta into this one DW and

00:21:18.680 --> 00:21:22.560
then same goes for DB respectively now here what I can do

00:21:22.560 --> 00:21:27.080
is. So, my new weight is nothing but my old weight basically

00:21:27.080 --> 00:21:39.240
so my old weight minus learning rate into DW into DW and my

00:21:39.240 --> 00:21:44.460
new bias is nothing but my old bias minus learning rate into

00:21:44.460 --> 00:21:49.780
DB okay fine so this is something that we can try to write

00:21:49.780 --> 00:21:53.500
down over here now so we can we can. We can try to like a

00:21:53.500 --> 00:21:56.080
print something over here so we are running this things for

00:21:56.080 --> 00:21:58.940
a hundred iterations right so we are running these things

00:21:58.940 --> 00:22:01.520
for the hundred iterations so maybe I would like to see that

00:22:01.520 --> 00:22:05.580
how it is trying to adjust the loss so on every 10 steps

00:22:05.580 --> 00:22:08.780
let's suppose I would like to see that how it is like going

00:22:08.780 --> 00:22:14.060
to adjust the entire like a losses for me basically so maybe

00:22:14.060 --> 00:22:20.220
I can write if a statement over here so if epoch percentile

00:22:20.220 --> 00:22:25.640
10 so remainder 10 is equal equal to zero. 0. 0.

00:22:53.340 --> 00:22:56.540
0. So this is like my print and then show me the changes in

00:22:56.540 --> 00:22:59.580
the weight. So after every 10 epoch, whatever weight change

00:22:59.580 --> 00:23:03.260
it is trying to perform, just try to show it to me, right?

00:23:03.440 --> 00:23:06.580
So just try to show me my weight changes and then similarly

00:23:06.580 --> 00:23:09.540
just try to show me my bias changes. So I'm just trying to

00:23:09.540 --> 00:23:11.980
parameterize each and everything. So show me a bias changes.

00:23:12.060 --> 00:23:15.240
So this is going to print and every 10 steps it is going to

00:23:15.240 --> 00:23:18.940
basically print it and then maybe I can come out of the

00:23:18.940 --> 00:23:22.580
loop, the main loop, basically the above one, I can try to

00:23:22.580 --> 00:23:25.520
come out of the loop and then I can try to send a test data,

00:23:25.620 --> 00:23:30.500
test underscore, test underscore data,

00:23:33.350 --> 00:23:35.970
test underscore data. Let's suppose test data is equal to

00:23:35.970 --> 00:23:38.450
seven. I'm going to give, yeah, test data is equal to seven.

00:23:38.530 --> 00:23:41.070
I'm going to give. So obviously this test data, my model has

00:23:41.070 --> 00:23:44.370
not seen, I have not included that as a part of my input. So

00:23:44.370 --> 00:23:46.550
I'm trying to give an input is equal to seven. I will be

00:23:46.550 --> 00:23:48.990
looking for a prediction basically, right? I'll be looking

00:23:48.990 --> 00:23:50.750
for a prediction. So how it is going to give a prediction

00:23:50.750 --> 00:23:54.630
obviously based on the value of W and B. So prediction.

00:23:55.090 --> 00:24:02.450
Okay. So prediction is equals to weight, uh, into like a

00:24:02.450 --> 00:24:06.690
test data plus biases. So whatever weight and biases it will

00:24:06.690 --> 00:24:09.030
be able to train. So based on that, it is going to give me

00:24:09.030 --> 00:24:12.310
the prediction. Yeah. It is going to give me the prediction

00:24:12.310 --> 00:24:14.930
and maybe I can try to like, uh, put into this prediction.

00:24:16.830 --> 00:24:19.310
So when this prediction, okay.

00:24:22.600 --> 00:24:26.700
So now in no time, right? It had no time. It is able to run

00:24:26.700 --> 00:24:29.520
itself in no time. It is able to run itself. And as you can

00:24:29.520 --> 00:24:33.500
see. That every time I'm going to run, right? So you will be

00:24:33.500 --> 00:24:35.680
able to see some different results, little bit of changes.

00:24:35.820 --> 00:24:37.460
You will be able to see because it's a neural network,

00:24:37.560 --> 00:24:40.000
right? So we are trying to change our value arbitrarily. We

00:24:40.000 --> 00:24:43.460
are not like, uh, writing some sort of exact formula over

00:24:43.460 --> 00:24:47.520
here. And as you can see that on epoch zero, right? The

00:24:47.520 --> 00:24:51.480
losses was this, my weight was this, my biases was this. Now

00:24:51.480 --> 00:24:55.580
it is trying to adjust the loss at epoch 10, weight is this,

00:24:55.680 --> 00:25:00.340
biases is this, then epoch 20, 30, 40, 50, 60, 90. So this

00:25:00.340 --> 00:25:04.400
is the loss, uh, loss is very, very small as you can see

00:25:04.400 --> 00:25:11.190
over here. And then, okay, uh, then we have a weight and

00:25:11.190 --> 00:25:14.750
then we have a biases over here and based on this weight and

00:25:14.750 --> 00:25:19.070
biases calculation, I have passed seven and then it is

00:25:19.070 --> 00:25:22.950
giving me 13.99. I was expecting 14 by the way, but yeah,

00:25:23.010 --> 00:25:26.070
it's nearby result, right? It's a nearby result I'm able to

00:25:26.070 --> 00:25:29.170
get. It simply means that, that my system is able to learn

00:25:29.170 --> 00:25:32.530
something. Yeah. It is able to basically learn something

00:25:32.530 --> 00:25:35.090
over here. Maybe I can try to nullify each and everything,

00:25:35.210 --> 00:25:40.790
right? Uh, yeah. Nullify my reset, my weight and everything.

00:25:41.290 --> 00:25:44.670
And then I can try to run it for maybe a thousand epoch. So

00:25:44.670 --> 00:25:48.170
I'm saying that just go for a thousand times again, like

00:25:48.170 --> 00:25:51.790
almost, uh, exact accurate result I'm able to get. It's a,

00:25:51.850 --> 00:25:54.030
one of the simple problems it would learn for a neural

00:25:54.030 --> 00:25:56.470
network, right? As you can see it's for a thousand times, it

00:25:56.470 --> 00:25:58.670
is trying to pass the data. It is trying to do a backward

00:25:58.670 --> 00:26:01.090
propagation. And then eventually it is trying to do a

00:26:01.090 --> 00:26:01.930
backward propagation. So it is trying to learn itself.

00:26:03.010 --> 00:26:07.310
Making sense guys. Yeah. Oh, Aman is saying videos not

00:26:07.310 --> 00:26:10.170
clear. Is it? I think I'm streaming in SD quality.

00:26:13.060 --> 00:26:16.080
It's a 4k quality in which I'm streaming as of now Aman by

00:26:16.080 --> 00:26:16.380
the way.

00:26:31.250 --> 00:26:33.610
Yeah biases changes is very good. It's going to be very,

00:26:33.670 --> 00:26:34.010
very small.

00:26:38.690 --> 00:26:42.930
Fine everyone. Yeah. So I believe, uh, uh, we all are able

00:26:42.930 --> 00:26:46.510
to like learn that how we can try to, so let me bring you

00:26:46.510 --> 00:26:48.130
this code. So this is the code guys, which I have written.

00:26:48.290 --> 00:26:51.070
Right. Uh, so maybe you can try to use this code. But yeah.

00:26:51.470 --> 00:26:55.430
So in the very, very simple way, just without using any kind

00:26:55.430 --> 00:26:59.430
of, uh, you know, uh, library inbuilt library, just by

00:26:59.430 --> 00:27:02.830
writing a simple Python code and the understanding that we

00:27:02.830 --> 00:27:05.850
have received from our theoretical class, we are able to

00:27:05.850 --> 00:27:08.610
build a neural network, right? We are able to build an

00:27:08.610 --> 00:27:10.610
event, but maybe I can try to change the data over here.

00:27:10.670 --> 00:27:13.810
Maybe I can try to give a three comma four comma seven comma

00:27:13.810 --> 00:27:18.050
eight comma nine, something like this. I can try to give,

00:27:18.210 --> 00:27:23.650
and I can say that let's try to learn it. Okay. So six, why

00:27:23.650 --> 00:27:28.790
I'm doing this by the way. So seven comma five comma six

00:27:28.790 --> 00:27:31.590
comma eight, some, some arbitrary pattern I have taken.

00:27:32.010 --> 00:27:34.170
Right. So yeah.

00:28:03.900 --> 00:28:07.440
So it is able to learn even like, uh, that pattern. And as

00:28:07.440 --> 00:28:10.840
you can see, my loss is a very, very, very, very high,

00:28:10.920 --> 00:28:13.560
right. It is, it simply means that it is not able to find

00:28:13.560 --> 00:28:16.120
any kind of a pattern over here. This is what it means by

00:28:16.120 --> 00:28:19.660
the way. Right. My loss is like previously loss was like,

00:28:19.740 --> 00:28:26.000
uh, some numbers. It simply means that like, uh, that the

00:28:26.000 --> 00:28:29.140
main number of the fraction till which data is or the value

00:28:29.140 --> 00:28:32.340
is going, but here you will be able to find out that my loss

00:28:32.340 --> 00:28:36.060
is in an absolute 4,798, which is a very, very high number.

00:28:36.200 --> 00:28:39.580
It is not even able to adjust itself in a last couple of

00:28:39.580 --> 00:28:42.500
iterations, right? Last couple of epoch. It is not able to

00:28:42.500 --> 00:28:45.840
even like, uh, uh, change itself means it is not able to

00:28:45.840 --> 00:28:48.740
learn further. So this is what happens when you give a data

00:28:48.740 --> 00:28:52.460
set. Uh. And, uh, that data set is not having any kind of a

00:28:52.460 --> 00:28:55.320
pattern into it. So if that data set is not having any kind

00:28:55.320 --> 00:28:58.500
of a pattern, obviously your neural network is going to be

00:28:58.500 --> 00:29:01.060
helpless. It will not be, if it is not able to find out the

00:29:01.060 --> 00:29:03.500
pattern simply means that it is not able to reduce the

00:29:03.500 --> 00:29:07.460
losses. Fine guys, everyone.

00:29:17.480 --> 00:29:20.460
Yup. So you can try to change this data. You can try to

00:29:20.460 --> 00:29:22.720
change this data and then you can try to maybe test it. You

00:29:22.720 --> 00:29:25.640
will be able to find out the exact relevancy that we have

00:29:25.640 --> 00:29:30.180
discussed in our theoretical class. So hope this is making

00:29:30.180 --> 00:29:33.520
sense. To all of us and, uh, hope all of you are able to

00:29:33.520 --> 00:29:37.620
build a neural network, a simple one, right? A very simple

00:29:37.620 --> 00:29:37.940
one.

00:29:47.210 --> 00:29:51.150
Yeah. Now. So what I can do is now maybe I can try to extend

00:29:51.150 --> 00:29:54.190
the same neural network. I'll try to do some sort of

00:29:54.190 --> 00:29:56.910
experimentation. I'll try to extend the same neural network.

00:29:57.270 --> 00:30:00.750
I'll try to place some sort of a hidden layer in between.

00:30:00.910 --> 00:30:03.650
This is just a linear network, which I have taken. So maybe

00:30:03.650 --> 00:30:07.190
I can try to like, uh, create some sort of a hidden layer in

00:30:07.190 --> 00:30:09.110
between. And then I'll try to do some sort of

00:30:09.110 --> 00:30:10.750
experimentation. I'll try to build the neural network that

00:30:10.750 --> 00:30:13.910
is also possible. So I can try to take the same input kind

00:30:13.910 --> 00:30:17.010
of a data. So my input is going to be, let's suppose, uh,

00:30:17.110 --> 00:30:19.290
you can, you can try to even like, uh, put an analogy over

00:30:19.290 --> 00:30:22.670
here that, uh, this is the number of hours I'm going to

00:30:22.670 --> 00:30:26.770
study. So one comma three comma four comma five, and then

00:30:26.770 --> 00:30:30.950
this is the score I will be able to get inside my, this is

00:30:30.950 --> 00:30:34.210
the marks. If I'm studying one hour, I'll get to, if I'm

00:30:34.210 --> 00:30:36.670
studying two hour, I'm going to get four just analogy.

00:30:36.930 --> 00:30:39.490
Otherwise it's just a data. Yeah. I'm just showing you a

00:30:39.490 --> 00:30:42.750
simple pattern. So five and then 10, the two, four, six,

00:30:42.810 --> 00:30:46.630
eight, and then 10. So this is the input data that I'm going

00:30:46.630 --> 00:30:50.070
to consider same as before. Now this time I'll just try to

00:30:50.070 --> 00:30:53.570
change a little bit. I'll try to change the architecture of

00:30:53.570 --> 00:30:58.850
my network. And what I can do is, so maybe I can try to pass

00:30:58.850 --> 00:31:03.110
up input data over here, right? I'll try to pass the input

00:31:03.110 --> 00:31:07.530
data. I'll try to keep a hidden layer in between, right?

00:31:07.630 --> 00:31:09.630
I'll try to keep. I'll try to keep a hidden layer in between

00:31:09.630 --> 00:31:14.250
and then there will be the output layer. So if I have a

00:31:14.250 --> 00:31:17.130
hidden layer, so obviously I have to place some sort of a

00:31:17.130 --> 00:31:20.050
activation functions. The previous one was a simple linear

00:31:20.050 --> 00:31:23.350
one. So input output Y is equal to C. That is the kind of a

00:31:23.350 --> 00:31:26.530
relation. Now maybe I can try to like a change something

00:31:26.530 --> 00:31:29.690
inside the hidden layer. So in that case I will have to

00:31:29.690 --> 00:31:33.390
define a weights in this place, right? So in that case I'll

00:31:33.390 --> 00:31:38.190
have to define the weights. So like a, and then even over

00:31:38.190 --> 00:31:40.750
here. So. So I will be having a weights and then I will be

00:31:40.750 --> 00:31:43.710
having an output. Then I'll calculate the loss loss, then

00:31:43.710 --> 00:31:47.430
backward propagation. And then finally I will be able to

00:31:47.430 --> 00:31:50.690
train the entire model. So let's do it. Let's write a code

00:31:50.690 --> 00:31:54.810
for that. So here, what I can do is I can maybe create a

00:31:54.810 --> 00:31:59.370
variable, a w one and B one. And for that, I'm going to give

00:31:59.370 --> 00:32:03.510
a value. So initial variable, uh, for weight is equal to,

00:32:03.590 --> 00:32:06.550
let's suppose 0.1, I'm going to consider and then zero.

00:32:06.970 --> 00:32:13.650
Okay. Okay. Then w two and a bias two. I'm just creating

00:32:13.650 --> 00:32:20.610
some like a random values over here. 0.1 minus 0.1 and then

00:32:20.610 --> 00:32:28.410
0.0 is the bias I'm going to consider 0.0. Then I'm going to

00:32:28.410 --> 00:32:37.790
take a w three and w four. Where is my w w three and w four.

00:32:38.170 --> 00:32:49.350
So 0.1 comma 0.1 and then bias three is equals to 0.0. So

00:32:49.350 --> 00:32:52.630
what I'm trying to say is that I'm talking about, uh, this

00:32:52.630 --> 00:32:55.630
kind of a neural network, right? So where we have a hidden

00:32:55.630 --> 00:32:58.490
layer inside hidden layer, we have two neurons. So that's

00:32:58.490 --> 00:33:00.930
the reason. So we have B one and B two, and then we have

00:33:00.930 --> 00:33:03.750
something over here. So which will be having a bias. So

00:33:03.750 --> 00:33:06.830
again, output neuron. So here again, we have a B three. So B

00:33:06.830 --> 00:33:08.870
one, B two, B three. So this is something that I'm going to

00:33:08.870 --> 00:33:14.650
consider as simple as that. Okay. So then we can try to take

00:33:14.650 --> 00:33:18.270
a learning rate initially. So learning rate is going to be 0

00:33:18.270 --> 00:33:23.610
.01, right? I'm going to take, okay. So these are the

00:33:23.610 --> 00:33:26.550
variable, which I'm going to consider for my neural network.

00:33:26.670 --> 00:33:30.570
And then if I am trying to create a hidden layer, so

00:33:30.570 --> 00:33:33.010
obviously I have to give an activation function. So

00:33:33.010 --> 00:33:35.650
activation function wise, I'm going to consider a relu

00:33:35.650 --> 00:33:38.370
activation function, right? Right. Relu activation function.

00:33:38.570 --> 00:33:41.650
So we know that, that what relu activation function does. So

00:33:41.650 --> 00:33:45.270
basically relu activation function, if data is positive, it

00:33:45.270 --> 00:33:49.750
is going to give you always like a maximum, right? And if

00:33:49.750 --> 00:33:51.910
data is negative, so in that case, it is going to give you a

00:33:51.910 --> 00:33:54.690
zero as simple as that. So I can try to create my own

00:33:54.690 --> 00:34:02.490
function. So relu stating that, that return, return, return

00:34:02.490 --> 00:34:08.550
what? So maximum for a zero till zero. All the input. So

00:34:08.550 --> 00:34:13.330
input, I have to give basically, so input is equals to,

00:34:13.390 --> 00:34:18.930
let's suppose X. So try to return me zero to X over here. So

00:34:18.930 --> 00:34:23.730
this is my relu function, which I have considered now. So I

00:34:23.730 --> 00:34:27.570
have to even give a derivative of a relu because I have to

00:34:27.570 --> 00:34:29.690
like write a function even for like a finding out the

00:34:29.690 --> 00:34:32.070
derivative of the relu, because this is what is going to

00:34:32.070 --> 00:34:35.070
happen into a backward propagation. I have to use a

00:34:35.070 --> 00:34:37.250
derivative of it. So what will be the derivative? Derivative

00:34:37.250 --> 00:34:40.330
of relu. So if you are going to find out the derivative of

00:34:40.330 --> 00:34:40.710
relu.

00:34:44.080 --> 00:34:48.260
So relu underscore derivative,

00:34:48.940 --> 00:34:53.180
derivative, just a spelling I'm going to write. So if there

00:34:53.180 --> 00:34:58.860
is a X, so in that case, it is going to give me return one.

00:34:59.680 --> 00:35:04.400
If X is greater than zero else, it is going to return zero

00:35:04.400 --> 00:35:07.240
as simple as that. Yeah. Yeah. So this is what our

00:35:07.240 --> 00:35:10.380
derivative of relu is going to be. Agree guys, everyone.

00:35:14.710 --> 00:35:15.110
Yep.

00:35:22.380 --> 00:35:22.780
Agree.

00:35:41.540 --> 00:35:45.580
Okay. Move ahead. So derivative of relu, we are able to find

00:35:45.580 --> 00:35:48.260
out now, same thing we'll do. So we are going to run through

00:35:48.260 --> 00:35:50.580
a for loop. So maybe for a hundred epoch, maybe for a

00:35:50.580 --> 00:35:53.500
thousand epochs. So I'll try to run this entire operations

00:35:53.500 --> 00:35:57.900
for multiple iterations. And then eventually we'll try to

00:35:57.900 --> 00:36:03.280
train the entire model so that that's a whole objective that

00:36:03.280 --> 00:36:08.820
we are going to run. Okay. So let's see how we can do it. So

00:36:08.820 --> 00:36:12.280
we have a relu, we have a derivative of a relu. Now let's

00:36:12.280 --> 00:36:16.920
run this entire things. So inside the epoch. So for epoch,

00:36:16.920 --> 00:36:20.560
epoch in range, maybe I can run it for a hundred times,

00:36:20.680 --> 00:36:26.080
maybe for a thousand times as per my wish. Now total loss is

00:36:26.080 --> 00:36:30.680
equals to zero, same approach. So I have not done anything

00:36:30.680 --> 00:36:33.660
much over here. So I'm just trying to add a new layer in

00:36:33.660 --> 00:36:36.180
between. That's the only thing which I'm trying to do. So

00:36:36.180 --> 00:36:44.780
for X comma Y, so in JIP and JIP of what? So JIP my input

00:36:44.780 --> 00:36:51.020
and my output, O U T P U T output. I believe that's a

00:36:51.020 --> 00:36:54.660
variable name. Yeah. So input and output. So JIP my input

00:36:54.660 --> 00:36:58.000
and JIP my output fine.

00:36:59.300 --> 00:37:03.360
Now so once I will be inside a loop. Obviously I have to

00:37:03.360 --> 00:37:06.180
write the forward pass. Now what is the meaning of this like

00:37:06.180 --> 00:37:10.080
a forward pass by the way? So meaning of this forward pass

00:37:10.080 --> 00:37:13.280
is I will try to pass a data, maybe two data at a time,

00:37:13.320 --> 00:37:16.260
maybe 10 data at a time depends, depends what is the number

00:37:16.260 --> 00:37:19.180
of data which I would like to pass. So maybe I can try to

00:37:19.180 --> 00:37:21.640
pass four data at a time. Maybe I can try to pass two data

00:37:21.640 --> 00:37:25.340
at a time. So once I'm going to pass a data, let me redraw

00:37:25.340 --> 00:37:26.400
this entire things.

00:37:29.380 --> 00:37:33.540
So let's suppose we are trying to pass a data. So this is my

00:37:33.540 --> 00:37:38.280
input, this is my input. So I'm trying to pass a data over

00:37:38.280 --> 00:37:42.980
here and then this is my output. So I'll just try to pass a

00:37:42.980 --> 00:37:46.140
data and in a forward pass, I'll try to do a math over here.

00:37:46.240 --> 00:37:51.360
So let me write down that part. So here let's suppose I'm

00:37:51.360 --> 00:37:56.640
going to name it as a Z1. So Z1 is nothing but a W1 into X

00:37:56.640 --> 00:38:04.700
data plus B1, a bias one. And then I can say that a H1

00:38:04.700 --> 00:38:09.400
hidden layer, a relu, I can try to call the function which I

00:38:09.400 --> 00:38:13.340
have already created, and then I can try to pass a Z1 over

00:38:13.340 --> 00:38:20.120
there. And then I can try to give a Z2. So Z2 is nothing but

00:38:20.120 --> 00:38:31.360
a W2 into X plus B2

00:38:31.360 --> 00:38:38.560
plus B1. And then H2 is nothing but a relu and then inside

00:38:38.560 --> 00:38:42.360
that, try to pass Z2. Now if you have to visualize this

00:38:42.360 --> 00:38:44.960
entire things, maybe you can try to visualize in this way

00:38:45.820 --> 00:38:49.960
and you can try to extend it even on as per your wish. So

00:38:49.960 --> 00:38:52.820
here let's suppose I'm giving an input, right? So let's

00:38:52.820 --> 00:38:55.960
suppose I'm giving just one input over here and we have two

00:38:55.960 --> 00:38:59.000
hidden layers, two hidden layers. So this is going over

00:38:59.000 --> 00:39:01.180
here, this is going over here. So let's suppose I'm trying

00:39:01.180 --> 00:39:03.820
to pass X. Over here, one single data I'm trying to pass.

00:39:04.060 --> 00:39:08.120
Now we have a hidden layer. So hidden neuron one, hidden

00:39:08.120 --> 00:39:12.860
neuron two, for example, and here we have a W1 and here we

00:39:12.860 --> 00:39:16.800
have a W2. Here we have a bias one and here we have a bias

00:39:16.800 --> 00:39:22.580
two. Now so inside that we have a relu and inside that we

00:39:22.580 --> 00:39:25.900
have a relu as a activation function. So same thing I have

00:39:25.900 --> 00:39:30.800
written over here. So the Z1 at this point of a time. So

00:39:30.800 --> 00:39:35.000
what is going to be? Z1, Z1 is nothing but X into W1 plus

00:39:35.000 --> 00:39:37.960
B1. So this is what I have written X into W1 plus B1 and

00:39:37.960 --> 00:39:41.220
then passing this Z1 into where? Into my hidden layer. I'm

00:39:41.220 --> 00:39:44.600
just trying to name it as a H1, right? Hidden layer into my

00:39:44.600 --> 00:39:47.840
relu I'm trying to pass. Now same will go for here, same

00:39:47.840 --> 00:39:51.780
will go for this one. So I'm trying to pass X, X into W2

00:39:51.780 --> 00:39:55.780
plus biases and here, so just before sending this data into

00:39:55.780 --> 00:40:00.080
the hidden, so there will be a Z2. Now this Z2 will go

00:40:00.080 --> 00:40:02.820
inside which one? This Z2 will go inside this activation

00:40:02.820 --> 00:40:06.840
function relu. Is it making sense with the code that we have

00:40:06.840 --> 00:40:13.790
written over here? Now if you have to pass two input at a

00:40:13.790 --> 00:40:16.470
time, if you have to pass three input at a time, then

00:40:16.470 --> 00:40:20.470
similarly you can try to define your weights. So this and

00:40:20.470 --> 00:40:24.250
this connection, then this and this connection, so you can

00:40:24.250 --> 00:40:26.410
try to define the weights and then you can keep on sending a

00:40:26.410 --> 00:40:30.530
data inside this one. Is it making sense to all of us? Yeah.

00:40:30.750 --> 00:40:33.890
A simple one, right? A simple code I have written. I believe

00:40:33.890 --> 00:40:38.230
you all are able to visualize it even. So depends like how

00:40:38.230 --> 00:40:40.990
we are like trying to like a build our network in the same

00:40:40.990 --> 00:40:44.050
way we can try to write that code over here. So I believe

00:40:44.050 --> 00:40:48.250
I'm able to explain you this set of the code now. So once

00:40:48.250 --> 00:40:52.490
I'm able to pass, so inside the hidden layer, once I'm able

00:40:52.490 --> 00:40:57.190
to send the data, now it is supposed to do a prediction.

00:40:57.390 --> 00:41:01.390
Okay. So now let's talk about same network. Let me remove

00:41:01.390 --> 00:41:07.230
this. This one. Yeah. Now, so I'm able to pass a data inside

00:41:07.230 --> 00:41:11.430
my relu function. It is going to give you some output. It is

00:41:11.430 --> 00:41:15.910
also going to give you some output, right? Now here we will

00:41:15.910 --> 00:41:20.810
be having W3 and here we'll be having a W4, weight 3 and

00:41:20.810 --> 00:41:24.950
weight 4. Now whatever output you are getting here, whatever

00:41:24.950 --> 00:41:28.390
output that you are getting here, that will be multiplied

00:41:28.390 --> 00:41:31.810
with W3. That will be multiplied with W4. That will be

00:41:31.810 --> 00:41:32.510
multiplied with W3. That will be multiplied with W4 and then

00:41:32.510 --> 00:41:37.170
added with bias. We have a bias over here, added with the

00:41:37.170 --> 00:41:42.010
biases and that is going to be the output. That is going to

00:41:42.010 --> 00:41:46.730
be basically Y prediction. Can I say that guys? Yeah. Can I

00:41:46.730 --> 00:41:49.810
say that? That is going to be the prediction by the way.

00:41:54.890 --> 00:41:57.550
Yes. So Sam is saying just we are replicating the neuron

00:41:57.550 --> 00:42:00.550
work. Exactly. Yes. Yeah. So just we are replicating the

00:42:00.550 --> 00:42:05.570
neuron work. So here, a Y prediction. Y prediction is

00:42:05.570 --> 00:42:10.670
nothing but. Yes. So Y prediction is nothing but a W3 into

00:42:10.670 --> 00:42:22.650
H1 plus a W4 into H2 plus B3. Yeah. So this is the

00:42:22.650 --> 00:42:26.230
prediction that you all will be able to get. Now just check

00:42:26.230 --> 00:42:28.510
with this network that we have drawn. Is it correct guys?

00:42:30.090 --> 00:42:32.750
Yeah. The Y prediction that I have written, is it correct?

00:42:41.300 --> 00:42:44.840
Yes. Yes, everyone. I think it's correct.

00:42:52.780 --> 00:42:56.740
Okay. So yeah, it's like a perfectly fine. The Y prediction

00:42:56.740 --> 00:42:59.580
that we are trying to write over here. Now once we are able

00:42:59.580 --> 00:43:02.680
to get Y prediction, then what? What is the next step for

00:43:02.680 --> 00:43:07.660
us? So once I am able to get a Y prediction, my next step is

00:43:07.660 --> 00:43:11.940
to calculate the error. Yeah. My next step is supposed to

00:43:11.940 --> 00:43:16.780
calculate the error. So here, what we can do is, so we can

00:43:16.780 --> 00:43:19.560
try to calculate the error. so what is the error by the way

00:43:19.560 --> 00:43:23.560
so error is nothing but y prediction that we are able to get

00:43:23.560 --> 00:43:28.120
minus y the actual y right the actual y this is going to be

00:43:28.120 --> 00:43:31.700
the error now loss so i'm just trying to like use a mean

00:43:31.700 --> 00:43:37.020
squared itself so here loss is nothing but your error square

00:43:37.020 --> 00:43:39.440
let's suppose you can try to use any other loss function

00:43:39.440 --> 00:43:43.180
depends upon you right depends upon you completely now what

00:43:43.180 --> 00:43:49.160
is the total loss so total loss is equals to total loss plus

00:43:49.160 --> 00:43:53.340
this particular loss okay fine that's again we are able to

00:43:53.340 --> 00:43:57.680
do it now so loss we are able to calculate total loss we are

00:43:57.680 --> 00:44:00.180
able to calculate now we will start moving into a backward

00:44:00.180 --> 00:44:05.840
direction yeah we have to update our like a biases b3 w3 w4

00:44:05.840 --> 00:44:09.120
b3 i think i have not written i should mention over here as

00:44:09.120 --> 00:44:13.040
a b3 yeah so basically we have to go into a backward

00:44:13.040 --> 00:44:13.040
direction and we have to go into a backward direction and we

00:44:13.040 --> 00:44:15.520
have to or we have to move into a backward pass and we have

00:44:15.520 --> 00:44:21.240
to change a b3 we have to change w3 w4 b1 b2 and w1 and w2

00:44:21.240 --> 00:44:24.640
so everything we have to change and that is possible when we

00:44:24.640 --> 00:44:27.760
are trying to do a backward pass right so in a backward pass

00:44:27.760 --> 00:44:29.880
so we are supposed to like change it and change it with

00:44:29.880 --> 00:44:35.140
respect to the loss that we have received so far yep okay so

00:44:35.140 --> 00:44:40.560
here right so here we can try to like a write a code in this

00:44:40.560 --> 00:44:46.780
way so a variable d l underscore d y d

00:44:49.370 --> 00:44:58.710
l underscore d y is equals to 2 into error i can try to

00:44:58.710 --> 00:45:03.450
write same theory so loss d l y like a y so when you are

00:45:03.450 --> 00:45:05.930
going to calculate it so you will be able to get 2 into

00:45:05.930 --> 00:45:13.690
error okay now so here d l with a w3 d l with w4 d l with b3

00:45:13.690 --> 00:45:18.430
we have to calculate so it's nothing but so derivation of

00:45:18.430 --> 00:45:23.530
like a d l with w3 so i'm going to write it down w3 is

00:45:23.530 --> 00:45:33.990
equals to this d l by d y into h1 the h1 that we are we have

00:45:33.990 --> 00:45:42.630
received so this is the h1 now d of w4 is nothing but d l d

00:45:42.630 --> 00:45:45.890
y into h2

00:45:48.070 --> 00:45:55.930
and derivative with db3 is going to be d l by d y so now we

00:45:55.930 --> 00:45:58.150
know the derivative and this is the only thing which is

00:45:58.150 --> 00:46:02.050
required so that i will be able to modify the things i will

00:46:02.050 --> 00:46:04.830
be able to modify the thing so i'm going to modify w3 i'm

00:46:04.830 --> 00:46:08.910
going to modify w4 i'm going to modify even my b3 now

00:46:08.910 --> 00:46:14.650
similarly right similarly so what we can do is we have to

00:46:14.650 --> 00:46:18.370
calculate even a gradient for a gradient for a hidden layer

00:46:18.370 --> 00:46:21.970
yeah so for this h1 and h2 we have to calculate a gradient

00:46:21.970 --> 00:46:26.190
so for h1 and h2 if we have to calculate the gradient so to

00:46:26.190 --> 00:46:31.650
calculate a gradient for example d h1 so it is going to be d

00:46:31.650 --> 00:46:37.450
l by d y into w3

00:46:37.450 --> 00:46:46.490
if i'm not wrong yeah and derivative of h2 h2 is going to be

00:46:46.490 --> 00:46:46.890
d l by d y into w3 if i'm not wrong yeah and derivative of

00:46:46.890 --> 00:46:51.030
h2 h2 is going to be d l by d y into w3

00:46:52.600 --> 00:46:53.280
if i'm not wrong yeah and derivative of h2 h2 is going to be

00:46:53.280 --> 00:46:55.060
d l by d y into w3 w4 is connected with w4 so it's going to

00:46:55.060 --> 00:46:57.080
be w4 okay

00:46:59.700 --> 00:47:09.480
now so now for like a z this one we have to calculate the

00:47:09.480 --> 00:47:12.680
derivative so that i will be able to change a value of w1

00:47:12.680 --> 00:47:16.680
and w2 okay let's do it then which depends upon eventually

00:47:16.680 --> 00:47:24.720
on a relu activation so d of z1 is equals to d of h1 which i

00:47:24.720 --> 00:47:30.920
have already created d of h1 okay into relu derivative and

00:47:30.920 --> 00:47:39.780
inside that i'm going to pass my z1 and d of z2 the input

00:47:39.780 --> 00:47:46.220
inside a relu is nothing but d h2 into relu derivative and

00:47:46.220 --> 00:47:53.680
here i'm going to pass basically z2 okay now so changes in

00:47:53.680 --> 00:48:03.680
w1 is going to be d of z1 into x same first line principle

00:48:05.180 --> 00:48:14.320
db1 is equals to dz1 okay so this is for z1 now same i can

00:48:14.320 --> 00:48:18.840
try to do a copy and paste for my z2 just i'll change the

00:48:18.840 --> 00:48:29.180
variable name so here like a two two two and then two okay

00:48:29.180 --> 00:48:33.220
now all these things are done so derivative calculation and

00:48:33.220 --> 00:48:35.920
everything is done now we can try to write directly changes

00:48:35.920 --> 00:48:40.720
into weight and biases so w1 w2 w3 w4 four weights and b1 b2

00:48:40.720 --> 00:48:43.860
b3 four three biases so basically we can we can try to write

00:48:43.860 --> 00:48:46.820
because whatever derivative is required to do the

00:48:46.820 --> 00:48:51.540
calculation so we know that derivative so w1 is equals to w1

00:48:51.540 --> 00:48:56.640
minus w1 new is equal to w1 old minus eta which is nothing

00:48:56.640 --> 00:48:59.980
but a learning rate learning rate we have already fixed into

00:48:59.980 --> 00:49:06.560
dw1 dw1 calculation we have already done now similarly so

00:49:06.560 --> 00:49:09.460
weight two if we have to change so new weight two is nothing

00:49:09.460 --> 00:49:16.560
but old weight two minus learning rate into dw2 already

00:49:16.560 --> 00:49:20.580
created now weight three is nothing but new weight three is

00:49:20.580 --> 00:49:26.880
nothing but old weight three minus learning rate into dw3

00:49:26.880 --> 00:49:33.040
now weight four is nothing but new weight four is a old

00:49:33.040 --> 00:49:41.560
weight four minus learning rate into dw3 sorry dw4 it'll

00:49:41.560 --> 00:49:46.600
look for and uh okay so all four weights has been updated i

00:49:46.600 --> 00:49:50.860
can now try to update my b1 b2 and b3 so new b1 is nothing

00:49:50.860 --> 00:50:04.740
but old b1 uh minus learning rate into dw1 now b2 is nothing

00:50:04.740 --> 00:50:13.080
but b2 minus learning rate into uh dw2 and

00:50:15.610 --> 00:50:19.770
then uh b3 we have three biases so b2 and b3 of B2 and dw3

00:50:19.770 --> 00:50:21.070
and then b3 we have three biases so old biases minus

00:50:21.070 --> 00:50:24.990
learning rate into dw2 of old biases minus learning rate and

00:50:24.990 --> 00:50:29.550
old three okay so this way i will be able to update our

00:50:29.550 --> 00:50:32.350
weights now my work is done i don't have to do anything and

00:50:32.350 --> 00:50:36.890
i just have to write the loop so that i will be able to like

00:50:36.890 --> 00:50:40.830
a print what is happening how things are changing how losses

00:50:40.830 --> 00:50:44.990
are changing how weights are changing uh in every epoch so i

00:50:44.990 --> 00:50:47.530
can try to write or maybe i can copy and paste the same line

00:50:47.530 --> 00:50:51.870
instead of writing again so yeah i can copy and paste the

00:50:51.870 --> 00:50:53.810
same line over here make

00:51:02.670 --> 00:51:06.350
the indentation proper okay so epoch 10 so it is going to

00:51:06.350 --> 00:51:10.930
print epoch total loss is the same one and i don't have just

00:51:10.930 --> 00:51:16.450
a one single w so i have w1 w2 w3 and b let's just try to

00:51:16.450 --> 00:51:20.050
print loss in this epoch you can try to print w1 w2 w3 w4

00:51:20.050 --> 00:51:23.730
all these things and again so you will be able to print your

00:51:23.730 --> 00:51:28.030
biases as well now so once this is done once this will be

00:51:28.030 --> 00:51:28.170
done you will be able to print your biases as well now so

00:51:28.170 --> 00:51:30.430
once this is done so what we can do is so we can try to

00:51:30.430 --> 00:51:35.750
write a prediction function so def p-r-e-d-i-c-t-i-o-n

00:51:35.750 --> 00:51:38.050
prediction function i can write a custom prediction function

00:51:38.050 --> 00:51:41.990
so where i'm going to pass one value of x and then it should

00:51:41.990 --> 00:51:44.630
give me the final outcome so how it is going to give me a

00:51:44.630 --> 00:51:48.550
prediction so if i'm going to pass x so x into w1 that will

00:51:48.550 --> 00:51:52.270
go into the relu x into w2 plus biases going to the relu

00:51:52.270 --> 00:51:55.750
again so entire like a function we have to write basically

00:51:55.750 --> 00:51:59.550
over here so that i will be able to like a so whatever we

00:51:59.550 --> 00:52:04.720
have written over here for doing this prediction so that is

00:52:04.720 --> 00:52:08.280
something that we have to copy and paste even over here so

00:52:08.280 --> 00:52:13.300
it is going to calculate z1 z2 h1 h2 and then it is going to

00:52:13.300 --> 00:52:22.520
basically calculate a y prediction fine so z1 h1 z2 h2 looks

00:52:22.520 --> 00:52:28.580
fine that's fine so return y

00:52:32.140 --> 00:52:32.500
predicted in this way if you enter your prediction function

00:52:32.500 --> 00:52:32.720
you will get  predict then for some reasons i have already

00:52:32.720 --> 00:52:42.860
used y pred so let me name it as y so y output and y output

00:52:42.860 --> 00:52:45.060
so this is my prediction function whatever data which i am

00:52:45.060 --> 00:52:46.840
going to pass i will be passing inside my prediction

00:52:46.840 --> 00:52:49.880
functions so once this entire training will be done and then

00:52:49.880 --> 00:52:55.720
i can try to call my prediction function so on the call

00:52:55.720 --> 00:52:57.380
stick shot test sound for the prediction structure i I can

00:52:57.380 --> 00:53:03.580
try to pass maybe 9 just to check and I'll be able to see

00:53:03.580 --> 00:53:08.160
the output. So now let's execute it. So I have passed 9 and

00:53:08.160 --> 00:53:13.380
it is able to give me 17, ideally 17.99. So it looks like

00:53:13.380 --> 00:53:17.020
that it is able to learn. It is able to learn in a perfect

00:53:17.020 --> 00:53:21.660
way and it is giving me almost close to a perfect result. So

00:53:21.660 --> 00:53:26.260
9 into 2 is equal to 18. This was my expectation. And yeah,

00:53:26.400 --> 00:53:29.460
it is, it is able to learn, right. It is able to learn based

00:53:29.460 --> 00:53:32.820
out of the entire network that we have written on ourself.

00:53:35.810 --> 00:53:37.570
Yes. Fine guys, everyone.

00:53:44.290 --> 00:53:48.510
So now pinging you the entire code guys. So this is the code

00:53:48.510 --> 00:53:52.530
that we have written. So again, if you will try to resemble

00:53:52.530 --> 00:53:55.650
with a theory, it's exactly same. There is no difference

00:53:55.650 --> 00:53:59.190
which you will be able to find out over here. So it's that

00:53:59.190 --> 00:54:01.530
same thing which we have written even in this particular

00:54:01.530 --> 00:54:06.070
place. Now. So this is just by using a core Python, a

00:54:06.070 --> 00:54:09.190
vanilla code that we have written, right? So it doesn't

00:54:09.190 --> 00:54:13.070
matter how complex a network is going to be. I will be able

00:54:13.070 --> 00:54:16.490
to write each and everything in the similar manner, right?

00:54:17.030 --> 00:54:20.510
Now what we can do is we can try to implement the exact the

00:54:20.510 --> 00:54:24.870
same thing with the help of maybe a library, maybe a

00:54:24.870 --> 00:54:28.330
TensorFlow I can try to use, and then I can try to write the

00:54:28.330 --> 00:54:30.870
exact same thing. Maybe I can try to use a PyTorch. And then

00:54:30.870 --> 00:54:35.330
you will see. That. How easy your life is. So of writing

00:54:35.330 --> 00:54:38.490
like a, these may not have a code. Maybe within like a four

00:54:38.490 --> 00:54:41.010
to five line of a code, I will be able to perform the exact

00:54:41.010 --> 00:54:44.330
same operation. So let's do it. Same operation, but with the

00:54:44.330 --> 00:54:48.530
help of library. So I'm going to use a TensorFlow library.

00:54:48.670 --> 00:54:51.610
So TensorFlow is a Pythonic library. Even TensorFlow

00:54:51.610 --> 00:54:54.470
supports a JS JavaScript, but I'm not going to use

00:54:54.470 --> 00:54:58.970
JavaScript over here as I assume that we all understand just

00:54:58.970 --> 00:54:59.990
a Python programming.

00:55:03.910 --> 00:55:08.890
TensorFlow. So import TensorFlow as TF. And if you don't

00:55:08.890 --> 00:55:11.650
have a TensorFlow in your system, if it is going to give you

00:55:11.650 --> 00:55:15.050
error that no model found call as TensorFlow, then do pip

00:55:15.050 --> 00:55:21.030
install TensorFlow. As simple as that. Import numpy N U M P

00:55:21.030 --> 00:55:26.930
Y as N P. Okay. So just do an import. So in my system

00:55:26.930 --> 00:55:29.950
TensorFlow is available, there is a very high chance that

00:55:29.950 --> 00:55:32.950
TensorFlow will not be available inside your system. Just,

00:55:33.170 --> 00:55:36.770
you have to do pip install TensorFlow in a cell. That's it.

00:55:36.930 --> 00:55:39.950
In a cell, just do pip install TensorFlow and you will be

00:55:39.950 --> 00:55:41.870
able to install it. Let me ping you the code guys.

00:55:46.540 --> 00:55:50.980
Yeah. So I've just pinged you the code. Now I'll be taking

00:55:50.980 --> 00:55:54.820
the same data. I'll be taking the exact same data, uh, the

00:55:54.820 --> 00:55:58.980
data that I have already taken. So input is this data,

00:55:59.100 --> 00:56:05.700
right? And my output is basically, sorry, my output is this

00:56:05.700 --> 00:56:07.960
data. So I'll be taking the same data. I'm not going to

00:56:07.960 --> 00:56:11.340
change the data at all. So here, what I can do is I can try

00:56:11.340 --> 00:56:15.420
to prepare my X and Y. So my X is going to be numpy dot

00:56:15.420 --> 00:56:19.800
array. So in the, in a numpy array format, I can try to

00:56:19.800 --> 00:56:23.920
convert it. So I can try to convert input into a numpy. I

00:56:23.920 --> 00:56:28.260
can try to write D type. So data type is equal to float.

00:56:28.520 --> 00:56:31.220
Let's suppose I'm converting my integer into a float format

00:56:31.220 --> 00:56:35.380
because it's important for a vector computation. And my Y is

00:56:35.380 --> 00:56:40.260
equals nothing. But numpy dot array, and then I can try to

00:56:40.260 --> 00:56:46.860
write output and D type is nothing but float again. Okay. So

00:56:46.860 --> 00:56:49.860
same data I have considered input and output the data, which

00:56:49.860 --> 00:56:51.760
I have considered in my previous example, I've just

00:56:51.760 --> 00:56:55.560
converted this data into a numpy compatible format so that

00:56:55.560 --> 00:56:57.940
my TensorFlow will be able to understand because the meaning

00:56:57.940 --> 00:57:03.120
of tensors is nothing but a vector basically. So it is very

00:57:03.120 --> 00:57:05.920
much closer to the numpy array that you used to create. So

00:57:05.920 --> 00:57:08.480
formatting, obviously, you have to take care of it, you have

00:57:08.480 --> 00:57:12.560
to convert in a proper format. Okay, now what we can do is

00:57:12.560 --> 00:57:16.600
so once my data set is ready, which is ready, all already,

00:57:16.740 --> 00:57:19.960
it's available already, it's ready. So now we can try to

00:57:19.960 --> 00:57:24.060
create a network where I have two hidden layers, one output

00:57:24.060 --> 00:57:27.740
layer inside a hidden layer, I have a relu function, right

00:57:27.740 --> 00:57:30.400
inside the hidden layer, I have a relu function. So I will

00:57:30.400 --> 00:57:33.440
try to create a network over here. So how I can create a

00:57:33.440 --> 00:57:40.040
network. So tensorflow dot keras. Okay. Keras dot I can try

00:57:40.040 --> 00:57:42.560
to call a sequential so sequential model we are trying to

00:57:42.560 --> 00:57:45.840
create, you have a multiple options over here, by the way.

00:57:45.960 --> 00:57:49.700
So sequential activation back end callbacks, there are so

00:57:49.700 --> 00:57:53.040
many options which keras is going to give you, I'm going to

00:57:53.040 --> 00:57:55.680
create a sequential model means layer by layer model. So

00:57:55.680 --> 00:57:58.640
after one layer, another layer will come into a picture. So

00:57:58.640 --> 00:58:02.520
keras and then sequential I can try to call. Okay. Now

00:58:02.520 --> 00:58:05.800
inside that. Inside. Inside this particular, like a

00:58:05.800 --> 00:58:10.320
function, I can try to create a list kind of a structure and

00:58:10.320 --> 00:58:13.780
then I can try to define the layers. Now what is my very

00:58:13.780 --> 00:58:16.800
first layer? Any idea guys? What is my first layer? So can I

00:58:16.800 --> 00:58:21.260
say that in my very first layer, I have two hidden neurons.

00:58:22.020 --> 00:58:25.840
Yeah. In my very first layer, I have two hidden neurons. So

00:58:25.840 --> 00:58:28.940
I will write simply two over here in a very first layer, I

00:58:28.940 --> 00:58:31.840
have two hidden neurons. So I'll simply write two over here,

00:58:31.880 --> 00:58:35.700
but not directly, not in this way. So tensorflow.keras

00:58:35.700 --> 00:58:39.140
.layer.

00:58:40.630 --> 00:58:45.350
So all this inbuilt functions are available dot dense. So

00:58:45.350 --> 00:58:48.230
dense. So I'm just trying to create the very first dense

00:58:48.230 --> 00:58:50.670
layer, a very first dense layer, which is technically a

00:58:50.670 --> 00:58:55.530
hidden layer. So here the very first layer that I have is

00:58:55.530 --> 00:59:00.390
having two neurons. So this dense takes number of units, as

00:59:00.390 --> 00:59:03.070
you can see, this dense takes activation functions,

00:59:03.310 --> 00:59:06.250
everything. Right? You can try to modify according to you.

00:59:06.330 --> 00:59:09.050
So unit means what is the number of neuron that you are

00:59:09.050 --> 00:59:11.750
looking for? So I'm looking for two neurons. Basically I can

00:59:11.750 --> 00:59:13.770
even look for four neuron, eight neuron. That's completely

00:59:13.770 --> 00:59:16.730
fine. Activation function wise, what is the activation

00:59:16.730 --> 00:59:19.570
function that I'm looking for? So I'm looking for ReLU

00:59:19.570 --> 00:59:22.170
activation function. So okay, fine. ReLU activation function

00:59:22.170 --> 00:59:27.470
I have written and then like a, I can, I can try to like a

00:59:27.470 --> 00:59:31.290
give the input shape. So what is the input which I will try

00:59:31.290 --> 00:59:36.950
to give? So inside this one inputs. I N P U T input

00:59:36.950 --> 00:59:41.130
underscore shape. So I would like to give one by one input.

00:59:41.250 --> 00:59:44.830
So 1D vector I am given input shape is this one 1D vector

00:59:44.830 --> 00:59:48.830
simple. So this will create the very first layer guys, the

00:59:48.830 --> 00:59:51.030
very first hidden layer that you are able to see. So this is

00:59:51.030 --> 00:59:55.050
going to create that one. Now the next layer inside the

00:59:55.050 --> 00:59:58.110
list. So what is the next layer inside the list? So my next

00:59:58.110 --> 01:00:01.670
layer is nothing but my output layer basically. Yeah. So in

01:00:01.670 --> 01:00:03.530
my output layer, what is the total number of neurons? Only

01:00:03.530 --> 01:00:11.290
one neuron I have. So tf dot tensorflow dot keras dot layers

01:00:11.290 --> 01:00:16.230
dot another dense layer. Yeah. And then here I can write

01:00:16.230 --> 01:00:20.490
one. Now this is going to create this network. Only this

01:00:20.490 --> 01:00:24.210
much is going to create this entire network for me. So this

01:00:24.210 --> 01:00:27.330
is my output layer and this is my hidden layer. And inside

01:00:27.330 --> 01:00:30.110
this very first hidden layer, I'm trying to give a pass a

01:00:30.110 --> 01:00:33.410
data. So it will be able to understand that. Now here, let's

01:00:33.410 --> 01:00:35.810
try to save into a variable called as model. You can take

01:00:35.810 --> 01:00:40.150
any variable name, that's fine. So model M anything is fine.

01:00:40.330 --> 01:00:43.710
Yeah. So inside a variable called as model, I'm going to

01:00:43.710 --> 01:00:46.050
save it. So let me ping you this piece of a code guys.

01:00:48.880 --> 01:00:50.900
Deepak is saying, can you please, I think Deepak it's done,

01:00:51.020 --> 01:00:55.240
right? And for everyone, I believe it's done.

01:01:00.180 --> 01:01:02.200
Yep. Everyone, please

01:01:05.340 --> 01:01:08.720
ping me in the chat guys. If you're able to follow along.

01:01:26.000 --> 01:01:28.160
Aman is saying, but we always can't do that. We can't always

01:01:28.160 --> 01:01:31.600
count a tensors, right? Like you have, are you talking about

01:01:31.600 --> 01:01:33.460
a hidden layer? A tensor means I'm talking about the data,

01:01:33.560 --> 01:01:38.100
right? That is the actual meaning of tensors. Now we try to

01:01:38.100 --> 01:01:40.660
control the inputs, basically how many inputs we have to

01:01:40.660 --> 01:01:43.540
give. Obviously we, we always try to control that part

01:01:44.940 --> 01:01:49.720
again. So here model we are able to create. Now once we are

01:01:49.720 --> 01:01:52.920
able to create the model, what we can do is so we can try to

01:01:52.920 --> 01:01:55.920
give other parameter for the model so that my model will be

01:01:55.920 --> 01:01:59.160
able to execute itself. Now what is the other parameter? So

01:01:59.160 --> 01:02:03.020
obviously I have to define my loss function, right? I have

01:02:03.020 --> 01:02:06.080
to define basically my optimizer. So which will help me out

01:02:06.080 --> 01:02:08.880
to calculate the loss and then back propagate. I have to

01:02:08.880 --> 01:02:11.240
define those things as well. This is just a architecture

01:02:11.240 --> 01:02:15.080
creation. So this is basically just a model, right? So

01:02:15.080 --> 01:02:17.420
obviously model is going to give me some prediction. I have

01:02:17.420 --> 01:02:19.880
to go ahead with a backward propagation in a backup

01:02:19.880 --> 01:02:22.240
propagation. I have to change my weights and biases, all

01:02:22.240 --> 01:02:26.920
those things. Yeah. So here what I can do is some model dot

01:02:26.920 --> 01:02:30.720
compile. I can try to call and here I can try to set those

01:02:30.720 --> 01:02:34.060
things so I can try to set my optimizer over here. Yeah.

01:02:34.300 --> 01:02:37.340
Bollard compile optimizer. So what kind of a cop optimizer I

01:02:37.340 --> 01:02:43.340
would like to consider. So tf dot keras dot keras dot. You

01:02:43.340 --> 01:02:47.000
can try to call optimizers function and inside that you will

01:02:47.000 --> 01:02:50.660
be able to find out all the optimizer function, ada delta,

01:02:50.840 --> 01:02:54.160
ada factor, ada guard, adam, adam max, everything,

01:02:54.320 --> 01:02:56.940
everything you will be able to get. So we are going to

01:02:56.940 --> 01:02:59.600
consider SGD. Stochastic gradient descent, because this is

01:02:59.600 --> 01:03:02.820
what we have discussed mathematically. And then for that

01:03:02.820 --> 01:03:06.580
optimization, I have to define a learning rate as well,

01:03:06.680 --> 01:03:10.460
right? So learning rate is nothing but 0.01. So learning

01:03:10.460 --> 01:03:13.580
rate I'm able to define apart from that. So we have to

01:03:13.580 --> 01:03:16.500
define our loss functions as well. So what is our loss

01:03:16.500 --> 01:03:19.500
function? Mean squared error. So I can try to define mean,

01:03:19.520 --> 01:03:26.300
mean, mean, underscore, S Q U A, S Q U A R E D underscore E

01:03:26.300 --> 01:03:28.660
double ROR. So all this mean squared error and everything is

01:03:28.660 --> 01:03:34.220
already available. So this is going to basically consider as

01:03:34.220 --> 01:03:37.220
a like a parameter. So it is going to consider a stochastic

01:03:37.220 --> 01:03:39.860
gradient descent optimizer, the same optimizer that we have

01:03:39.860 --> 01:03:43.280
mathematically discussed. And then loss function wise, it is

01:03:43.280 --> 01:03:45.720
going to consider this particular loss function learning

01:03:45.720 --> 01:03:47.800
rate wise. This is something that it is going to consider

01:03:47.800 --> 01:03:53.160
automatically. Yeah. So fine. This will try to compile all

01:03:53.160 --> 01:03:56.080
is like fine. Let me ping you this piece of a code now.

01:03:58.000 --> 01:04:02.880
Yeah. Now what we can do is now what we can do. So now we

01:04:02.880 --> 01:04:07.260
can try to send a data. So here I can try to call model dot

01:04:07.260 --> 01:04:12.680
fit basically. And then I can try to pass my X data and my Y

01:04:12.680 --> 01:04:16.500
data so that it will start doing a training for how many

01:04:16.500 --> 01:04:20.000
epoch I would like to like train this data. So I can try to

01:04:20.000 --> 01:04:23.180
give even an epoch. So I want those things to 100 epoch.

01:04:23.340 --> 01:04:25.780
Last times I will, I have written the for loop basically.

01:04:25.880 --> 01:04:28.780
Right. So loop by loop. So I was trying to pass the data a

01:04:28.780 --> 01:04:31.660
hundred times. Fine. There is a parameter called as a

01:04:31.660 --> 01:04:35.160
verbose, which is going to help you out to see the entire

01:04:35.160 --> 01:04:38.100
working internal working and the changes the way we have

01:04:38.100 --> 01:04:40.220
done that with the help of print basically. So this is,

01:04:40.240 --> 01:04:42.540
verbose is nothing but an alternative of the printer

01:04:42.540 --> 01:04:45.680
statement. This is going to print this much. Yeah. So

01:04:45.680 --> 01:04:49.420
verbose is equal to one. If you are going to keep zero, then

01:04:49.420 --> 01:04:53.220
obviously it is not going to print anything. Now, once it

01:04:53.220 --> 01:04:56.120
will be able to fit, it means it will be able to train. When

01:04:56.120 --> 01:05:01.080
in itself, you can do a prediction. The model. Project. You

01:05:01.080 --> 01:05:04.320
can try to call and you can try to pass your own data over

01:05:04.320 --> 01:05:08.180
here as an array. So maybe I can try to pass mine. Even last

01:05:08.180 --> 01:05:11.320
time I have passed mine, it has given me 17.99 as an output.

01:05:11.560 --> 01:05:15.500
Yeah. So I can try to like, uh, call this one and execute

01:05:15.500 --> 01:05:24.960
now as you can see that it is. The bit that I mym. I have

01:05:24.960 --> 01:05:29.400
called this fit basically, it has started executing epoch

01:05:29.400 --> 01:05:32.960
one of 100, two of 100, three of 100, five of 100, six of

01:05:32.960 --> 01:05:37.180
100 and then you will be able to see that it is decreasing a

01:05:37.180 --> 01:05:41.540
loss. So the initial loss was 44 and now it has been

01:05:41.540 --> 01:05:47.540
decreased to 8.65 basically, it has decreased to 8.65 and

01:05:47.540 --> 01:05:51.740
yeah, so I'm trying to pass some data over here for a

01:05:51.740 --> 01:05:52.200
prediction.

01:05:55.380 --> 01:06:01.740
Yeah, so it is going to give me a prediction is equals to 5

01:06:01.740 --> 01:06:06.660
.2. It simply means that my model is not able to learn much

01:06:06.660 --> 01:06:09.860
for nine it is giving me like a let

01:06:13.700 --> 01:06:21.000
me check with 90 I'm expecting 180 by the way, yeah, it is

01:06:21.000 --> 01:06:27.820
not able to do like prediction. So it looks like it is it is

01:06:27.820 --> 01:06:32.840
not able to learn much basically. It is it is not able to

01:06:32.840 --> 01:06:36.580
learn much. And anyhow, I'm able to see that I'm getting a

01:06:36.580 --> 01:06:39.880
huge loss over here. So maybe I can I can try to like change

01:06:39.880 --> 01:06:42.240
a couple of things in this place. So maybe I can try to

01:06:42.240 --> 01:06:48.140
change learning rate to 0.001. Maybe I can try to test with

01:06:48.140 --> 01:06:53.100
that one. Not much of improvement.

01:06:56.860 --> 01:06:59.620
Things are getting overflowed with TensorFlow, I don't think

01:06:59.620 --> 01:07:04.220
that it should overflow. It's one of the easiest way I'm

01:07:04.220 --> 01:07:06.820
like where it is getting overflowed. So we have two layers.

01:07:06.940 --> 01:07:09.260
One is a hidden layer. One is output layer. So we have

01:07:09.260 --> 01:07:12.320
simply created two layers, right? We have simply created two

01:07:12.320 --> 01:07:16.340
layers. And then we have to basically like pass all the

01:07:16.340 --> 01:07:18.380
parameter for the training. So we have to pass a loss

01:07:18.380 --> 01:07:22.280
function, we have to pass basically a learning rate, simple

01:07:22.280 --> 01:07:25.140
and then we have to pass a epoch. So we have passed the

01:07:25.140 --> 01:07:28.460
epoch so that it will be able to start training the entire

01:07:28.460 --> 01:07:31.960
data. So let me train it for 10000. Okay. Let's see whether

01:07:31.960 --> 01:07:38.680
we are able to reduce a loss or not doesn't looks like that

01:07:38.680 --> 01:07:40.260
it is able to reduce the loss by the way.

01:07:43.760 --> 01:07:46.540
Yeah so question guys now I'm open for the question.

01:08:49.090 --> 01:08:53.510
What is SGD? What is optimizer? So please try to attend all

01:08:53.510 --> 01:08:56.730
the class everyone because in every classes I can't start

01:08:56.730 --> 01:09:00.270
from a zero and then start explaining you everything from

01:09:00.270 --> 01:09:03.450
the basic I believe we have already talked about it. So what

01:09:03.450 --> 01:09:06.850
optimizer does and then how it is. So that loss rate of

01:09:06.850 --> 01:09:08.610
change. So what is the change of loss with respect to W and

01:09:08.610 --> 01:09:10.770
B that we are trying to create which was helping me to

01:09:10.770 --> 01:09:13.430
change the values. What was that? That was basically an

01:09:13.430 --> 01:09:17.350
optimizer. And we talked about even I believe like a SGD a

01:09:17.350 --> 01:09:19.890
stochastic gradient descent that step by step one by one so

01:09:19.890 --> 01:09:20.930
it will try to change the things.

01:09:24.200 --> 01:09:28.520
Okay so even after a 10,000 epochs it is not able to make

01:09:28.520 --> 01:09:31.820
any kind of changes. Now what I can do to build a better

01:09:31.820 --> 01:09:36.060
network so maybe I'm taking only two hidden layers I can try

01:09:36.060 --> 01:09:39.520
to increase to 20 right. So that is one of the ways. Okay so

01:09:39.520 --> 01:09:43.620
let me like make a changes and instead of like a 10,000

01:09:43.620 --> 01:09:48.000
epochs I'll just take a thousand epochs. And yeah this time

01:09:48.000 --> 01:09:51.920
it seems like it's working for me as you can see. So I'm

01:09:51.920 --> 01:09:55.760
trying to pass 90 I was expecting basically 180 but yeah I'm

01:09:55.760 --> 01:09:59.920
able to get a close result. So last time my number of

01:09:59.920 --> 01:10:04.000
neurons inside hidden layers was just two I have given. This

01:10:04.000 --> 01:10:07.220
time I have given basically 20. So two was not able to train

01:10:07.220 --> 01:10:10.780
well. So I have given 20 and now I'm able to see that that

01:10:10.780 --> 01:10:16.590
yes it is able to train well fine. What is Barbos? Five

01:10:16.590 --> 01:10:19.650
times I have repeated in the same class itself. So Barbos is

01:10:19.650 --> 01:10:22.410
basically nothing but a parameter by which you are able to

01:10:22.410 --> 01:10:25.230
see this particular result the output inside the console.

01:10:25.550 --> 01:10:28.290
Even I told you that it's basically an alternative of the

01:10:28.290 --> 01:10:32.310
print that you have written this print basically yeah is it

01:10:32.310 --> 01:10:34.250
that tough to understand how

01:10:41.160 --> 01:10:44.360
to find out the best neurons there is no exact and accurate

01:10:44.360 --> 01:10:47.300
way. Basically. So you have to do a hyper parameter tuning.

01:10:47.440 --> 01:10:49.800
That is the only way. So you have to go out with a multiple

01:10:49.800 --> 01:10:52.040
iterations with a different permutation and combination.

01:10:52.200 --> 01:10:54.000
That's the only way that's the reason. So we used to talk

01:10:54.000 --> 01:11:00.020
about the hyper parameter tuning over here. Yeah so now as

01:11:00.020 --> 01:11:03.520
you can see that yeah so we are able to build a like a

01:11:03.520 --> 01:11:07.140
somewhat close to a perfect model right so which is able to

01:11:07.140 --> 01:11:10.620
do a better prediction and even if you'll check the loss so

01:11:10.620 --> 01:11:13.960
loss has decreased a lot right the entire loss has decreased

01:11:13.960 --> 01:11:17.220
a lot over here. So previously I was getting eight I have

01:11:17.220 --> 01:11:19.800
done not much basically over here. So what I have done is

01:11:19.800 --> 01:11:22.160
like I have changed the number of neurons number of neurons

01:11:22.160 --> 01:11:24.500
into a hidden layers so that it will be having multiple

01:11:24.500 --> 01:11:30.580
weights now yeah so w1 w2 w4 w5 w6 and w20 and similarly so

01:11:30.580 --> 01:11:33.400
on the other side where we have w3 w4 will be having a

01:11:33.400 --> 01:11:36.320
multiple weight. So in total like a 40 weights I'm talking

01:11:36.320 --> 01:11:39.240
about 20 on this side 20 on this side. So instead of

01:11:39.240 --> 01:11:43.900
training for I'm training 40 and you must have heard about

01:11:43.900 --> 01:11:46.280
the elements. Right. So basically this elements has been

01:11:46.280 --> 01:11:49.980
trained on 1 billion parameter this LLM has been trained on

01:11:49.980 --> 01:11:52.740
400 billion parameter now what is the meaning of those

01:11:52.740 --> 01:11:56.140
billions right people say that 400 billion parameter or like

01:11:56.140 --> 01:11:59.400
Lama has released 17 billion 70 billion 7 0 billion

01:11:59.400 --> 01:12:02.020
parameter model what is the meaning of that parameter by the

01:12:02.020 --> 01:12:06.040
way nothing this weights basically the training parameter

01:12:06.040 --> 01:12:08.040
the trainable parameter so now what is the trainable

01:12:08.040 --> 01:12:10.940
parameter by the way over here weights and biases obviously

01:12:10.940 --> 01:12:14.180
right so this is something called as like a trailer

01:12:14.180 --> 01:12:14.560
parameter.

01:12:22.080 --> 01:12:28.440
Fine guys yep everyone and I believe like Pythonic approach

01:12:28.440 --> 01:12:30.720
maybe a little bit tough but this approach is not at all

01:12:30.720 --> 01:12:33.200
tough it's a very flat things basically it's a very flat

01:12:33.200 --> 01:12:36.120
thing right so layer by layer layer by layer the way I'm

01:12:36.120 --> 01:12:38.660
able to visualize it the way in the similar manner I'm able

01:12:38.660 --> 01:12:43.940
to write the code yeah okay. So once we are able to do it

01:12:43.940 --> 01:12:48.600
now let's move ahead with the next step and now let's try to

01:12:48.600 --> 01:12:52.060
create RNN LSTM model. The problem is that we are not able

01:12:52.060 --> 01:12:53.420
to do it. So the problem that we talked about in my studies

01:12:53.420 --> 01:12:57.480
class so I was like talking about a word called as help

01:12:57.480 --> 01:13:00.900
right so it should be able to predict the next word or maybe

01:13:00.900 --> 01:13:04.180
a next sequence it is supposed to predict with the help of a

01:13:04.180 --> 01:13:06.760
network LSTM network so this is something that we have

01:13:06.760 --> 01:13:09.760
discussed in my studies class let's let's try to like write

01:13:09.760 --> 01:13:13.720
a code and let's try to pass our data and let's see so how

01:13:13.720 --> 01:13:16.300
it is going to work whether it is able to learn something or

01:13:16.300 --> 01:13:19.400
not and if it will not be able to learn something I have a

01:13:19.400 --> 01:13:22.440
multiple option to like you know change. I can even change

01:13:22.440 --> 01:13:24.680
the optimizer so instead of using a static gradient descent

01:13:24.680 --> 01:13:28.180
optimizer I can go ahead with the Adam which is one of the

01:13:28.180 --> 01:13:31.160
best one right so there are a lot of things with which I

01:13:31.160 --> 01:13:33.660
will be able to play maybe I will be able to add one more

01:13:33.660 --> 01:13:37.480
layer over here right just like a tensor board playground

01:13:37.480 --> 01:13:40.560
which I have shown it to you yeah just like a tensor board

01:13:40.560 --> 01:13:43.620
playground. So in the similar manner I can write a code I

01:13:43.620 --> 01:13:46.080
can try to expand and maybe increase or decrease the number

01:13:46.080 --> 01:13:48.900
of neurons number of layers whatever I want I can just do it

01:13:48.900 --> 01:13:51.560
with the help of code. And not by writing much line of a

01:13:51.560 --> 01:13:56.540
code by writing a very simple one. Now so moving ahead and

01:13:56.540 --> 01:14:03.260
now let's try to talk about basically help word or like a

01:14:03.260 --> 01:14:07.020
data that we were talking about like a help. So yesterday I

01:14:07.020 --> 01:14:09.920
think I have given you the example let's try to solve it

01:14:09.920 --> 01:14:13.540
with the help of practical examples. So HELP I want my

01:14:13.540 --> 01:14:17.900
system I want my model to learn this data HELP help so that

01:14:17.900 --> 01:14:21.400
if I'm going to give. H maybe it will be able to predict our

01:14:21.400 --> 01:14:25.220
next sequence ELP if I'm going to give HEL maybe it will be

01:14:25.220 --> 01:14:27.700
able to predict the next sequence which is P something like

01:14:27.700 --> 01:14:30.180
that right something like that so it will be able to predict

01:14:30.180 --> 01:14:34.840
the next one it should be able to you know give you the next

01:14:34.840 --> 01:14:39.280
outcome basically so that is that is something which which I

01:14:39.280 --> 01:14:44.200
would like to like train on basically so let's try to do one

01:14:44.200 --> 01:14:48.460
thing so let's try to break down our data. Which is.

01:14:48.460 --> 01:14:51.800
Basically required for my neural network so I'm going to

01:14:51.800 --> 01:14:57.700
break it down into HELP a separate character and then to all

01:14:57.700 --> 01:15:01.200
those character I'm going to define maybe a like I'm going

01:15:01.200 --> 01:15:05.720
to define basically indexes right I can do it manually I can

01:15:05.720 --> 01:15:08.680
do it by calling an enumeration function the both is fine.

01:15:08.800 --> 01:15:12.860
Let me do it like a manually over here char so let me create

01:15:12.860 --> 01:15:19.820
a variable char to IDX variable. So character to like ID I'm

01:15:19.820 --> 01:15:24.440
just trying to define so here I can create a dictionary and

01:15:24.440 --> 01:15:28.820
as a key let's suppose if there is a H I can give 0 to it

01:15:28.820 --> 01:15:34.580
then if there is a E so I can give basically 1 to it and

01:15:34.580 --> 01:15:40.700
then if I have a L as a string so I can try to give

01:15:40.700 --> 01:15:47.400
basically 2 to it and if I have a P I can try to. Basically

01:15:47.400 --> 01:15:51.720
3 to it again so I have done these things manually but yeah

01:15:51.720 --> 01:15:54.600
I can try to call enumeration function with the help of that

01:15:54.600 --> 01:15:57.660
I will be able to do it even in an automatic way so nothing

01:15:57.660 --> 01:16:00.940
not a big deal I would say now I just have to reverse it

01:16:00.940 --> 01:16:05.120
reverse it out right so my index should become a key and my

01:16:05.120 --> 01:16:08.660
key should become my indexes so maybe I can write a code for

01:16:08.660 --> 01:16:11.880
that one maybe I can write a dictionary comprehension so for

01:16:11.880 --> 01:16:18.720
key and value pair in char2dx.com. item let's try to pull

01:16:18.720 --> 01:16:27.520
the items items and then redefine the value and key I'm just

01:16:27.520 --> 01:16:30.540
reversing it I'm just trying to reverse it and I'm going to

01:16:30.540 --> 01:16:42.320
name that variable as a IDX to char V is not defined K and V

01:16:42.320 --> 01:16:48.220
right not Y yeah so basically if you will check this

01:16:48.220 --> 01:16:51.460
variables. So I have created manually and then I have even

01:16:51.460 --> 01:16:54.720
like a created character to IDX and IDX to character this is

01:16:54.720 --> 01:16:58.220
just a name just reverse of it right I have created because

01:16:58.220 --> 01:17:02.880
I need like these data. Now what I can do is so I have to

01:17:02.880 --> 01:17:05.840
create my input data and I have to create my output data

01:17:05.840 --> 01:17:08.280
even in my last case right I have an input data I had an

01:17:08.280 --> 01:17:12.520
output data so that it will be able to learn yeah it will be

01:17:12.520 --> 01:17:17.100
able to learn so here I can try to create my input a

01:17:17.100 --> 01:17:22.360
sequence. Yes. Let's suppose variable name so input data now

01:17:22.360 --> 01:17:25.240
what is going to my input data so let's suppose I would like

01:17:25.240 --> 01:17:31.100
to give H E L as an input data and E L P as an output data

01:17:31.100 --> 01:17:35.780
so context window size 3 3 so H E L as an input data and E L

01:17:35.780 --> 01:17:38.600
P as output data I can consider anything like depends upon

01:17:38.600 --> 01:17:43.580
me but yeah let's go ahead and consider in this way. So here

01:17:43.580 --> 01:17:47.460
if I have to create my input sequence maybe I can write my

01:17:47.460 --> 01:17:51.240
dictionary comprehension so for C in for character in

01:17:51.240 --> 01:17:57.100
basically data the help data that we have considered so

01:17:57.100 --> 01:18:04.200
start and go till minus 1 means just leave the last one and

01:18:04.200 --> 01:18:14.200
then care to IDX C so now this is going to prepare my input

01:18:14.200 --> 01:18:17.760
sequence what it is trying to do nothing a numeric value as

01:18:17.760 --> 01:18:21.700
you can see. So I can even do it manually so without even

01:18:21.700 --> 01:18:23.380
writing a code but I'm just trying to automate the entire

01:18:23.380 --> 01:18:27.500
process so 0 1 2 now what is the value of 0 0 value is H 1 E

01:18:27.500 --> 01:18:32.420
2 L so H E L technically I'm trying to treat as a input

01:18:32.420 --> 01:18:35.420
sequence in a number because I can't try to like give

01:18:35.420 --> 01:18:39.680
directly H and E and L so I'm just trying to convert it into

01:18:39.680 --> 01:18:42.040
a number and eventually I'll try to convert this entire

01:18:42.040 --> 01:18:46.460
things into a one hot encoding into a higher dimension I'll

01:18:46.460 --> 01:18:49.680
try to convert the data. So that learning will be better now

01:18:49.680 --> 01:18:55.560
input sequence and then target underscore a sequence S E Q

01:18:55.560 --> 01:18:59.960
target underscore sequence and then I can write the exact

01:18:59.960 --> 01:19:06.460
same code for C in data and I have to start from one go to

01:19:06.460 --> 01:19:10.420
last so I'm just skipping the very first one and for those

01:19:10.420 --> 01:19:22.940
data. C H A R care to IDX C. Give me the indexes give me the

01:19:22.940 --> 01:19:25.440
indexes so if you are going to check your target sequences

01:19:25.440 --> 01:19:31.420
it should be 1 2 and 3 right ELP 1 2 and 3 making sense guys

01:19:31.420 --> 01:19:37.540
yeah everyone let me ping you this code so

01:19:40.230 --> 01:19:42.950
I'm just trying to create my input and output data so that

01:19:42.950 --> 01:19:46.530
model will be able to learn right now once I'm able to

01:19:46.530 --> 01:19:50.090
create this data input sequence and my output sequence which

01:19:50.090 --> 01:19:54.110
is my target sequence. So again in form of numbers what we

01:19:54.110 --> 01:19:58.930
can do is so we can try to convert this data into one hot

01:19:58.930 --> 01:20:02.490
basically we can try to convert this entire data set into a

01:20:02.490 --> 01:20:08.630
one hot encoding yes now can we visualize the neural network

01:20:08.630 --> 01:20:11.170
sir yeah you can try to visualize the neural network there

01:20:11.170 --> 01:20:15.390
is a tool called as Netron N E T R O N Netron with the help

01:20:15.390 --> 01:20:18.590
of that you will be able to. So if you will go to Google.

01:20:18.670 --> 01:20:24.170
And. Maybe if you will try to search for. N E T R O N Netron

01:20:24.170 --> 01:20:25.310
N

01:20:27.520 --> 01:20:31.480
E T R O N yeah Netron so basically with the help of Netron

01:20:31.480 --> 01:20:34.920
you will be able to visualize your like model so you can

01:20:34.920 --> 01:20:39.040
just create any kind of a model save it and then try to

01:20:39.040 --> 01:20:42.120
upload that particular model and you will be able to

01:20:42.120 --> 01:20:48.650
visualize it any kind of a model layer by layer yep so

01:21:21.100 --> 01:21:26.100
as you have asked for this one so maybe I can try to. Save

01:21:26.100 --> 01:21:29.020
this model the model that I have created right okay so as

01:21:29.020 --> 01:21:34.600
Deepak have asked let me like do it for him so model dot I

01:21:34.600 --> 01:21:41.540
can try to call save SAV save SAV yeah save and then I can

01:21:41.540 --> 01:21:46.240
try to give my model name so maybe I will give Euron

01:21:46.240 --> 01:21:51.740
underscore Gen AI Gen AI first class anything I can try to

01:21:51.740 --> 01:21:56.980
give dot H5 so in a H5 I can try to. Save this entire model

01:21:56.980 --> 01:22:01.140
so after training again I have called fit right once again

01:22:01.140 --> 01:22:05.640
so you will be able to see that that it has saved a model so

01:22:05.640 --> 01:22:09.560
there is a H5 file which is available to me yeah there is a

01:22:09.560 --> 01:22:15.460
H5 file which is available to me and once this H5 file is

01:22:15.460 --> 01:22:20.020
available to me what I can do is I can go to this Netron

01:22:20.020 --> 01:22:25.680
which is a online tool and open model I can try to search

01:22:25.680 --> 01:22:31.620
for my model so your class directory is let me check what is

01:22:31.620 --> 01:22:34.560
your class directory 30th March right 30th March class

01:22:34.560 --> 01:22:41.980
directory where I am trying to save it so 30th March

01:22:45.110 --> 01:22:51.480
yeah yeah so there is a H5 file which is available call it

01:22:51.480 --> 01:22:58.800
and this is the architecture and layer it is going to

01:22:58.800 --> 01:23:04.040
unwrap. Yeah so there is an input right there is an input

01:23:04.040 --> 01:23:07.420
that I am trying to give there is a dense layer and then

01:23:07.420 --> 01:23:11.240
there is a dense and then there is a dense so here we have

01:23:11.240 --> 01:23:14.100
like a 20 units and all the other parameter you will be able

01:23:14.100 --> 01:23:15.680
to see it here fine

01:23:20.040 --> 01:23:25.460
guys yeah so you can even save a model as a physical file

01:23:25.460 --> 01:23:28.840
and then you can even try to visualize those models fine

01:23:40.960 --> 01:23:46.020
everyone okay so let's let's move ahead someone is saying in

01:23:46.020 --> 01:23:48.780
the chat I have a question. Can you please explain and you

01:23:48.780 --> 01:23:51.900
can you can try to like save this particular model into a

01:23:51.900 --> 01:23:54.320
different format so one is a H5 format in which I was trying

01:23:54.320 --> 01:23:59.560
to save so that is one and then you can even try to save

01:23:59.560 --> 01:24:02.400
this model let me take a model

01:24:05.830 --> 01:24:12.770
.save I can try to call here and maybe I can try to save it

01:24:12.770 --> 01:24:19.290
in a .pb format so prodbob format basically so even in this

01:24:19.290 --> 01:24:23.470
way it will be able to save the model. As you can see in a

01:24:23.470 --> 01:24:26.230
prodbob format so it is going to create a multiple files

01:24:26.230 --> 01:24:29.590
like it is going to create the asset file variable file it

01:24:29.590 --> 01:24:31.610
is going to create a checkpointing file it is going to

01:24:31.610 --> 01:24:35.110
create a lot of file in a H5 format this is like a keras

01:24:35.110 --> 01:24:38.130
model format it is just going to create only one file but

01:24:38.130 --> 01:24:41.610
yeah so in case of a prodbob file which is an intermediate

01:24:41.610 --> 01:24:43.990
file so it is going to create a multiple one and then save

01:24:43.990 --> 01:24:47.130
the model it needs a lot of backup file so I generally

01:24:47.130 --> 01:24:50.990
advise people to like go ahead with H5 file format but yeah

01:24:50.990 --> 01:24:54.270
you will be able to load. Both the one so I can try to load

01:24:54.270 --> 01:25:00.070
even from a prodbob let me load save model prodbob now

01:25:02.220 --> 01:25:05.460
so prodbob model visualization is going to be bit different

01:25:05.460 --> 01:25:07.460
as compared to the so

01:25:10.020 --> 01:25:12.740
it is it is going to show you like a in a bit detail way so

01:25:12.740 --> 01:25:16.700
if you are going to call those models into a netron but bit

01:25:16.700 --> 01:25:21.740
messy and you can call like all the models which is

01:25:21.740 --> 01:25:27.720
available over here so fingerprint keras metadata. So this

01:25:27.720 --> 01:25:30.120
is basically. Basically a keras based metadata it is same as

01:25:30.120 --> 01:25:34.340
like a H5 you will be able to find out and then fingerprint

01:25:34.340 --> 01:25:41.760
.prodbob not able to load fine so yeah you can try to use a

01:25:41.760 --> 01:25:44.760
netron app and by which you will be able to like you know

01:25:44.760 --> 01:25:51.780
see the models in its graphical representation format okay

01:25:53.670 --> 01:25:56.490
Salim Muvad guys everyone okay

01:26:12.910 --> 01:26:18.350
so here we have basically input we have input sequence and

01:26:18.350 --> 01:26:21.670
we have a target over here now what we have to do is so we

01:26:21.670 --> 01:26:25.890
have to basically do a one hot encoding so that I will be

01:26:25.890 --> 01:26:29.330
able to convert my one single data into multiple dimension

01:26:29.330 --> 01:26:34.130
yeah what I can do is I can try to call one hot encoding and

01:26:34.130 --> 01:26:37.570
then I can try to pass my input sequences I can try to say

01:26:37.570 --> 01:26:41.130
that okay this is the size of the encoding that you have to

01:26:41.130 --> 01:26:46.650
create so I can try to call tensorflow one hot right one hot

01:26:46.650 --> 01:26:51.630
directly and input I believe we all remember the one hot

01:26:51.630 --> 01:26:54.410
encoding that we have learnt and even we have done the

01:26:54.410 --> 01:26:59.270
practical at that point of a time so depth I can try to

01:26:59.270 --> 01:27:04.030
define is equals to four over here and then this will

01:27:04.030 --> 01:27:09.570
eventually become my x let's suppose so input sequence okay

01:27:09.570 --> 01:27:15.630
that's fine now so I just have to expand the dimensions so

01:27:15.630 --> 01:27:19.250
one is to three is to four okay let's do it. So I can try to

01:27:19.250 --> 01:27:26.870
convert this x so tf dot expand them just for passing a data

01:27:26.870 --> 01:27:30.050
so I'm just expanding because like it will be available into

01:27:30.050 --> 01:27:32.450
2D I'm just trying to convert into a 3D so that one by one

01:27:32.450 --> 01:27:35.510
one by one I will be able to pass the encoded data basically

01:27:35.510 --> 01:27:41.670
so here x and then I can try to write axis is equals to zero

01:27:41.670 --> 01:27:45.610
so across zero axis just give me a shape of one is to three

01:27:45.610 --> 01:27:52.850
is to four. Compatible shape. Okay. So y is equals to tf dot

01:27:52.850 --> 01:28:01.850
expand dimension and then here target sequence fine and axis

01:28:01.850 --> 01:28:06.750
is equals to zero okay so if I'm going to show you my x so

01:28:06.750 --> 01:28:09.570
this is my how x looks like a three dimension data as you

01:28:09.570 --> 01:28:13.070
can see right so one is to three is to four. So three is to

01:28:13.070 --> 01:28:17.070
four into 3D axis right and then if I'll show you my y. So

01:28:17.070 --> 01:28:22.510
my y is again available as one two and three with this

01:28:22.510 --> 01:28:27.410
particular shape. So here we are able to create my x and we

01:28:27.410 --> 01:28:32.390
are able to create my y. Now input data is done encoding and

01:28:32.390 --> 01:28:36.150
everything is done my data is ready to pass and it's already

01:28:36.150 --> 01:28:40.210
been encoded as you can see now here an output data is also

01:28:40.210 --> 01:28:42.890
ready right so if I'm going to pass like this data so this

01:28:42.890 --> 01:28:45.250
is the output which I'm expecting so the very first one. I'm

01:28:45.250 --> 01:28:47.190
going to pass one e and then l and then p this is what it

01:28:47.190 --> 01:28:51.070
means this is something which I'm expecting. Now what I can

01:28:51.070 --> 01:28:56.770
do is I can try to create the architecture of the model

01:28:56.770 --> 01:29:03.710
right so tensorflow dot keras dot sequential model I can try

01:29:03.710 --> 01:29:09.150
to call over here and inside a sequential model so I can try

01:29:09.150 --> 01:29:12.190
to create a different different kind of a layer the way we

01:29:12.190 --> 01:29:16.130
have created so far. Yeah. So I can call over here and then

01:29:16.130 --> 01:29:20.150
inside that the very first layer which I have to create is

01:29:20.150 --> 01:29:23.710
LSTM layer so I can directly create those LSTM sensor flow

01:29:23.710 --> 01:29:26.210
dot last time we were trying to create a dense layer right

01:29:26.210 --> 01:29:31.910
this time I'm I'll try to call for the LSTM layer LSTM layer

01:29:32.430 --> 01:29:36.230
so yeah LSTM layer it is going to create now I can decide

01:29:36.230 --> 01:29:40.010
that how many LSTM will be there right how many LSTM we are

01:29:40.010 --> 01:29:42.870
supposed to give so let's suppose I'm giving 8 LSTM sequence

01:29:42.870 --> 01:29:45.690
in a sequence 8 LSTM. I'm connected with each and every one

01:29:45.690 --> 01:29:49.090
right so yesterday we have seen only one component likewise

01:29:49.090 --> 01:29:52.950
I'm trying to replicate 8 connected component basically and

01:29:52.950 --> 01:29:56.130
then return sequence means everyone should pass a data to

01:29:56.130 --> 01:29:59.190
the next one right it's going to be true so yeah everyone is

01:29:59.190 --> 01:30:02.990
going to pass a data to the next one right every like a LSTM

01:30:02.990 --> 01:30:07.430
should pass the data to the next one and then input input

01:30:07.430 --> 01:30:13.830
input underscore save so what is the input data which I'm

01:30:13.830 --> 01:30:16.770
going to give so obviously I'm going to give this input data

01:30:16.770 --> 01:30:20.690
so what is the shape 3 cross 4 3 cross 4 is the data set

01:30:20.690 --> 01:30:23.890
which I'm trying to pass so I'll try to write the exact same

01:30:23.890 --> 01:30:28.230
shape over here so 3 cross 4 data I'm going to pass inside

01:30:28.230 --> 01:30:31.070
this one so this is my very first layer which I will end up

01:30:31.070 --> 01:30:35.890
creating now once my data will be inside a LSTM layer then

01:30:35.890 --> 01:30:39.170
there is a next layer which I can try to create maybe just

01:30:39.170 --> 01:30:44.310
with the like a activation function to get the output. So

01:30:44.310 --> 01:30:47.170
tensorflow dot k ras this is the output layer which I'm

01:30:47.170 --> 01:30:54.230
trying to create dot layers dot dense right and here so I

01:30:54.230 --> 01:30:57.570
can say that that there will be like a four neurons

01:30:59.300 --> 01:31:03.080
and then activation wise so I can try to write maybe a

01:31:03.080 --> 01:31:06.040
linear activation so I don't want any kind of a modification

01:31:06.040 --> 01:31:09.700
so I would rely on a softmax function to give me the final

01:31:10.220 --> 01:31:12.600
probability score probability score for like doing a

01:31:12.600 --> 01:31:16.540
prediction. So here this is the network. Which I will end up

01:31:16.540 --> 01:31:20.260
creating very simple right now this is the model this is the

01:31:20.260 --> 01:31:25.980
model that I have created model okay now once I'm able to

01:31:25.980 --> 01:31:30.420
create the model then I have to even give a input by which

01:31:30.420 --> 01:31:34.120
it will be able to run itself so input wise same thing that

01:31:34.120 --> 01:31:35.940
we have given here so

01:31:38.110 --> 01:31:42.890
optimizer learning rate losses so all these things we have

01:31:42.890 --> 01:31:45.850
to define in this particular place as well so that same

01:31:45.850 --> 01:31:47.790
thing same pattern. So model. So model we are able to design

01:31:47.790 --> 01:31:51.810
means layers we are able to design so model.compile I can

01:31:51.810 --> 01:31:55.830
try to call and then I can try to call an optimizer

01:31:55.830 --> 01:31:59.570
OPTIMIJDR. So optimizer wise I can try to call a source

01:31:59.570 --> 01:32:04.110
gradients or maybe I can try to call a best optimizer ADAM

01:32:04.110 --> 01:32:12.200
momentum based optimizer we can try to call. Loss function

01:32:12.200 --> 01:32:16.020
wise so I can try to call maybe a sparse categorical cross

01:32:16.020 --> 01:32:20.100
entropy because it's a. Like a. Classification that we are

01:32:20.100 --> 01:32:24.340
trying to do so maybe we can try to call tensorflow dot

01:32:24.340 --> 01:32:27.560
keras everything is available here so I can I just have to

01:32:27.560 --> 01:32:35.040
call losses dot sparse categorical cross entropy so this is

01:32:35.040 --> 01:32:38.520
the loss which I'm going to call and then I can say that

01:32:38.520 --> 01:32:43.720
that from logits is equals to from logits means try to go

01:32:43.720 --> 01:32:48.780
for the softmax this is the meaning of the logits. Means try

01:32:48.780 --> 01:32:51.640
to find out the probability of occurrence of everything so

01:32:51.640 --> 01:32:56.220
now this model will consider these parameter as a optimizer

01:32:56.220 --> 01:32:59.380
and a loss function and the final prediction function inside

01:32:59.380 --> 01:33:04.300
this one. Now once this is done I can try to call for the

01:33:04.300 --> 01:33:11.380
execution right so my network is done my parameter for the

01:33:11.380 --> 01:33:15.240
model is done. Now let's call for the execution over here so

01:33:15.240 --> 01:33:18.140
that it will start passing a data and then it will start

01:33:18.140 --> 01:33:22.720
training. The entire model so I can write over here so for I

01:33:22.720 --> 01:33:26.920
in a range of or instead of I can write a proper variable

01:33:26.920 --> 01:33:33.920
name. Epoch in a range of let's suppose a 200 so I'm going

01:33:33.920 --> 01:33:36.180
to start like I'm going to run it for 200 times let's

01:33:36.180 --> 01:33:41.880
suppose I can run it even for 1000 times so here model dot

01:33:41.880 --> 01:33:48.300
model dot train on batch so batch by batch train it. And.

01:33:48.420 --> 01:33:52.900
Then take basically my X data and take basically my Y data

01:33:52.900 --> 01:33:58.400
so take basically my X data and take basically my Y data now

01:33:58.400 --> 01:34:02.500
keep on training it and then while you are training try to

01:34:02.500 --> 01:34:10.520
give me some hint as well so if epoch so for every 50 epoch

01:34:10.520 --> 01:34:16.100
try to give me some output which by which I will be able to

01:34:16.100 --> 01:34:18.300
understand that you are able to train it or you are not able

01:34:18.300 --> 01:34:22.840
to train it. So here model. And X.

01:34:25.560 --> 01:34:29.300
Or you D. P. O. T. Output. So I'm just trying to like a

01:34:29.300 --> 01:34:31.980
print what will happen in between whatever will happen in

01:34:31.980 --> 01:34:35.700
between I'm just trying to print it. So here it is going to

01:34:35.700 --> 01:34:39.080
give me the output now what is the exact output so it is

01:34:39.080 --> 01:34:45.040
going to be basically. And so flow. A. R. G. M. X. R. Max.

01:34:45.980 --> 01:34:48.480
It should give me I'll explain to this by don't worry.

01:34:49.980 --> 01:34:56.760
Output. Output. On. A. X. I. S. Is equals to. Minus. One. So

01:34:56.760 --> 01:35:03.200
column of six. And then convert this one into. A. N. U. M.

01:35:03.940 --> 01:35:12.640
P. Y. Num. Pi. And then try to take out the. Zero. One. Now

01:35:12.640 --> 01:35:15.380
this is going to be. I'm just trying to print nothing but a

01:35:15.380 --> 01:35:16.600
probability. Prediction.

01:35:19.750 --> 01:35:20.810
Over. Here.

01:35:23.600 --> 01:35:25.280
No. Prediction. Character. Is.

01:35:27.960 --> 01:35:31.480
A. P. R. E. D. Underscore. Character. The actual character

01:35:31.480 --> 01:35:34.100
which it is going to print. Is nothing but.

01:35:37.090 --> 01:35:43.790
Dot. Join. So string join operation. And. I'll go to my. I.

01:35:43.830 --> 01:35:48.030
D. X. That's the reason. So I have created my. I. D. X. To.

01:35:48.110 --> 01:35:53.850
Care. I. To get. I. For. I. In.

01:35:57.110 --> 01:36:03.930
I. In. Red. Okay. Now. Final. Line. Of. A. Piece. Code.

01:36:05.650 --> 01:36:11.250
Formatted. A. String. Print. The. Epoch. For. Me. So. Print.

01:36:11.570 --> 01:36:20.840
Epoch. Now. Print. A. Prediction. For. Me. P. R. E. D. I. C.

01:36:20.900 --> 01:36:24.540
T. I. O. N. Prediction. For. Me. So. Nothing. But. My.

01:36:24.760 --> 01:36:31.120
Prediction. Character. P. R. E. D. Underscore. P. R. U. D.

01:36:31.300 --> 01:36:35.140
Underscore. C. H. A. R. Prediction. Character. Now. Print.

01:36:35.180 --> 01:36:41.480
A. Loss. For. Me. Print. A. Loss. For. Me. So. Basically. L.

01:36:41.560 --> 01:36:45.960
O. Double. S. Loss. Sequence. Item. 0. Expected. String.

01:36:45.960 --> 01:36:49.360
Instead. List. Found. So. Prediction. Character. Join. I. D.

01:36:49.440 --> 01:36:55.800
X. Okay. So. Expected. String. Instead. List. Found. I. D.

01:36:55.860 --> 01:37:01.120
X. O. It. Should. Be. Inside. The. List. Comprehension.

01:37:01.260 --> 01:37:02.700
Right. P.

01:37:21.980 --> 01:37:25.260
And. Basically. It. Should. Predict. E. L. P. That's. A.

01:37:25.320 --> 01:37:27.080
Target. Which. I. Have. Given. As. You. Can. See. Inside.

01:37:27.120 --> 01:37:30.500
My. X. And. Y. Data. So. X. Y. Is. I'm. Trying. To. Give. H.

01:37:30.540 --> 01:37:35.180
E. L. And. Y. Y. Is. I'm. Expecting. This. One. So. On.

01:37:35.380 --> 01:37:38.620
Epoch. Zero. It. Is. Not. Able. To. Do. It. On. Epoch. 50.

01:37:38.620 --> 01:37:43.180
It. Is. Trying. To. Predict. EEP. On. Epoch. 100. It. Is.

01:37:43.200 --> 01:37:47.840
Trying. To. Predict. EEP. On. Epoch. 150. Right. 150. It.

01:37:47.920 --> 01:37:52.340
Is. Able. To. Print. E. L. P. So. This. Is. The.

01:37:52.980 --> 01:37:56.480
Expectation. That. I. Had. Maybe. I. Can. Give. 2001. Over.

01:37:56.540 --> 01:38:01.140
Here. So. I. Can. Print. Till. 200. Yeah. So. Now. It. Is.

01:38:01.220 --> 01:38:03.940
Able. To. Print. E. L. P. E. L. P. E. L. P. Because. Of.

01:38:03.980 --> 01:38:06.580
The. Previous. Like. Learning. That. It. Is. Trying. To.

01:38:06.600 --> 01:38:09.380
Use. Let. Me. Try. To. Reset. The. Entire. Things.

01:38:13.240 --> 01:38:17.420
Yeah. Let. Me. Bring. You. The. Code. Guys. Inside. The.

01:38:17.520 --> 01:38:20.980
Chat. So. As. You. Can. See. That. That. On. 200. Epoch.

01:38:21.160 --> 01:38:25.680
Yes. It. Is. Able. To. Print. And. My. Loss. Is. Very. Very.

01:38:25.920 --> 01:38:29.140
Very. Very. Less. Right. So. This. Is. Something. That. You.

01:38:29.220 --> 01:38:32.860
All. Will. Be. Able. To. Get. Now. So. Here. We. Are.

01:38:32.920 --> 01:38:35.760
Trying. To. Call. Model. And. We. Trying. To. Pass. X. Now.

01:38:35.840 --> 01:38:39.240
Just. Try. To. Print. What. Is. The. Output. That. You.

01:38:39.280 --> 01:38:43.160
Will. Be. Able. To. Get. So. Output. Is. Nothing. But. It.

01:38:43.200 --> 01:38:46.220
Is. Trying. To. Give. Me. The. Same. Vector. Output. With.

01:38:46.300 --> 01:38:48.320
The. Shape. This. One. Every. Time. So. It. Is. Trying. To.

01:38:48.340 --> 01:38:51.800
Give. Me. Basically. A. Tensors. As. An. Output. It. Trying.

01:38:51.920 --> 01:38:54.160
To. Me. Tensors. Now. What. I'm. Trying. To. Is. So. I'm.

01:38:54.240 --> 01:38:57.640
Just. Trying. To. Take. Out. Basically. A. Prediction. One.

01:38:58.500 --> 01:39:00.900
And. Then. So. Basically. I'm. Trying. To. Take. Out. The.

01:39:01.000 --> 01:39:02.740
Prediction. One. And. Then. I'm. Trying. To. Basically.

01:39:02.980 --> 01:39:05.360
Find. Out. The. Arg. Max. Arg. Max. Means. Basically. I'm.

01:39:05.420 --> 01:39:05.960
Trying. To. Use. The.

01:39:29.840 --> 01:39:32.520
Soft. My. Network. Is. Able. To. Learn.

01:39:45.130 --> 01:39:49.750
Yes. Everyone. Now. I. Try. To. Save. It. And. Then. I. Can.

01:39:49.830 --> 01:39:53.470
Try. To. Show. You. Like. How. This. Network. Actually.

01:39:53.550 --> 01:39:57.790
Looks. Like. So. Model. Dot. I. Can. Try. To. Call. Save.

01:39:57.950 --> 01:40:06.030
And. Then. Maybe. You. Run. Underscore. LSTM. Dot. H5. Save.

01:40:06.110 --> 01:40:11.090
It. And. Then. Maybe. I. Can. Try. To. Like. A. Go. To.

01:40:11.110 --> 01:40:11.350
Google.

01:40:16.140 --> 01:40:16.420
Neutron.

01:40:25.620 --> 01:40:30.800
You. Run. LSTM. And. This. Is. It. Looks. Like. So. Eight.

01:40:30.960 --> 01:40:34.740
Unit. Of. LSTM. Basically. I. Have. Used. Biases. Kernel.

01:40:34.740 --> 01:40:36.840
Size. Everything. It. Going. To. Show. You.

01:40:53.700 --> 01:40:56.960
So. This. Is. A. Demo. Guys. A. Very. Very. Small. Demo. Of.

01:40:57.300 --> 01:41:00.500
Building. A. LSTM. Network. And. Then. Try. I'm. Just.

01:41:00.540 --> 01:41:02.480
Trying. To. Train. Based. On. My. Couple. Of. Characters.

01:41:02.740 --> 01:41:05.620
So. I'm. Just. Trying. To. Teach. My. Model. That. Okay. If.

01:41:05.860 --> 01:41:08.600
Giving. You. These. Three. Words. Then. Give. Me. These.

01:41:08.600 --> 01:41:11.300
Three. Words. In. Return. In. A. Back. And. Yes.

01:41:11.680 --> 01:41:14.860
Successfully. My. Model. Is. Able. To. Understand. This.

01:41:14.960 --> 01:41:17.400
Kind. Of. A. Sequence. Just. Like. Your. WhatsApp. So. When.

01:41:17.460 --> 01:41:19.600
You. Type. Something. When. You. Try. To. Type. One. Or.

01:41:19.660 --> 01:41:21.080
Two. Word. It. Will. Start. Giving. You. A. Next.

01:41:21.280 --> 01:41:23.260
Prediction. So. Similar. Kind. Of. Things. Yes. I'm. Able.

01:41:23.300 --> 01:41:26.000
To. Do. It. Just. With. The. Very. Simple. Example. Which.

01:41:26.040 --> 01:41:32.420
Is. H. E. L. P. Help. Yes. So. Now. Open. For. Question.

01:41:32.500 --> 01:41:37.220
Guys. Please. Go. Ahead. Share. The. Model. Compile. Line.

01:41:37.300 --> 01:41:37.480
So.

01:42:01.780 --> 01:42:09.660
I'm. Going. To. Convert. Into. A. Zip. And. Here. It. Goes.

01:42:14.750 --> 01:42:17.890
Yeah. So. You. Have. A. Complete. File. Now. You. Have. A.

01:42:17.950 --> 01:42:21.690
Zip. File. You. Can. Download. It. And. Each. Every. Line.

01:42:21.690 --> 01:42:27.080
Of. A. Code. So. Fine. Everyone. Successfully. We. Are.

01:42:27.180 --> 01:42:30.060
Able. To. Build. A. LSTM. How. We. Are. Able. To. Build. A.

01:42:30.140 --> 01:42:32.720
Neural. Network. With. The. Help. Of. A. Plain. Simple.

01:42:32.880 --> 01:42:36.180
Python. Code. A. Plus. We. Are. Able. To. Build. A. Neural.

01:42:36.220 --> 01:42:38.620
Network. With. The. Help. Of. Even. A. TensorFlow. Flow.

01:42:38.760 --> 01:42:43.900
Library. So. This. How. Easy. It. Is. By. The. Way.

01:42:44.800 --> 01:42:47.260
Building. A. Model. With. The. Help. Of. TensorFlow. Is.

01:42:47.320 --> 01:42:50.400
Not. At. All. A. Big. Deal. Especially. Keras. Keras. Is. A.

01:42:50.460 --> 01:42:52.560
High. Level. API. Is. So. Building. A. Model. With. The.

01:42:52.620 --> 01:42:55.460
Help. Of. Keras. Is. Not. At. All. A. Big. Deal. Doesn't.

01:42:55.480 --> 01:42:58.540
Matter. How. Complex. The. Model. Is. So. I. Can. Try. To.

01:42:58.580 --> 01:43:01.700
Design. Any. Kind. Of. Architecture. And. Then. Eventually.

01:43:01.780 --> 01:43:03.840
I. Will. Be. Able. To. Just. Have. To. Call. The. Library.

01:43:03.960 --> 01:43:05.960
Layer. By. Layer. By. Layer. Connect. Each. And. Everyone.

01:43:06.820 --> 01:43:09.860
And. Then. My. Entire. Model. Will. Be. Ready.

01:43:14.220 --> 01:43:15.080
Yes. Everyone.

01:43:33.830 --> 01:43:39.250
Okay. So. So. We. Can. Use. A. Data. Set. For. A. Model.

01:43:39.350 --> 01:43:41.650
Building. It. Will. Be. More. Interesting. We. Will. Try.

01:43:41.750 --> 01:43:43.630
To. Do. It. Some. Those. Obviously. We. Are. Going. To. Not.

01:43:43.730 --> 01:43:45.830
Just. Use. A. One. Or. Two. Data. Set. We. Going. To. Use.

01:43:45.890 --> 01:43:45.930
A.

01:44:54.030 --> 01:44:56.190
Massive. So. Yeah. You. Can. Call. The. Model. Evolution.

01:44:56.430 --> 01:44:58.150
That's. That's. Completely. Fine. So. You. Can. Just. Try.

01:44:58.170 --> 01:45:03.370
To. Call. Like. Model. Dot. Function. Is. Already.

01:45:03.610 --> 01:45:06.750
Available. So. Model. Dot. Evaluate. Inbuilt. Function. Is.

01:45:06.830 --> 01:45:09.590
Available. Just. Like. Cycle. Libraries. You. Can. Try. To.

01:45:09.630 --> 01:45:12.690
Pass. Your. X. You. Can. Try. To. Pass. Your. Barbos.

01:45:13.350 --> 01:45:16.490
Basically. Over. Here. Is. Equals. To. One. And. Then. You.

01:45:16.670 --> 01:45:21.510
Will. Be. Able. To. Like. Evaluate. So. Based. On. The. Law.

01:45:21.530 --> 01:45:23.350
So. This. Is. Basically. The. Evaluation.

01:45:36.480 --> 01:45:40.180
You. Can. Even. Try. To. Accuracy. So. How. Accurate. It.

01:45:40.220 --> 01:45:41.440
Is. In. Terms. Of. Predicting. Something.

01:46:01.580 --> 01:46:04.380
Pooran. Is. Saying. Depth. Is. Equals. To.

01:46:19.990 --> 01:46:21.890
You. Know. Not. This. One.

01:46:30.120 --> 01:46:32.120
So. You. Are. Talking. About. This. This. Particle.

01:46:32.380 --> 01:46:32.580
Parameter.

01:46:50.580 --> 01:46:53.080
So. Basically. Depth. Is. Equals. To. Four. Simply. Be. Is.

01:46:53.120 --> 01:46:54.860
That. We. Are. Trying. To. Get. The. Outer. Layer. For.

01:46:54.900 --> 01:46:56.860
Giving. Me. A. Prediction. Right. So. Is. To. Give. Me. A.

01:46:56.960 --> 01:47:00.060
Prediction. For. H. For. E. For. L. And. For. P. That's.

01:47:00.100 --> 01:47:02.220
The. Reason. So. We. Mentioned. Like. A. Four. Over. Here.

01:47:02.460 --> 01:47:05.220
This. Is. It. Is. Trying. To. Represent. Is. The. Outer.

01:47:05.220 --> 01:47:08.700
Dense. Layer. For. Like. Giving. Me. A. Final. Prediction.

01:47:12.100 --> 01:47:14.420
It. Is. Six. Cell. No. It. Is.

01:47:32.170 --> 01:47:34.650
Six. Cell. At. All. It. Is. Just. Going. To. Store. The.

01:47:34.770 --> 01:47:38.530
Weights. That's. It. The. Trained. Weights. So. It. Is. Not.

01:47:38.690 --> 01:47:39.110
Storing. Anything.

01:47:42.920 --> 01:47:44.440
And. The. Model. Architecture. It. Is. Going. To. Hold.

01:47:45.060 --> 01:47:46.780
That's. The. Reasons. I. Shown. You. This. Part. Right. So.

01:47:46.940 --> 01:47:48.780
Over. Here. You. Can. Go. This. Is. What. It. Is. Trying.

01:47:48.840 --> 01:47:52.280
To. Store. Right. So. Basically. It. Is. Trying. To. Take.

01:47:52.300 --> 01:47:55.660
The. Input. And. Then. Number. Of. Dense. Layer. Inside.

01:47:55.760 --> 01:47:58.500
This. One. Is. Eight. As. Can. See. Right. So. This. Is.

01:47:58.520 --> 01:48:02.000
The. Input. Design. To. Take. Now. We. Have. This. LSTM.

01:48:02.080 --> 01:48:04.920
Entire. Network. One. By. One. If. You. See. That.

01:48:05.180 --> 01:48:07.120
Parameter. You. Will. Be. Able. To. Understand. Things. In.

01:48:07.160 --> 01:48:11.520
A. Very. Very. In. Depth. Way. Right. And. Even. You. Will.

01:48:11.580 --> 01:48:14.760
Be. Able. To. Understand. That. What. Is. The. Tensors.

01:48:15.060 --> 01:48:17.520
Value. For. Each. And. Everything. So. This. Is. Something.

01:48:18.060 --> 01:48:19.920
If. You. Are. Going. To. Expand. This. Right. So. Now.

01:48:19.960 --> 01:48:21.620
These. Are. The. Parameter. These. Are. The. Weights. Now.

01:48:21.660 --> 01:48:23.600
This. Is. Something. That. System. Is. Trying. To. Store.

01:48:23.860 --> 01:48:26.780
Actually. You. Will. Be. Able. To. Even. Check. Our. Values.

01:48:27.100 --> 01:48:29.100
So. What. Is. The. Value. Or. What. Is. The. Parameter.

01:48:29.580 --> 01:48:32.060
That. It. Has. Basically. Trained. And. What. It. Is.

01:48:32.080 --> 01:48:34.260
Trying. To. Hold. Inside. It. But.

01:48:42.640 --> 01:48:43.580
Is. The.

01:48:46.680 --> 01:48:48.600
Exact. Value. Which. Model. Is. Storing.

01:48:54.670 --> 01:48:57.850
And. Whenever. You. Are. Going. To. Pass. The. New. Data.

01:48:57.850 --> 01:49:00.590
So. It. Is. Trying. Give. You. The. Result. And. In. A.

01:49:00.690 --> 01:49:03.950
Better. Way. As. We. All. Are. Going. To. Progress. And.

01:49:04.150 --> 01:49:07.590
Even. I. Be. Using. This. Visualization. Plus. The. Numbers.

01:49:07.610 --> 01:49:09.370
That. We. Are. Able. To. Get. On. The. Other. End.

01:49:09.810 --> 01:49:10.050
Everything.

01:49:14.150 --> 01:49:16.670
Okay. So. With. That. Guys. I'm. Done. For. Today's. Class.

01:49:17.050 --> 01:49:20.130
In. Next. Class. So. I'll. Try. To. Extend. A. Same.

01:49:20.390 --> 01:49:25.990
Lecture. And. I'll. Talking. About. A. Basically. An.

01:49:26.190 --> 01:49:29.590
Introduction. Type. Of. RNN. So. I. Have. To. About.

01:49:29.670 --> 01:49:33.690
Basically. A. GRU. So. Gated. Recurrent. Unit. So. LSTM. I.

01:49:33.730 --> 01:49:36.070
Have. Already. Discussed. And. Then. This. Is. Something.

01:49:36.150 --> 01:49:39.430
Which. I. Will. Be. Discussing. This. And. This. And. Then.

01:49:40.530 --> 01:49:44.970
Hopefully. By. Like. A. Sunday. Next. Week. Saturday. And.

01:49:45.050 --> 01:49:47.950
Sunday. So. We. Will. Be. Able. To. Discuss. Our. Attention.

01:49:48.390 --> 01:49:51.150
Mechanism. And. Its. Architecture. So. Technically. I. Will.

01:49:51.230 --> 01:49:53.570
Be. Able. To. Talk. About. The. Research. Paper. By. Sunday.

01:49:54.490 --> 01:49:57.050
So. That's. A. Whole. Idea. That's. A. Idea. For. Our. Next.

01:49:57.090 --> 01:50:00.630
Class. And. Yeah. So. Then. We. Are. Going. To. Start. The.

01:50:00.970 --> 01:50:02.910
Concept. Behind. Generative. AI. So. That. You. Can.

01:50:02.990 --> 01:50:07.130
Understand. Each. And. Everything. So. With. Guys. Thank.

01:50:07.190 --> 01:50:09.550
You. So. Much. I. Believe. I. Already. Shared. All. The.

01:50:09.690 --> 01:50:09.970
Resources.

01:50:13.180 --> 01:50:16.980
So. Resources. Are. Already. Available. Inside. Your. Chat.

01:50:17.080 --> 01:50:21.160
Box. And. Eventually. Into. A. Resource. Section. Just. Try.

01:50:21.180 --> 01:50:24.100
To. Practice. Try. To. Build. Something. Some. Just. Try.

01:50:24.120 --> 01:50:25.900
To. Do. Some. Of. Experimentation. By. Changing. The.

01:50:25.960 --> 01:50:27.540
Number. Of. Layers. By. Changing. All. The. Other.

01:50:28.420 --> 01:50:30.000
Functions. That. We. Are. Trying. To. Use. Inside. The.

01:50:30.100 --> 01:50:32.780
Model. And. Yeah. In. Case. Of. Any. Kind. Of. A. Doubt. Do.

01:50:32.840 --> 01:50:35.700
Let. Me. Know. So. With. That. Guys. Thank. So. Much. Any.

01:50:35.760 --> 01:50:37.860
How. We. All. Are. Connected. What. WhatsApp. So. Yeah.

01:50:37.860 --> 01:50:39.980
Feel. Free. To. Ping. Me. Over. There. Thank. You. So. Much.

01:50:39.980 --> 01:50:42.840
Everyone. Thanks. Take. Care. Everyone.

