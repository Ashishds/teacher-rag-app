WEBVTT

00:00:54.580 --> 00:00:57.560
of the word embedding technique that we are going to discuss

00:00:57.560 --> 00:00:58.700
in

00:01:14.440 --> 00:01:17.300
very very detail very depth we are going to talk about this

00:01:17.300 --> 00:01:18.880
raghu is saying your

00:01:46.850 --> 00:01:49.610
voice is clear when connected to a headphone mic when

00:01:49.610 --> 00:01:52.450
computer audio voice is low so maybe you can try to check

00:01:52.450 --> 00:01:56.310
your uh like rd audio of your computer system

00:02:10.360 --> 00:02:14.920
because from my end uh it's fine so i'm taking classes back

00:02:14.920 --> 00:02:17.080
to back so i don't think that uh it's

00:02:20.660 --> 00:02:25.940
an issue from my end and even i have joined through a mobile

00:02:25.940 --> 00:02:29.460
phone so in wild phone so right everyone uh

00:02:34.380 --> 00:02:40.980
santosh is saying in a group i don't think so uh please

00:02:40.980 --> 00:02:42.320
refresh

00:02:44.420 --> 00:02:48.180
yeah so santosh is saying that there is a delay okay

00:02:51.670 --> 00:02:54.210
so we'll wait for a couple of more minute guys and then we

00:02:54.210 --> 00:02:57.010
are going to start so that every one of us can join yeah so

00:02:57.010 --> 00:03:00.350
just just wait for like a two three minutes more so

00:03:06.660 --> 00:03:09.900
by the way today class so as you can see uh chapter name so

00:03:09.900 --> 00:03:12.600
we are going to talk about a word to vector which is again a

00:03:12.600 --> 00:03:13.080
word to vector which is a word to vector so all this we are

00:03:13.080 --> 00:03:13.140
going to talk about is uh in a way that we will be in the

00:03:13.140 --> 00:03:15.200
section of the video on embedding the technical that we are

00:03:15.200 --> 00:03:19.160
going to discuss in very very detail very depth we are going

00:03:19.160 --> 00:03:19.940
to talk about this

00:03:31.770 --> 00:03:34.630
raghu is saying your voice is clear when connected to a

00:03:34.630 --> 00:03:37.650
headphone mic when computer audio voice is low so maybe you

00:03:37.650 --> 00:03:41.870
can try to check your uh like rd audio of your computer

00:03:41.870 --> 00:03:46.150
system because uh from my end uh it's fine so i'm taking

00:03:46.150 --> 00:03:49.250
classes back to back so i don't think that uh it's an issue

00:03:49.250 --> 00:03:52.930
from my end and even i have joined through a mobile phone so

00:03:52.930 --> 00:03:53.810
in wild phones also it's gap and i will be UAM and something

00:03:53.810 --> 00:03:53.810
like this so i will be on the same video so that's the point

00:03:53.810 --> 00:03:53.810
for today so it's okay to watch this video for the rest of

00:03:53.810 --> 00:03:53.810
the day that's it's also i will be looking for the next

00:03:53.810 --> 00:03:58.010
session so it looks fine to me. Yeah? Tamil Silvam Deepak

00:03:58.010 --> 00:04:01.390
will also convert to vector. Yeah, even yesterday, we were

00:04:01.390 --> 00:04:04.290
trying to convert everything into a vector. So vector means

00:04:04.290 --> 00:04:07.730
nothing but an array. So basically, vector means a value

00:04:07.730 --> 00:04:12.830
with a particular value with a direction. That is something

00:04:12.830 --> 00:04:13.370
called as vector.

00:04:17.870 --> 00:04:21.850
My audio is 100%. Yeah, that happens, actually, because the

00:04:21.850 --> 00:04:25.750
audio output for all the devices is not going to be same,

00:04:25.750 --> 00:04:28.950
especially if you will talk about a laptop, right? And even

00:04:28.950 --> 00:04:31.410
extended monitor. So it's not going to be same. Even I have

00:04:31.410 --> 00:04:36.250
a different kind of a monitor. And basically, the audio

00:04:36.250 --> 00:04:39.010
output is not at all same for everything. So that's the

00:04:39.010 --> 00:04:42.470
reason I'm asking you. Just check. I think that will work

00:04:42.470 --> 00:04:45.950
for you. Otherwise, you can try to install something called

00:04:45.950 --> 00:04:49.570
as audio enhancer. So those kind of softwares are also

00:04:49.570 --> 00:04:51.150
available. So that is going to work for you.

00:04:58.060 --> 00:05:02.620
OK, so let's get started. Let me share my screen. And let's

00:05:02.620 --> 00:05:05.540
start talking about the word to vector.

00:05:09.000 --> 00:05:14.460
OK, my screen's sharing. So I'm sharing my screen, guys. And

00:05:14.460 --> 00:05:19.600
let me open up a VS Code. And please try to open up your VS

00:05:19.600 --> 00:05:22.720
Code. We are going to write a lot of code in my today's

00:05:22.720 --> 00:05:27.460
class. So your batch is basically Gen AI.

00:05:30.610 --> 00:05:38.250
Fine, guys. Today is basically 16th March. M-A-R-C-H. 16th

00:05:38.250 --> 00:05:46.970
March, word2vec.ipynb, dot I-P-Y-N-B. So this is the

00:05:46.970 --> 00:05:49.530
notebook that we are going to use in my today's class. So

00:05:49.530 --> 00:05:53.030
let's get started, guys. Please give a break every 40

00:05:53.030 --> 00:05:57.650
minutes. Don't worry, you will not get bored. So just do a

00:05:57.650 --> 00:06:00.970
coding with me. So fine, guys. Like I said, so in today's

00:06:00.970 --> 00:06:03.190
class, we are going to talk about basically a technique

00:06:03.190 --> 00:06:06.150
called as word2vec. So yesterday, I was talking about a

00:06:06.150 --> 00:06:08.170
couple of technique, like a TF, IDF, I have already

00:06:08.170 --> 00:06:11.130
discussed. Apart from that, so bag of word is something

00:06:11.130 --> 00:06:13.790
which we have already discussed. Apart from that, the worst

00:06:13.790 --> 00:06:16.810
technique was one hot encoding that we have also discussed.

00:06:16.950 --> 00:06:21.890
Now, we were talking about all of this technique so that we

00:06:21.890 --> 00:06:25.430
can try to convert our data, which is eventually a word,

00:06:25.570 --> 00:06:30.670
into some of the numerical representation by vectorizing it

00:06:30.670 --> 00:06:33.770
or by converting it into maybe 0, 1, maybe into some

00:06:33.770 --> 00:06:36.710
numbers. And this is what we have done with the help of all

00:06:36.710 --> 00:06:39.170
these three technique that we have discussed yesterday. Now,

00:06:39.230 --> 00:06:42.770
the extension of this technique is going to be a

00:06:42.770 --> 00:06:45.890
word2vector. The problem with a previous technique was that

00:06:45.890 --> 00:06:49.690
it was not able to understand the ordering of the word. It

00:06:49.690 --> 00:06:53.550
was not able to understand a semantic and syntactic meaning

00:06:53.550 --> 00:06:57.870
of the word. Technically, syntactic is nothing but it's

00:06:57.870 --> 00:07:01.090
ordering with respect to ordering of the word. So all this

00:07:01.090 --> 00:07:06.750
technique was not at all useful. So if we have to. We have

00:07:06.750 --> 00:07:09.830
to understand meaning and a relationship between each and

00:07:09.830 --> 00:07:14.130
every word because they are not holding any kind of a

00:07:14.130 --> 00:07:17.170
relationship or any such kind of information, which will be

00:07:17.170 --> 00:07:20.410
able to establish a relationship between each and every word

00:07:20.410 --> 00:07:24.330
or maybe its grammars or maybe after which word I'm supposed

00:07:24.330 --> 00:07:28.870
to generate or what is our probability that next word is

00:07:28.870 --> 00:07:32.530
going to be x, y, g if the previous word is maybe a, b, c.

00:07:32.940 --> 00:07:37.250
So these kind of understanding, we were not able to get from

00:07:37.250 --> 00:07:40.490
all these three techniques. And this is where a word2vector

00:07:40.490 --> 00:07:44.010
mechanism comes into a picture. So word2vector is an

00:07:44.010 --> 00:07:48.250
algorithm which actually works on a philosophy of a neural

00:07:48.250 --> 00:07:51.830
network. So technically, it's a neural network, which people

00:07:51.830 --> 00:07:57.230
have already created in 2013. So basically, in 2013, 11 year

00:07:57.230 --> 00:08:01.270
or 12 year back from today, so Google has introduced this

00:08:01.270 --> 00:08:04.610
particular technique. So the system will be able to

00:08:04.610 --> 00:08:08.970
understand. Uh. Semantic meaning of the word or ordering of

00:08:08.970 --> 00:08:12.370
the word. And based on that, it will be able to generate

00:08:12.370 --> 00:08:15.590
maybe a surrounding word or maybe it will be able to

00:08:15.590 --> 00:08:18.770
generate a next word. And that technique is technically

00:08:18.770 --> 00:08:24.290
called as word2vector. So it's a neural network, by the way,

00:08:24.350 --> 00:08:28.530
where there is only one hidden layer. And there is a

00:08:28.530 --> 00:08:31.550
probability function which has been used in between so that

00:08:31.550 --> 00:08:34.890
it will be able to generate a next data or maybe a

00:08:34.890 --> 00:08:39.470
surrounding data. Again, if I'll talk about a word2vector.

00:08:39.550 --> 00:08:42.430
So there are two things, guys. There are two things which

00:08:42.430 --> 00:08:45.110
will come in between. And these two things I'm going to

00:08:45.110 --> 00:08:51.330
discuss. So inside this word2vector, basically. So the very

00:08:51.330 --> 00:08:55.410
first thing that we are going to discuss is called a CBOW,

00:08:55.430 --> 00:09:00.470
continuous bag of word. This is the full form of CBOW, CBOW,

00:09:00.570 --> 00:09:04.610
I can say. Continuous bag of word. And the second approach.

00:09:04.830 --> 00:09:08.030
Which is defined inside this word2vector algorithm is

00:09:08.030 --> 00:09:13.090
basically called as a skipgram. a skip, g-r-a-m, skipgram.

00:09:13.350 --> 00:09:16.390
Now what is the difference between this CBOW and a skipgram?

00:09:16.630 --> 00:09:21.430
So CBOW is going to predict our next word. For example, so

00:09:21.430 --> 00:09:25.690
if I'm trying to write a sentence that I am teaching,

00:09:25.990 --> 00:09:31.430
teaching, dash, dash, dash. So maybe it is going to generate

00:09:31.430 --> 00:09:35.050
this particular next word. This is what CBOW is. going to

00:09:35.050 --> 00:09:38.610
perform siva is going to do now if i'll talk about a skip

00:09:38.610 --> 00:09:42.710
gram so let's suppose if i'm going to mention a teaching t-a

00:09:42.710 --> 00:09:46.330
-c-h-i-n-g so it is going to generate a surrounding word

00:09:46.330 --> 00:09:49.690
sudhanshu is teaching or maybe sudhanshu who is a part of

00:09:49.690 --> 00:09:53.430
huron is going to teach something like that skip gram is

00:09:53.430 --> 00:09:56.890
going to generate so these are the words skip gram will be

00:09:56.890 --> 00:09:59.550
able to generate and this is the word which ceiba is going

00:09:59.550 --> 00:10:04.350
to generate so technically both of these terms belongs to a

00:10:04.350 --> 00:10:07.290
word to vector but yeah so both has been used for a

00:10:07.290 --> 00:10:10.530
different different use cases and in detail in depth i'm

00:10:10.530 --> 00:10:12.770
going to show you each and everything don't worry about it

00:10:12.770 --> 00:10:14.850
each and everything i'm just trying to give you the

00:10:14.850 --> 00:10:19.630
introduction or a little bit of context out of it now so

00:10:19.630 --> 00:10:24.610
this one is faster and but yeah for some complex generation

00:10:24.610 --> 00:10:28.450
or some complex task so maybe it is not going to give you a

00:10:28.450 --> 00:10:33.270
better result i would say this is bit slower and again it is

00:10:33.270 --> 00:10:36.330
going to capture things in a much better way as compared to

00:10:36.330 --> 00:10:39.150
c bow we'll try to understand that part so whatever i'm

00:10:39.150 --> 00:10:41.950
trying to say so we'll try to understand it practically if

00:10:41.950 --> 00:10:44.730
i'm trying to say that this is faster or this is slower if

00:10:44.730 --> 00:10:46.690
i'm trying to say that this is a little bit worse this is a

00:10:46.690 --> 00:10:49.610
little bit better we'll try to understand all of these

00:10:49.610 --> 00:10:52.970
contexts in a practical manner so let's try to do this

00:10:52.970 --> 00:10:56.230
implementation in a step-by-step manner so that we'll be

00:10:56.230 --> 00:11:00.070
able to understand this each and everything okay shall we

00:11:00.070 --> 00:11:03.890
start guys everyone say yes please inside a group uh

00:11:08.630 --> 00:11:11.830
deepak is saying when i'm not giving any context also your

00:11:11.830 --> 00:11:16.850
own assist can easily recognize it how it is done sir so

00:11:16.850 --> 00:11:19.910
devans is saying it is a rag system already trained on your

00:11:19.910 --> 00:11:22.290
own data set along with integration to the external sources

00:11:22.290 --> 00:11:25.410
yeah that's true we are trying to make it uh better so maybe

00:11:25.410 --> 00:11:30.390
in next i would say a couple of um a month most probably uh

00:11:30.390 --> 00:11:33.270
you will be able to see a response from your own assist

00:11:33.270 --> 00:11:35.910
which is going to be way better as compared to the response

00:11:35.910 --> 00:11:40.470
that system is trying to give as of now so we are not just

00:11:40.470 --> 00:11:44.910
teaching ai we are also using ai in our system heavily and

00:11:44.910 --> 00:11:47.730
in next couple of week the kind of a system that we are

00:11:47.730 --> 00:11:52.810
going to launch for a mock interview you will be able to see

00:11:52.810 --> 00:11:55.930
that in a real time your system will be able to see your

00:11:55.930 --> 00:11:58.430
screen in real time your system will be able to understand

00:11:58.430 --> 00:12:01.050
everything everything means literally everything whether

00:12:01.050 --> 00:12:04.110
it's a code or whether it's a resumé that you're going to

00:12:04.110 --> 00:12:06.770
share or whether it's a document you are going to share and

00:12:06.770 --> 00:12:10.850
it is going to ask a question or you will be able to break a

00:12:10.850 --> 00:12:14.150
conversation with my system your own system uh just like a

00:12:14.150 --> 00:12:17.030
human being and we are going to release it uh for one

00:12:17.030 --> 00:12:20.330
purpose so that all of you will be able to do uh interview

00:12:20.330 --> 00:12:22.810
preparation apart from that so we are releasing basically

00:12:23.470 --> 00:12:27.730
upgraded version of a resume analyzer so where obviously it

00:12:27.730 --> 00:12:29.110
is going to give you a lot of information using it so final

00:12:29.110 --> 00:12:29.130
question it will give you resume you are going to write

00:12:29.130 --> 00:12:31.950
the準備 even now it is giving you a resume but it will make

00:12:31.950 --> 00:12:36.530
sure that your resume score is more than 90 plus that's the

00:12:36.530 --> 00:12:39.650
first part it is going to generate a cover later it is going

00:12:39.650 --> 00:12:42.870
to give you a interview question and that too round wise

00:12:42.870 --> 00:12:45.850
round one round two round three round four round five it is

00:12:45.850 --> 00:12:49.590
going to even give you a possible dsa question if you are

00:12:49.590 --> 00:12:52.590
coming from a tech background let's suppose if there is a ca

00:12:52.590 --> 00:12:55.130
or there is a lawyer who is trying to use the same system it

00:12:55.130 --> 00:12:57.970
will not give a dsa question obviously so uh so many many

00:12:57.970 --> 00:13:01.850
things many things and everywhere so we are using ai heavily

00:13:01.850 --> 00:13:07.670
so it's not like i'm just teaching ai i'm also using ai to

00:13:07.670 --> 00:13:11.370
build my system so that we can serve our student in a best

00:13:11.370 --> 00:13:16.810
possible way okay so let's get started so first of all guys

00:13:16.810 --> 00:13:21.850
you have to install a couple of library yeah hey you don't

00:13:21.850 --> 00:13:24.530
on which model you are trained okay that's a great question

00:13:24.530 --> 00:13:30.700
you don't will answer okay so first of all guys uh try to

00:13:30.700 --> 00:13:34.220
install couple of things over here so maybe we can try to

00:13:34.220 --> 00:13:38.720
install pip install gen sim so inside a gen sim library you

00:13:38.720 --> 00:13:41.580
will be able to find out this word to weg so just try to

00:13:41.580 --> 00:13:45.020
install a gen sim over here g e n s i m so just do pip

00:13:45.020 --> 00:13:48.160
install gen sim and it will be able to install it in your

00:13:48.160 --> 00:13:50.420
base environment i'm not creating any external environment

00:13:50.420 --> 00:13:53.160
for now inside the best environment itself you can try to

00:13:53.160 --> 00:13:56.320
install uh in my system it's already available but yeah it's

00:13:56.320 --> 00:13:59.400
advisable to all of you that please install this one and i'm

00:13:59.400 --> 00:14:02.380
pinging you inside your chat as well guys so yeah here is a

00:14:02.380 --> 00:14:07.520
ping now so once you are able to install this gen sim we

00:14:07.520 --> 00:14:11.120
have to import a couple of libraries so first of all import

00:14:11.120 --> 00:14:21.540
not ik i m p o r t import n l n l t k so import nltk then

00:14:21.540 --> 00:14:27.540
import g n s i m why i'm making a spelling mistake gen

00:14:30.130 --> 00:14:34.530
sim and then import string

00:14:37.060 --> 00:14:45.780
matplotlib dot pi plot not pilab pi p l o t pi plot as p l t

00:14:45.780 --> 00:14:55.120
alias and then from gen sim gen sim gen sim dot dot models

00:14:55.120 --> 00:15:03.000
so import what word to vec here so this one we have to

00:15:03.000 --> 00:15:12.100
import now from nltk dot tokenize just try to import

00:15:14.760 --> 00:15:20.060
word tokenize and then from nltk

00:15:21.160 --> 00:15:26.420
dot corpus so let's try to import stop words that we have

00:15:26.420 --> 00:15:34.320
already done in my first class stop words then i can try to

00:15:34.320 --> 00:15:40.020
import maybe numpy if i have to use it somewhere so numpy as

00:15:40.380 --> 00:15:47.580
np uh import maybe a regular expression regex basically so

00:15:47.580 --> 00:15:51.380
these are the import guys we all have to do and uh if you

00:15:51.380 --> 00:15:54.280
are getting any kind of error like a model not find kind of

00:15:54.280 --> 00:15:56.660
error then in that case you will have to do the installation

00:15:56.660 --> 00:15:59.440
so i have done the import and i have pinged you the code

00:15:59.440 --> 00:16:02.520
inside your chat as well so please try to structure the code

00:16:02.520 --> 00:16:06.300
and then try to install it yeah everyone uh

00:16:10.550 --> 00:16:13.550
sir based on their resume and experience the latest job

00:16:13.550 --> 00:16:16.310
opening they can directly apply for would be better for the

00:16:16.310 --> 00:16:19.150
job seeker as well yeah so bharti we are releasing that

00:16:19.150 --> 00:16:22.050
system as well so basically what will happen is like uh in

00:16:22.050 --> 00:16:24.950
the system that we are going to release over there so we are

00:16:24.950 --> 00:16:27.750
helping you out in terms of building a resume in terms of

00:16:27.750 --> 00:16:31.130
building a cover letter or like uh preparing for the entire

00:16:31.130 --> 00:16:34.950
interview any kind of interview for everyone plus to

00:16:34.950 --> 00:16:38.130
practice a interview mock interview audio kind of a system

00:16:38.130 --> 00:16:42.490
and apart from that so we are also releasing a system almost

00:16:42.490 --> 00:16:45.930
we have like a build it 90 work is already done so we are

00:16:45.930 --> 00:16:49.290
testing it in our dev environment uh soon it will be into a

00:16:49.290 --> 00:16:51.650
production environment as well and then after that so soon

00:16:51.650 --> 00:16:54.870
it will be available into your mobile device as well so in a

00:16:54.870 --> 00:16:56.970
first release so we are going to release it in a web mode

00:16:56.970 --> 00:17:00.850
and then into a app mode uh so over there you will be able

00:17:00.850 --> 00:17:04.790
to see even a job notification so we are planning to send a

00:17:04.790 --> 00:17:09.510
job notification on a daily basis again with a relevancy not

00:17:09.510 --> 00:17:12.610
a random job notification to everyone so with a relevancy

00:17:12.610 --> 00:17:16.830
and that to a job notification from across a world across a

00:17:16.830 --> 00:17:20.030
world let's suppose someone is living in uk and maybe over

00:17:20.030 --> 00:17:22.090
there knockery is not going to work knockery.com is not

00:17:22.090 --> 00:17:24.050
going to work over there maybe something else is going to

00:17:24.050 --> 00:17:27.490
work over there so it will try to pull a job from that

00:17:27.490 --> 00:17:27.970
reason and it will try to pull a job from that reason it

00:17:27.970 --> 00:17:30.690
will try to do a semantic search similarity it will try to

00:17:30.690 --> 00:17:32.730
check and based on that it will try to give you the

00:17:32.730 --> 00:17:36.650
recommendation so that probability of getting a call will be

00:17:36.650 --> 00:17:39.770
very very high and for each and every job so we are even

00:17:39.770 --> 00:17:43.950
planning to like a tune a resume so it is going to give you

00:17:43.950 --> 00:17:48.830
a multiple version of your resume and heavily we have used

00:17:48.830 --> 00:17:52.970
ai and again nlp because we are dealing with the speech and

00:17:52.970 --> 00:17:56.930
text so a real use case of nlp and ai i would say

00:18:00.010 --> 00:18:01.070
친구 has launched a guys pankt a lightweight rom virus game so

00:18:01.070 --> 00:18:01.310
directly in the 2005 update they did the nl tk download for

00:18:01.310 --> 00:18:01.310
youtube he has written something things like nltk.download

00:18:01.310 --> 00:18:05.450
so along with me my team is like doing it building it again

00:18:05.450 --> 00:18:07.370
so just this import guys x so just this import guys after

00:18:07.370 --> 00:18:09.530
this import after this import what you have to do is so just

00:18:09.530 --> 00:18:11.370
try to write like x so just this import guys after this

00:18:11.370 --> 00:18:16.270
import a N l T k download D o w n allure d download and

00:18:16.270 --> 00:18:22.170
download maybe n k T so i think we have already downloaded

00:18:22.170 --> 00:18:24.850
this we can even write all and then it will download

00:18:24.850 --> 00:18:29.190
everything think NLTK dot again download and download

00:18:29.190 --> 00:18:35.690
basically the stop word stop words so this is going to

00:18:35.690 --> 00:18:37.710
download basically all the stop words which is available

00:18:37.710 --> 00:18:41.750
inside NLTK and again this is going to download some corpus

00:18:41.750 --> 00:18:45.830
so which is already available inside NLTK it's not hard and

00:18:45.830 --> 00:18:48.970
fast to download this one it's completely fine you can try

00:18:48.970 --> 00:18:52.390
to create a list of your own stop word you can use it it's

00:18:52.390 --> 00:18:54.770
well and good for me like I don't have any kind of issue

00:18:54.770 --> 00:18:57.630
because at the end of the day even this is going to give you

00:18:57.630 --> 00:19:00.470
a list of the word that's it or list of the sentences

00:19:00.470 --> 00:19:03.050
nothing more than that so instead of using this one maybe

00:19:03.050 --> 00:19:06.090
you can try to use your own and even I will be using my own

00:19:06.090 --> 00:19:11.150
going forward in my multiple sessions okay now so what we

00:19:11.150 --> 00:19:14.490
have to do is so we have to first of all prepare a data set

00:19:14.490 --> 00:19:18.370
right so we have to prepare a data set in such a way that we

00:19:18.370 --> 00:19:21.650
have to remove all the stop words we have to tokenize the

00:19:21.650 --> 00:19:24.490
data so we have to break it down into a small small chunk.

00:19:24.610 --> 00:19:28.550
We have to convert all the data into its lower case so that

00:19:28.550 --> 00:19:31.830
my data set will be normalized we have to remove all the

00:19:31.830 --> 00:19:36.630
punctuation marks we have to remove all the stop words we

00:19:36.630 --> 00:19:40.190
have to remove all the numbers and a special case character

00:19:40.190 --> 00:19:44.210
from a data set so technically what I'm trying to say is

00:19:44.210 --> 00:19:47.310
that we are trying to do a data pre-processing before

00:19:47.310 --> 00:19:51.290
training our model which is a word to vector model so we

00:19:51.290 --> 00:19:55.570
will try to basically pre-process the data set. So Bharti

00:19:55.570 --> 00:19:58.230
Singh this is really good yeah that's the reason so we are

00:19:58.230 --> 00:20:00.390
going to increase the price because it's costing us a lot

00:20:00.390 --> 00:20:04.270
now we are not able to generate even a single rupees of

00:20:04.270 --> 00:20:08.030
profit out of it but yeah the system is going to be amazing

00:20:08.030 --> 00:20:12.630
for sure okay so here let's try to create a corpus c-o-r-p-u

00:20:12.630 --> 00:20:15.510
-s corpus maybe you can try to give some different name

00:20:15.510 --> 00:20:19.390
that's fine so let's try to create a corpus here guys corpus

00:20:19.390 --> 00:20:23.270
means a data set basically I'm going to create. So. So here

00:20:23.270 --> 00:20:27.410
I'm going to write some of the data so my name is basically

00:20:27.410 --> 00:20:38.350
Sudhanshu Kumar dot and then comma I used to teach teach all

00:20:38.350 --> 00:20:44.730
the data stack along with orbs and clouds maybe you can

00:20:44.730 --> 00:20:48.770
create your own data guys it's fine exclamation marks and

00:20:48.770 --> 00:20:54.950
then some different data some noisy data some words some

00:20:54.950 --> 00:20:58.350
punctuation maybe I can try to mention over here just a

00:20:58.350 --> 00:21:05.790
noisy data I'm trying to include into this one and then NLP

00:21:05.790 --> 00:21:16.970
is very very amazing amazing we are trying to try to learn

00:21:16.970 --> 00:21:30.210
word to vec. So., and we will try to build two models for

00:21:30.210 --> 00:21:39.090
word to vec like a c bow and skip gram so these are the two

00:21:39.090 --> 00:21:44.950
model that we are going to practice we will also work on

00:21:44.950 --> 00:21:55.090
cleaning the data which is. basically a part of part of data

00:21:55.090 --> 00:22:04.410
processing processing okay now so natural

00:22:09.050 --> 00:22:23.890
language processing is a part of AI AI okay fine. My. Phone.

00:22:23.950 --> 00:22:26.690
Now. Number is phone number

00:22:28.480 --> 00:22:41.590
is something like this you'll find word to vec is being used

00:22:41.590 --> 00:22:47.910
for word embeddings.

00:22:49.130 --> 00:23:00.190
It is going to perform better than. One hot. Bag of word.

00:23:01.810 --> 00:23:03.810
Or. TF.

00:23:06.660 --> 00:23:11.120
IDF. Okay. So I'm just I've just created a data guys you can

00:23:11.120 --> 00:23:14.820
try to create your own data that's like a fine for me so if

00:23:14.820 --> 00:23:17.480
you are looking for my data so I'm bringing you this data

00:23:17.480 --> 00:23:18.600
you can try to use it.

00:23:22.520 --> 00:23:26.660
So this is basically my data right so this is basically data

00:23:26.660 --> 00:23:30.640
that. I have created I have already pinned you this data

00:23:30.640 --> 00:23:33.140
guys inside your chat so you can try to use my data or you

00:23:33.140 --> 00:23:35.860
can try to create your own data doesn't matter at all it's

00:23:35.860 --> 00:23:39.500
completely fine. Now so in this data you will be able to

00:23:39.500 --> 00:23:44.160
find out that I have used some unwanted character over here.

00:23:44.280 --> 00:23:47.260
I have a punctuation mark somewhere. Somewhere. Like a.

00:23:47.780 --> 00:23:50.720
Comma. Discrimination marks. All those things somewhere I

00:23:50.720 --> 00:23:53.760
have a number somewhere I have a hyphen so a lot of noisy

00:23:53.760 --> 00:23:58.900
things I have inside my data set and again so somewhere my

00:23:58.900 --> 00:24:01.960
data set is available in an upper case somewhere data set is

00:24:01.960 --> 00:24:06.480
available into a lower cases. So my data set is not a clean

00:24:06.480 --> 00:24:09.800
data set my data set is not an identical data set so what I

00:24:09.800 --> 00:24:14.340
can do is maybe I can try to clean this entire data set and

00:24:14.340 --> 00:24:17.360
once I will be able to clean this data set. So then I can

00:24:17.360 --> 00:24:21.420
try to use it for my training purposes. So then I will be

00:24:21.420 --> 00:24:25.240
able to use it for the training purposes. So OK. Let's try

00:24:25.240 --> 00:24:28.740
to clean a data set in a step by step manner. So in a very

00:24:28.740 --> 00:24:31.720
first place what we can do is so we can try to take this

00:24:31.720 --> 00:24:37.400
data and we can try to convert this entire data into its

00:24:37.400 --> 00:24:40.520
lower case. So I believe we all are aware about it. How to

00:24:40.520 --> 00:24:44.560
convert a data set into a lower cases. Yeah. So how to

00:24:44.560 --> 00:24:46.920
convert this entire data set into a lower case. Any idea

00:24:46.920 --> 00:24:49.180
guys how to convert this data into a lower cases. I think

00:24:49.180 --> 00:24:51.260
it's a basic Python thing. Yeah.

00:24:56.800 --> 00:24:57.660
Yes. Everyone.

00:25:03.860 --> 00:25:06.700
Yeah. Yeah. So let's suppose I have this data. So I'll just

00:25:06.700 --> 00:25:10.100
try to take this data string. I would say I'm going to

00:25:10.100 --> 00:25:12.800
create one function basically but yeah step by step manner.

00:25:12.880 --> 00:25:16.180
So I'll try to show you the purpose of that function in a

00:25:16.180 --> 00:25:18.240
very first place. So let's suppose I'm going to take this

00:25:18.240 --> 00:25:20.920
string. Yeah. So I have taken this string basically over

00:25:20.920 --> 00:25:25.840
here and if I have to convert this entire string into a

00:25:25.840 --> 00:25:30.380
lower case what I can do is a string dot I can try to call a

00:25:30.380 --> 00:25:33.260
lower and then eventually it is going to convert everything

00:25:33.260 --> 00:25:38.740
into a lower case as simple as that. Yep. Fine. Now. So this

00:25:38.740 --> 00:25:42.520
is one of the things that we are going to perform on all of

00:25:42.520 --> 00:25:45.380
these data is the list. So obviously we'll can try to go

00:25:45.380 --> 00:25:47.860
through a for loop. So for loop will try to read line by

00:25:47.860 --> 00:25:49.860
line line by line line by line and then it will try to

00:25:49.860 --> 00:25:53.100
implement this one. So this is a very first operation. So

00:25:53.100 --> 00:25:55.620
I'm just trying to show you. You are breakdown of the

00:25:55.620 --> 00:25:58.780
operation. I'll try to merge all of these operation in one

00:25:58.780 --> 00:26:02.660
single function. I'll apply this entire things with the help

00:26:02.660 --> 00:26:06.320
of that one single function. But yeah, so this is going to

00:26:06.320 --> 00:26:09.560
convert it into a lower case. Now let's suppose in my data

00:26:09.560 --> 00:26:12.740
set there is some number, right? So there is some number

00:26:12.740 --> 00:26:15.000
which is available. Let's suppose if I would like to remove

00:26:15.000 --> 00:26:18.440
these numbers out of this data set. So how I can do that? So

00:26:18.440 --> 00:26:21.020
maybe I can try to use a regular expression. So regular

00:26:21.020 --> 00:26:25.060
expression re I have already done the import. And I can try

00:26:25.060 --> 00:26:28.660
to call regular expression and then subtract basically and I

00:26:28.660 --> 00:26:33.580
can try to write a regular expression for a removal of for

00:26:33.580 --> 00:26:38.360
identification of the number. So here slash D is basically

00:26:38.360 --> 00:26:42.500
slash D plus is going to work for me and it is going to

00:26:42.500 --> 00:26:46.760
remove this num. It is going to replace it with this one. So

00:26:46.760 --> 00:26:48.900
it is going to substitute. Basically, this is the meaning of

00:26:48.900 --> 00:26:51.660
sub, right? So it is going to substitute all the number with

00:26:51.660 --> 00:26:56.140
a blank. As you can see. For what? So for a string for a

00:26:56.140 --> 00:26:59.120
string, I can try to pass as you can see that there was a

00:26:59.120 --> 00:27:02.260
number and what system has done that that system has like a

00:27:02.260 --> 00:27:06.240
substituted blank with respect to the numbers wherever we

00:27:06.240 --> 00:27:09.100
have our numbers. So for this number system has basically

00:27:09.100 --> 00:27:13.720
cleaned it system has removed all the numbers. Now what we

00:27:13.720 --> 00:27:18.060
can do is so we can even try to remove basically all the

00:27:18.060 --> 00:27:21.140
punctuations. We have a punctuation marks as you can see dot

00:27:21.140 --> 00:27:23.920
we have inside my data set. So if I would like to remove a

00:27:23.920 --> 00:27:26.580
punctuation marks, any idea how we can remove a punctuation

00:27:26.580 --> 00:27:32.160
marks? Yeah. Any, any idea guys, how we can remove a

00:27:32.160 --> 00:27:32.940
punctuation by the way.

00:27:40.140 --> 00:27:44.380
So I believe I have done the import of a string, right? I

00:27:44.380 --> 00:27:46.880
have done the import of a string. Now if you are going to

00:27:46.880 --> 00:27:51.260
call that import a string, S-T-R-I-N-G string and dot

00:27:51.260 --> 00:27:54.640
punctuation. So this is going to give you a list of all the

00:27:54.640 --> 00:27:56.560
punctuations, the other punctuation, which is already

00:27:56.560 --> 00:28:00.060
available, right? Inside this string dot punctuation, I have

00:28:00.060 --> 00:28:02.140
already done the import. If you have not done that, just do

00:28:02.140 --> 00:28:05.720
import string. So this is going to list down all the

00:28:05.720 --> 00:28:08.600
punctuation. Now what I have to do is so I just have to call

00:28:08.600 --> 00:28:12.540
my string and then I have to apply this one that wherever

00:28:12.540 --> 00:28:18.180
there is a punctuation, just try to remove all the

00:28:18.180 --> 00:28:22.980
punctuation as simple as that. So to do that, yeah, even I

00:28:22.980 --> 00:28:25.460
can write a regex, that's completely fine. So even with the

00:28:25.460 --> 00:28:28.260
help of regex, I will be able to remove. Or maybe I can try

00:28:28.260 --> 00:28:31.480
to use a inbuilt function. And even with the help of that, I

00:28:31.480 --> 00:28:34.420
will be able to use it. So here my data is available into a

00:28:34.420 --> 00:28:38.180
string S. So what I can do is a string dot translate. There

00:28:38.180 --> 00:28:41.720
is a function called as translate. Now what it does, so

00:28:41.720 --> 00:28:44.580
basically replace each character in a string using a given

00:28:44.580 --> 00:28:48.140
translation table. Simple, right? So whatever translation

00:28:48.140 --> 00:28:50.100
table that we are going to give, we are going to say that

00:28:50.100 --> 00:28:54.380
just use just a string dot punctuation and remove all the

00:28:54.380 --> 00:28:56.420
punctuation. So basically that is a table that we are trying

00:28:56.420 --> 00:29:00.140
to ingest. And based on that, it will try to remove it as

00:29:00.140 --> 00:29:03.000
simple as that. So here a string dot translate is an inbuilt

00:29:03.000 --> 00:29:06.360
function. Like I said, I can even try to use a regex. I can

00:29:06.360 --> 00:29:09.360
even write my custom for loop and then I can write a code

00:29:09.360 --> 00:29:13.180
inside it. So there are multiple ways. There is not just one

00:29:13.180 --> 00:29:16.760
single way for doing it, just a Python code. So I can write

00:29:16.760 --> 00:29:19.560
it in my own way. So I'm just writing it in my own way.

00:29:19.620 --> 00:29:25.040
That's it. So a string dot, I can try to write like a make a

00:29:25.040 --> 00:29:31.080
translation. And then here. So quotes and then quotes and

00:29:31.080 --> 00:29:36.320
not, not this one quote, and then a string dot translation

00:29:36.980 --> 00:29:42.420
tree are in, sorry, in string dot punctuation. I'm going to

00:29:42.420 --> 00:29:45.840
call now what this is going to perform. So this is going to

00:29:45.840 --> 00:29:48.980
remove the punctuation. As you can see that dot is gone,

00:29:49.060 --> 00:29:51.480
right? Dot was the punctuation that we have full stop was

00:29:51.480 --> 00:29:54.480
the punctuation. So now that punctuation has gone. So this

00:29:54.480 --> 00:29:59.100
is what my system is trying to. I do. So fine. I'm able to

00:29:59.100 --> 00:30:01.800
remove the punctuation. I'm able to remove basically a

00:30:01.800 --> 00:30:07.860
numbers. I'm able to convert my data into a lower cases. Now

00:30:07.860 --> 00:30:11.840
what we can do is, so maybe we can try to tokenize a data,

00:30:11.940 --> 00:30:15.900
right? Tokenize a data. So I can simply try to call a word

00:30:15.900 --> 00:30:19.720
tokenizer. So what tokenizer, and I can try to pass my data.

00:30:19.940 --> 00:30:22.980
So in this way, it is going to tokenize my entire data. We

00:30:22.980 --> 00:30:25.020
all understand that. What is the meaning of tokenization?

00:30:25.160 --> 00:30:28.320
And we have already used it. Even in my very first class,

00:30:28.480 --> 00:30:31.540
right? So what tokenizer? If you don't want to follow this

00:30:31.540 --> 00:30:34.320
approach, maybe what you can do is you can try to call a

00:30:34.320 --> 00:30:37.960
split and even a split is going to do the exact same thing.

00:30:38.080 --> 00:30:41.840
So you can try to call a split. It's your choice. So coding

00:30:41.840 --> 00:30:44.780
wise, I can write a same piece of a code in hundreds of

00:30:44.780 --> 00:30:48.200
different way. Depends upon me, right? So there is no

00:30:48.200 --> 00:30:50.500
restriction. Maybe you can go ahead with your own approach.

00:30:50.640 --> 00:30:54.420
Otherwise you can follow my approach now. So here we are

00:30:54.420 --> 00:30:58.380
able to tokenize a data. Fine. Uh, now what we have to do

00:30:58.380 --> 00:31:02.620
is, so we have to basically remove a stop words, right? So

00:31:02.620 --> 00:31:05.300
whatever stop words, which is given inside a NLTK library.

00:31:05.640 --> 00:31:10.280
So I have to remove the stop words, not an issue. So I can

00:31:10.280 --> 00:31:13.800
try to like, uh, uh, remove it. I will be able to do it. So

00:31:13.800 --> 00:31:17.620
here to do that, what I can do is, so maybe I can try to

00:31:17.620 --> 00:31:22.380
call this list. So let's suppose a string S1, and I'm trying

00:31:22.380 --> 00:31:24.820
to say that this is the data which is available inside the

00:31:24.820 --> 00:31:28.260
string S1. I will try to iterate through this data, and then

00:31:28.260 --> 00:31:31.480
I'll try to check with the stop words and I will be able to

00:31:31.480 --> 00:31:37.120
eventually remove all the stop words from the data set. So

00:31:37.120 --> 00:31:42.180
here I can write a loop for S, uh, for I in basically this

00:31:42.180 --> 00:31:47.600
data set list that we have. So for I in this one, so if,

00:31:47.740 --> 00:31:56.100
right, so if I not in basically stop a word. Yeah. So I can

00:31:56.100 --> 00:32:01.440
write a loop for S, S, T, O, P, a stop word dot words, and

00:32:01.440 --> 00:32:05.940
then English E N G L I S S. So English word I'm trying to

00:32:05.940 --> 00:32:17.160
match. So in that case, do what so far I, if I is not in, so

00:32:17.160 --> 00:32:23.740
not I is, so if I not in this stop word. So in that case,

00:32:23.760 --> 00:32:28.740
try to filter. It out and let me write the comprehension for

00:32:28.740 --> 00:32:31.560
this one. Otherwise it will give me the issue.

00:32:34.140 --> 00:32:40.400
So here just try to give me I okay. So now as you can see,

00:32:40.440 --> 00:32:42.980
I'm just trying to run through the loop, nothing much,

00:32:43.060 --> 00:32:46.320
right? So this is the loop for I in S1. So this S1 is

00:32:46.320 --> 00:32:49.360
nothing but a list of the data. And I'm saying that, that if

00:32:49.360 --> 00:32:54.140
I not in a stop words, right, then give me the I as simple

00:32:54.140 --> 00:32:56.920
as that. If I is in a stop words, it will remove it

00:32:56.920 --> 00:33:00.520
automatically. So here you must be able to see that my is

00:33:00.520 --> 00:33:04.800
removed is, is removed because my, and is, is basically part

00:33:04.800 --> 00:33:07.500
of the English stop words that we have already downloaded

00:33:07.500 --> 00:33:12.140
from our NLTK library. Okay. So these are the process guys

00:33:12.140 --> 00:33:15.260
that we are able to do, uh, five processes, I believe. So

00:33:15.260 --> 00:33:18.980
first is a lowercase conversion, then removing all the

00:33:18.980 --> 00:33:22.760
numerical data, then removing all the punctuation marks,

00:33:22.880 --> 00:33:27.120
tokenizing our data, and then removing the stop words. Now

00:33:27.120 --> 00:33:32.240
I'll call this things as a pre-processing of the data. So

00:33:32.240 --> 00:33:34.960
this is going to help me out in terms of pre-processing the

00:33:34.960 --> 00:33:39.180
entire data set. So keeping all this five things in a mind,

00:33:39.280 --> 00:33:42.440
I can try to create one identical function. So which will

00:33:42.440 --> 00:33:45.820
try to take a data and which is going to work for me. So

00:33:45.820 --> 00:33:49.640
which is going to apply all this five on a single data. So I

00:33:49.640 --> 00:33:55.600
can write a def and then word underscore pre processing.

00:33:55.820 --> 00:33:56.280
Okay.

00:33:58.980 --> 00:34:01.660
Processing. This is the function I'm going to write inside

00:34:01.660 --> 00:34:04.300
that I'm going to pass a data, which is a text as a

00:34:04.300 --> 00:34:09.140
parameter, right? And then I can start writing all of these

00:34:09.140 --> 00:34:13.000
codes. So the very first code is convert into a lower. So

00:34:13.000 --> 00:34:15.960
whatever text that you are going to pass, it is going to

00:34:15.960 --> 00:34:22.200
convert into a lower. So this equal to text. So text is a

00:34:22.200 --> 00:34:24.940
data that we are going to pass, let's suppose, so converted

00:34:24.940 --> 00:34:30.440
into a lower first. And then. Again, update this text with

00:34:30.440 --> 00:34:37.840
this regular expression. Yeah. So regular expression. So

00:34:37.840 --> 00:34:42.400
here replace S with text. So it is going to basically remove

00:34:42.400 --> 00:34:46.740
all the numerical values from our data and it is going to

00:34:46.740 --> 00:34:51.460
update the same text. Then what is next? So basically we

00:34:51.460 --> 00:34:55.140
have to remove a punctuation so I can try to use this line

00:34:55.140 --> 00:34:59.620
of a code for removing the punctuation. Okay. Again, let me

00:34:59.620 --> 00:35:07.220
change this S with text and is equal to text. Text. Okay. So

00:35:07.220 --> 00:35:09.800
I'm just going to update a text. So it will try to do this,

00:35:09.880 --> 00:35:12.880
then this, and then this, it is updating the same text

00:35:12.880 --> 00:35:18.660
value. And then we have to tokenize it. So word.tokenized I

00:35:18.660 --> 00:35:23.760
have called. So here text is equals to tokenize and tokenize

00:35:23.760 --> 00:35:27.920
basically at same text data. Okay. So this is the process

00:35:27.920 --> 00:35:31.080
number four. And then once I am able to tokenize it, so

00:35:31.080 --> 00:35:38.800
maybe I can try to, I can try to remove the stop words. So

00:35:38.800 --> 00:35:43.860
again, I'm going to update the text. So for I in, I can

00:35:43.860 --> 00:35:48.800
write text basically. So for I in text, if not in a stop

00:35:48.800 --> 00:35:53.180
words, rest is fine. Okay. So this is basically a pre

00:35:53.180 --> 00:35:55.900
-processing I have written inside this function. And then

00:35:55.900 --> 00:35:59.820
finally. Okay. I'm going to return. So I'm going to return

00:35:59.820 --> 00:36:04.940
what? So I'm going to return basically a text. So word

00:36:04.940 --> 00:36:09.040
tokenize. So this is going to give me the list. So text will

00:36:09.040 --> 00:36:14.620
become a list over here. And then I'm trying to pass a text.

00:36:14.720 --> 00:36:18.760
Let me change our variable name over here. So word, let me

00:36:18.760 --> 00:36:26.100
keep it as word and W R D word. And for word in this one

00:36:26.100 --> 00:36:30.100
return. W R D word. Okay. So it is going to take the data

00:36:30.100 --> 00:36:33.180
and then it is going to return the word. So this is a full

00:36:33.180 --> 00:36:35.960
scale function with all this five operation, which I have

00:36:35.960 --> 00:36:39.840
done individually. I have created making sense guys, let me

00:36:39.840 --> 00:36:41.120
ping you this function inside the chat.

00:36:44.410 --> 00:36:46.910
Does data pre-processing required for creating a AI based

00:36:46.910 --> 00:36:50.510
application without this, you will not be able to do, or you

00:36:50.510 --> 00:36:53.030
will not be able to build any AI application, not just a

00:36:53.030 --> 00:36:55.090
data pre-processing. Data pre-processing is a very small

00:36:55.090 --> 00:36:58.950
part. I would say a complete ETL pipelining is required.

00:36:59.030 --> 00:37:03.290
Okay. So if you will go and check my generative AI interview

00:37:03.290 --> 00:37:08.650
series. So I think on day one or day two, I talked about a

00:37:08.650 --> 00:37:13.150
model fine tuning and where I have all did or creation of

00:37:13.150 --> 00:37:16.330
any kind of a LLM or SLM model, right? So largely my model,

00:37:16.390 --> 00:37:18.410
or maybe a small language model I have discussed

00:37:18.410 --> 00:37:21.450
architecture wise. And over there, you will be able to find

00:37:21.450 --> 00:37:25.430
out that I have discussed about a complete ETL pipelining,

00:37:25.550 --> 00:37:28.110
not just this small component. This is a very, very small

00:37:28.110 --> 00:37:32.530
component, right? Maybe not even a 0.5% of the entire

00:37:32.530 --> 00:37:36.170
process. So complete ETL pipelining is actually required to

00:37:36.170 --> 00:37:38.530
build such kind of a model. It's not like you will just call

00:37:38.530 --> 00:37:41.630
the model, pass the data and then bingo, no, not going to

00:37:41.630 --> 00:37:47.080
happen in that way. Will any embedding models we can try to

00:37:47.080 --> 00:37:49.900
use for data? Yeah. So we are going to use a Amarnath. We

00:37:49.900 --> 00:37:52.460
are going to use a basically like a pre-trained model for

00:37:52.460 --> 00:37:54.780
the embeddings. As of now, we are trying to create our own

00:37:54.780 --> 00:37:57.400
embeddings because I'm trying to teach you from the scratch.

00:37:57.920 --> 00:38:00.820
But going forward, when we will try to build a fuller scale

00:38:00.820 --> 00:38:03.160
models. Okay. Because especially in a fine tuning, so over

00:38:03.160 --> 00:38:06.820
there we'll be using a pre-trained model for the embedding.

00:38:10.380 --> 00:38:14.400
Yeah. Okay. So this is one of the function guys I have

00:38:14.400 --> 00:38:17.680
created. Now what I can do is I can try to pass this entire

00:38:17.680 --> 00:38:23.460
data, my entire corpus inside this function. So my corpus is

00:38:23.460 --> 00:38:27.380
this, C-O-R-P-U-S. This is my corpus. Yeah. This is my

00:38:27.380 --> 00:38:30.780
dataset, which I have created. I have written a lot of

00:38:30.780 --> 00:38:34.720
things over here about me, about the classes. And about NLP

00:38:34.720 --> 00:38:38.960
as well. So maybe I can try to pass this data inside this

00:38:38.960 --> 00:38:42.940
function with the help of for loop. Yeah. So one by one, one

00:38:42.940 --> 00:38:45.720
by one, I'll keep on passing the data and it will keep on

00:38:45.720 --> 00:38:50.940
returning me a tokenized data. Yes. So here what I can do is

00:38:50.940 --> 00:38:58.640
I can try to write for sentence in corpus. So one by one,

00:38:58.660 --> 00:39:01.720
I'm saying that, that go through each and every sentence one

00:39:01.720 --> 00:39:05.540
by one, one by one, one by one, right, go through it. And

00:39:05.540 --> 00:39:10.100
I'm just going to keep it inside the comprehension without

00:39:10.100 --> 00:39:13.020
even comprehension, you can write it. So no issue at all. So

00:39:13.020 --> 00:39:17.000
whatever sentence it is trying to take out, just try to call

00:39:17.000 --> 00:39:20.240
this function. So word preprocessing function that I have

00:39:20.240 --> 00:39:24.220
created. So word preprocessing function, and then pass those

00:39:24.220 --> 00:39:29.960
sentence over here as simple as that. So line by line, line

00:39:29.960 --> 00:39:32.520
by line, one by one. Okay. So it will try to extract a data

00:39:32.520 --> 00:39:35.680
and then it will keep on passing those data inside this

00:39:35.680 --> 00:39:39.140
particular function. Same thing, right? One by one, line by

00:39:39.140 --> 00:39:41.940
line. We are trying to pass it with the help of for loop and

00:39:41.940 --> 00:39:46.000
then, uh, try to store it inside a variable. So processed,

00:39:46.140 --> 00:39:52.920
processed underscore C O R P U S processed corpus. Okay. So

00:39:52.920 --> 00:39:57.040
if everything goes fine, so this is the processed corpus

00:39:57.040 --> 00:40:01.800
that you will be able to get. As you can see. My name is

00:40:01.800 --> 00:40:05.540
Dhanush Kumar. It has removed my, and is it has even removed

00:40:05.540 --> 00:40:09.440
basically, uh, punctuation marks, which was a dot. You can

00:40:09.440 --> 00:40:11.420
try to compare it with the corpus. This was my original

00:40:11.420 --> 00:40:14.460
corpus. My name is Dhanush Kumar dot. It has removed each

00:40:14.460 --> 00:40:16.800
and everything. It has tokenized it. Now this is my data.

00:40:17.280 --> 00:40:20.440
The second one, this is my data. This is my data. This is my

00:40:20.440 --> 00:40:23.700
data. So it has already cleaned each and everything for me.

00:40:23.820 --> 00:40:27.380
So for whatever I have written the code, right? So obviously

00:40:27.380 --> 00:40:30.080
as per my code, it has cleaned each and everything. And this

00:40:30.080 --> 00:40:34.000
is the final corpus I'm able to receive. And this is the

00:40:34.000 --> 00:40:39.780
corpus which I'm going to use to train my model. Fine guys.

00:40:43.590 --> 00:40:50.390
Yes. Everyone. Let me ping this one to everyone. Yeah.

00:40:53.440 --> 00:40:56.960
So let me move ahead. Are we all prepared, able to prepare

00:40:56.960 --> 00:41:00.400
the data? All of us.

00:41:20.910 --> 00:41:23.710
I have given dot at the end of the sentence. It's not able

00:41:23.710 --> 00:41:26.550
to remove the last dot. No Jaidul. It will be able to remove

00:41:26.550 --> 00:41:30.450
it. So basically check your punctuation. This line of code.

00:41:31.570 --> 00:41:33.690
Yeah. It will be able to remove for sure.

00:41:37.070 --> 00:41:43.570
Okay. So let's, let's move ahead and let's try to use this

00:41:43.570 --> 00:41:46.610
data. Just use my function. So I have already pinged you a

00:41:46.610 --> 00:41:49.110
function. Let me ping you once again, this same function,

00:41:49.210 --> 00:41:53.350
anyhow, this will be available inside your resource section,

00:41:53.470 --> 00:41:55.510
but yeah, I've just pinged you the function. You can use my

00:41:55.510 --> 00:42:02.020
function. Maybe you can cross check. Okay, fine. So here,

00:42:02.140 --> 00:42:06.580
uh, basically I'm able to pre-process the entire data set.

00:42:06.740 --> 00:42:09.720
So this is the data set, which is actually required for me

00:42:09.720 --> 00:42:14.260
to send it inside a model so that my model will be able to

00:42:14.260 --> 00:42:17.320
learn. As of now, I'm not talking about a model architecture

00:42:17.320 --> 00:42:22.020
guys, like I said, what to vector is a neural network. I

00:42:22.020 --> 00:42:24.700
haven't talked about a neural network. I'll start talking

00:42:24.700 --> 00:42:27.520
about a neural network from my next class onwards. So where

00:42:27.520 --> 00:42:29.660
I'll be talking about a mathematical intuition. What is the

00:42:29.660 --> 00:42:32.320
intuition behind it? How forward propagation happens? How

00:42:32.320 --> 00:42:34.860
back propagation happens. What is the meaning of a hidden

00:42:34.860 --> 00:42:38.040
layer? What is the meaning of basically like optimization

00:42:38.040 --> 00:42:41.660
over there? Everything, everything in very, very detail and

00:42:41.660 --> 00:42:45.700
depth with a mathematics, with our derivation, with a chain

00:42:45.700 --> 00:42:48.580
rule, right? With the quarterback deviation, second or

00:42:48.580 --> 00:42:52.240
derivation. So I'll be talking about the entire concept of a

00:42:52.240 --> 00:42:55.340
neural network because as per your syllabus, I have to talk

00:42:55.340 --> 00:43:00.320
about basically RNN. LSTM, GRU or encoder. Right. decoder or

00:43:00.320 --> 00:43:03.940
transformer architecture all those things and everything is

00:43:03.940 --> 00:43:07.440
technically a neural network right and as we will progress

00:43:07.440 --> 00:43:09.880
the complexity of the neural network will keep on increasing

00:43:09.880 --> 00:43:14.060
so to build a base for all of you those who are not able to

00:43:14.060 --> 00:43:16.820
recall the actual concept of the neural network i'll be

00:43:16.820 --> 00:43:19.820
talking about a neural network concept from next class

00:43:19.820 --> 00:43:24.840
onwards so at that point of a time some fine day i'll be

00:43:24.840 --> 00:43:27.260
even talking about this word to vector neural network

00:43:27.260 --> 00:43:30.360
internal architecture it's a very small one there is only

00:43:30.360 --> 00:43:33.440
one hidden layer so it's not a big neural network if i'll

00:43:33.440 --> 00:43:36.200
talk about a word to vector as of now i'm just trying

00:43:36.200 --> 00:43:39.080
directly going to use it from a gen sim library as simple as

00:43:39.080 --> 00:43:45.540
that yeah okay so here what i can do is i have already done

00:43:45.540 --> 00:43:50.740
an import of word to vec this one from a gen sim right gen

00:43:50.740 --> 00:43:53.500
sim is the library so from gen sim i have already done the

00:43:53.500 --> 00:43:57.880
import of word to vec what i will do is so i'll try to use

00:43:57.880 --> 00:44:04.280
this word to vec import and i'll try to pass a data inside

00:44:04.280 --> 00:44:08.040
this one which data obviously this data i'm going to pass

00:44:08.040 --> 00:44:11.600
pre-processed corpus i'm going to pass inside word to vec

00:44:11.600 --> 00:44:15.500
now there are so many uh you will be able to see that there

00:44:15.500 --> 00:44:18.260
are so many parameter which is available inside this one so

00:44:18.260 --> 00:44:20.620
maybe you can even go ahead with the hyper parameter tuning

00:44:20.620 --> 00:44:25.880
and you can try to build a best of best your model but i'm

00:44:25.880 --> 00:44:27.520
going to consider a couple of parameters which are available

00:44:27.520 --> 00:44:28.080
inside this one so let's say i have a parameter over here

00:44:28.080 --> 00:44:32.000
which i'm going to explain you even so here sentence is

00:44:32.000 --> 00:44:36.160
equal to this pre-processed corpus i'm going to give simple

00:44:36.160 --> 00:44:39.280
right sentence is equal to pre-processed corpus now what is

00:44:39.280 --> 00:44:41.860
my pre-processed corpus obviously this one right all these

00:44:41.860 --> 00:44:45.760
lines i have i have that inside list inside a list so all

00:44:45.760 --> 00:44:48.480
these lines i am going to give that's the first part second

00:44:48.480 --> 00:44:53.400
part is i'm going to define that what should be the size of

00:44:53.400 --> 00:44:58.760
the vector means so when i'm trying to convert my data into

00:44:58.760 --> 00:45:02.300
a numerical representation so obviously it is going to

00:45:02.300 --> 00:45:06.920
occupy a dimension right so maybe with the help of 100 maybe

00:45:06.920 --> 00:45:11.060
with the help of 200 maybe with the help of 2000 numbers it

00:45:11.060 --> 00:45:15.040
is going to represent one of my data so this is where this

00:45:15.040 --> 00:45:20.360
vector size vec vec tor vector size parameter comes into a

00:45:20.360 --> 00:45:23.960
picture so you can try to give this vector size is equal to

00:45:23.960 --> 00:45:29.440
50 100 300 depends right but at the end of the day what it

00:45:29.440 --> 00:45:31.940
is going to represent so it is going to represent a

00:45:31.940 --> 00:45:36.520
dimension for a single word that okay so this is the number

00:45:36.520 --> 00:45:39.940
of dimension by which i am trying to represent so more the

00:45:39.940 --> 00:45:43.120
dimension more it will be able to understand the

00:45:43.120 --> 00:45:46.060
relationship because it will be having more flexibility even

00:45:46.060 --> 00:45:48.760
yesterday right so you must have seen that when i was trying

00:45:48.760 --> 00:45:51.400
to teach you one hot encoding when i was trying to teach you

00:45:51.400 --> 00:45:57.680
a tf idf or a bag of word i was representing a data by using

00:45:57.680 --> 00:46:03.340
a number of unique words right so dimension was what number

00:46:03.340 --> 00:46:06.940
of unique word yes guys everyone if you remember the study

00:46:06.940 --> 00:46:11.540
itself i talked about it yeah so what was the dimension by

00:46:11.540 --> 00:46:15.480
the way so dimension was basically a number of unique word

00:46:15.480 --> 00:46:20.060
right now here in case of what to vector i can try to define

00:46:20.060 --> 00:46:23.540
my own dimension it's basically a hyper parameter i I can

00:46:23.540 --> 00:46:26.720
try to use a different, different hyperparameter. I can try

00:46:26.720 --> 00:46:30.420
to train it. So, maybe I can write 50 over here. Maybe I can

00:46:30.420 --> 00:46:33.640
write 100 over here. Maybe I can write 300 over here. Now,

00:46:33.780 --> 00:46:37.060
so this is the dimension which will be used to represent one

00:46:37.060 --> 00:46:41.580
single word. Any word, right? One single word. Okay? So,

00:46:41.620 --> 00:46:46.020
maybe I can try to give 100 over here, vector size. Now, the

00:46:46.020 --> 00:46:49.520
another one is basically a window. Window parameter. Now,

00:46:49.560 --> 00:46:51.840
what is the number of, like, what is the meaning of this

00:46:51.840 --> 00:46:54.060
window parameter? Parameter? So, basically, this window

00:46:54.060 --> 00:46:59.200
parameter always try to define a maximum distance. The

00:46:59.200 --> 00:47:01.460
window. So, maybe, let's suppose if I'm giving window is

00:47:01.460 --> 00:47:04.740
equals to 5 over here, right? So, this window is equal to 5

00:47:04.740 --> 00:47:09.400
is going to define a maximum distance between a target word

00:47:09.400 --> 00:47:12.420
and the context word that you have given. Now, what is the

00:47:12.420 --> 00:47:17.940
meaning of that, right? What is the, what is the meaning of

00:47:17.940 --> 00:47:21.160
this one? So, meaning of this one is, let's suppose I have

00:47:21.160 --> 00:47:25.600
my corpus, right? So, my name is Sudhanshu Kumar. And let's

00:47:25.600 --> 00:47:28.400
suppose after that, I have to print something. My name is

00:47:28.400 --> 00:47:30.600
Sudhanshu Kumar Singh, let's suppose. I have to print Singh

00:47:30.600 --> 00:47:35.420
over here. So, 1, 2, 3, 4, 5. So, this is going to be the

00:47:35.420 --> 00:47:38.840
context. And then, target word is going to be Singh, for

00:47:38.840 --> 00:47:42.420
example. The next word which it is going to print. So, here

00:47:42.420 --> 00:47:44.980
it is going to consider a window size. So, it is, it will

00:47:44.980 --> 00:47:47.360
try to understand the relationship between these five word.

00:47:47.600 --> 00:47:50.320
And then, it is going to print the next one. So, you can try

00:47:50.320 --> 00:47:53.200
to again increase and decrease. Okay? Window size. But yeah,

00:47:53.240 --> 00:47:55.820
this is the meaning of a window size, basically. Right? So,

00:47:55.900 --> 00:47:59.100
if window size is larger, obviously, it will be able to

00:47:59.100 --> 00:48:05.340
capture more semantic relationship. And if window size is

00:48:05.340 --> 00:48:09.260
smaller, right? So, if you are giving a smaller window size,

00:48:09.380 --> 00:48:12.420
so it will be able to capture more syntactic relationship.

00:48:13.100 --> 00:48:16.800
Again, there is a contradiction. Right? If window size is

00:48:16.800 --> 00:48:19.520
larger, it will be able to understand more semantic

00:48:19.520 --> 00:48:22.140
relationship. Semantic means grammatical. Yeah? So,

00:48:22.400 --> 00:48:24.840
grammatical relationship, it will be able to understand.

00:48:24.900 --> 00:48:27.520
Relationship between like which one will come after what. If

00:48:27.520 --> 00:48:29.840
larger window size you are going to give. Because you are

00:48:29.840 --> 00:48:33.680
giving more flexibility to learn, basically, what will come

00:48:33.680 --> 00:48:37.080
after what. A longer window size. Longer sentence. If you

00:48:37.080 --> 00:48:40.040
are giving a smaller window size, in that case, you are

00:48:40.040 --> 00:48:45.440
trying to force your model to learn syntactic. Means order

00:48:45.440 --> 00:48:48.400
of the word. Ordering of the word. So, which should be

00:48:48.400 --> 00:48:52.320
placed before what and after what. Like that. So, So, it's a

00:48:52.320 --> 00:48:54.660
trade-off, by the way. So, that's the reason. So, we have to

00:48:54.660 --> 00:48:57.360
consider this window size in a balanced way. Or maybe we can

00:48:57.360 --> 00:49:00.000
try to even call for the hyperparameter. So, that

00:49:00.000 --> 00:49:03.700
hyperparameter tuning is going to give me the best parameter

00:49:03.700 --> 00:49:06.280
for my data. Because at the end of the day, I am using my

00:49:06.280 --> 00:49:10.540
own data to do all these operations. Right? But, yeah. So,

00:49:10.580 --> 00:49:12.640
this is the advantage and disadvantage you will be able to

00:49:12.640 --> 00:49:15.340
get with respect to a window size. Hope all of you are able

00:49:15.340 --> 00:49:19.060
to understand what is the meaning of this window size. Then,

00:49:19.100 --> 00:49:22.180
there is another parameter that you will be able... To find

00:49:22.180 --> 00:49:28.000
out. Called as minimum count. This parameter. Yeah. This

00:49:28.000 --> 00:49:31.700
parameter. So, what is the meaning of this one? Minimum

00:49:31.700 --> 00:49:36.620
count. So, basically, let's suppose in my whole data. Right?

00:49:36.980 --> 00:49:40.520
Whole data. Let's suppose I don't want to consider those

00:49:40.520 --> 00:49:46.020
words which is occurring like, you know, very less. Right?

00:49:46.100 --> 00:49:48.920
Very less number of times. I would like to consider only

00:49:48.920 --> 00:49:51.640
those data which is occurring maybe more than... A

00:49:51.640 --> 00:49:53.940
particular number which I have given. For example, if I have

00:49:53.940 --> 00:49:57.460
given minimum count is equals to 1. Right? So, it is going

00:49:57.460 --> 00:50:01.180
to consider all the data. So, this is the data, let's

00:50:01.180 --> 00:50:02.980
suppose. Right? And let's suppose Sudhanshu is appearing

00:50:02.980 --> 00:50:06.060
only once. It is going to consider Sudhanshu. But, if I am

00:50:06.060 --> 00:50:09.560
going to give minimum count is equal to 2. Right? If

00:50:09.560 --> 00:50:13.400
Sudhanshu appears only one times in my entire corpus. In my

00:50:13.400 --> 00:50:16.620
entire corpus. Right? It is not going to consider Sudhanshu.

00:50:17.320 --> 00:50:19.880
This is the meaning of minimum count. So, minimum count I am

00:50:19.880 --> 00:50:23.200
trying to give 1. So, that it will be... Able to, like, take

00:50:23.200 --> 00:50:27.820
basically, like, all the words. So, even if it is going to

00:50:27.820 --> 00:50:32.300
appear only for once. It is going to understand it. Now, the

00:50:32.300 --> 00:50:39.040
another parameter is basically skip gram variable. So, here,

00:50:39.080 --> 00:50:41.860
basically what happens is. In the beginning, I told you.

00:50:42.120 --> 00:50:47.560
What two vector gives you two different kind of a model. One

00:50:47.560 --> 00:50:51.540
is a C-Bow. Continuous bag of word. And one is a... Skip

00:50:51.540 --> 00:50:56.660
gram. C-Bow is being used to generate one target word. For

00:50:56.660 --> 00:51:00.740
example. So, my name is. If I have written. So, next word is

00:51:00.740 --> 00:51:03.880
Sudhanshu. It is supposed to generate Sudhanshu. Right? Now,

00:51:03.960 --> 00:51:07.240
if I am giving just Sudhanshu. And it is supposed to

00:51:07.240 --> 00:51:10.500
generate all the surrounding words. Like, my name is. All

00:51:10.500 --> 00:51:12.860
the surrounding. Right? Before and after. Then, in that

00:51:12.860 --> 00:51:16.700
case. Skip gram will be used. Otherwise, target word

00:51:16.700 --> 00:51:19.860
generation. For target word generation. C-Bow will be used.

00:51:20.060 --> 00:51:23.820
So, to control. You can control it with the help of SG

00:51:23.820 --> 00:51:28.380
parameter. So, if SG is equals to 0. This means skip gram.

00:51:28.600 --> 00:51:31.160
The full form of this one is. Skip gram is equal to 0. In

00:51:31.160 --> 00:51:35.940
that case. It is going to create basically a C-Bow. So, word

00:51:35.940 --> 00:51:42.240
2. Vec. Underscore C-Bow. So, C-Bow model. It is going to

00:51:42.240 --> 00:51:44.300
create. This is just my variable name I have given. Right?

00:51:44.640 --> 00:51:49.500
If skip gram is equal to 0. Now, I can try to copy the same

00:51:49.500 --> 00:51:55.940
thing. Right? Over here. And, here. What to vector? Skip

00:51:55.940 --> 00:52:00.600
gram. And, here. Skip gram is equals to 1. So, this is what

00:52:00.600 --> 00:52:04.540
this parameter says. If skip gram is equals to 1. Right? So,

00:52:04.640 --> 00:52:09.260
as you can see. 1 for skip gram. Otherwise, C-Bow. Fine? So,

00:52:09.300 --> 00:52:11.800
C-Bow. And, we all understand that. What is the difference

00:52:11.800 --> 00:52:15.500
between C-Bow and a skip gram. C-Bow works for a target

00:52:15.500 --> 00:52:20.680
word. Given a context word. Skip gram works for. Basically,

00:52:21.100 --> 00:52:25.420
the context given a target word. Or, word in the middle.

00:52:26.980 --> 00:52:30.380
Making sense? Yes? Everyone? I will show you. Even, I will

00:52:30.380 --> 00:52:33.300
show you the result. So, I am not just like saying that.

00:52:33.720 --> 00:52:36.200
This is it. But, yeah. Any question guys?

00:52:45.610 --> 00:52:48.950
Yeah. Any question for me? By the way. On these parameters.

00:52:49.190 --> 00:52:50.910
So, I believe I am able to explain you all the parameters.

00:52:51.250 --> 00:52:53.330
What is the meaning of vector size? What is the meaning of

00:52:53.330 --> 00:52:55.210
window? What is the meaning of minimum count? What is the

00:52:55.210 --> 00:52:58.410
meaning of skip gram 0? Skip gram 1? Skip gram 0 means C

00:52:58.410 --> 00:53:02.130
-Bow. I am creating. Skip gram 1 means. S-G-1 means. So, I

00:53:02.130 --> 00:53:04.430
am trying to create the skip gram model. And, what is the

00:53:04.430 --> 00:53:06.250
difference between these two models? I think we all

00:53:06.250 --> 00:53:10.390
understand. So, now. Just try to execute it. And, it will be

00:53:10.390 --> 00:53:13.690
able to complete the training. So, it has completed the

00:53:13.690 --> 00:53:17.150
training. I am not able to see any kind of error so far. It

00:53:17.150 --> 00:53:19.930
simply means that. That it is able to complete the training.

00:53:20.210 --> 00:53:24.970
Now, once training is done. Once my model is built. Maybe, I

00:53:24.970 --> 00:53:28.530
can try to call this model. So, yeah. Gensim model. What to

00:53:28.530 --> 00:53:32.010
vector. What to vector at this object has been built. And,

00:53:32.150 --> 00:53:35.650
again. So, this object has been built. As simple as that.

00:53:35.910 --> 00:53:39.110
So, these are the two models. I am able to build over here.

00:53:39.310 --> 00:53:41.830
One is a C-Bow. One is a skip gram. I have pinned you inside

00:53:41.830 --> 00:53:44.990
the chat. So, yeah.

00:53:47.690 --> 00:53:51.070
Okay. So, Deepak is saying S-G one more time, sir. Okay,

00:53:51.130 --> 00:53:55.630
fine. Let me explain you in this way. So, I'll take one

00:53:55.630 --> 00:53:59.030
example. So, let's suppose. I have two tasks, right? For

00:53:59.030 --> 00:54:07.620
task number one is that I'm writing. My name is now based on

00:54:07.620 --> 00:54:10.380
the training. Whatever data that I have given at the time of

00:54:10.380 --> 00:54:14.120
training, right? It is supposed to generate this data. It is

00:54:14.120 --> 00:54:17.420
supposed to generate this data. Now, task number two. This

00:54:17.420 --> 00:54:21.180
is my task number task number one. This is my task number

00:54:21.180 --> 00:54:27.660
two. So, I have given basically S-U-D-H-A-N-S-H-U one single

00:54:27.660 --> 00:54:30.560
word. I have given. And it is supposed to generate something

00:54:30.560 --> 00:54:34.540
after something before, right? Now, task number one and task

00:54:34.540 --> 00:54:37.420
number two. As you can see, it's a very different task. So,

00:54:37.480 --> 00:54:40.180
in task number one, you are giving a context word and then

00:54:40.180 --> 00:54:43.260
you are generating that target the next one, right? In task

00:54:43.260 --> 00:54:46.360
number two. So, you are trying to give one single word in

00:54:46.360 --> 00:54:48.280
between and then you are trying to generate the context

00:54:48.280 --> 00:54:52.180
before and after. So, this task can be achieved with the

00:54:52.180 --> 00:54:55.820
help of C-Bow. This is called as C-Bow. If someone is going

00:54:55.820 --> 00:54:58.360
to ask you that what is the meaning of continuous bag of

00:54:58.360 --> 00:55:01.660
word? So, basically continuous bag of word is one kind of a

00:55:01.660 --> 00:55:04.540
model available inside word to vector or one kind of a

00:55:04.540 --> 00:55:07.120
neural network which is available inside word to vector,

00:55:07.280 --> 00:55:11.320
right? Which is going to generate the target simple. Now, if

00:55:11.320 --> 00:55:15.220
I'll talk about a skip gram. So, this approach is called as

00:55:15.220 --> 00:55:19.280
a skip gram. Skip gram. Skip gram is a another type of

00:55:19.280 --> 00:55:22.800
model. S-K-I-P-G-R-A-M. Skip gram. So, skip gram is

00:55:22.800 --> 00:55:25.240
basically another type of model that I will be able to

00:55:25.240 --> 00:55:27.780
create. So, which will help me out to generate a word.

00:55:27.800 --> 00:55:30.660
Before and which will help me out to generate word after.

00:55:30.960 --> 00:55:34.680
Now, this is the two model I have created. This is model

00:55:34.680 --> 00:55:38.040
number one for task task one and this is model number two

00:55:38.040 --> 00:55:41.000
for task two. I have created over here as you can see now

00:55:41.000 --> 00:55:44.020
how to create it with the help of only this parameter. Rest

00:55:44.020 --> 00:55:46.240
of the parameter is going to be same. So, skip gram is a

00:55:46.240 --> 00:55:49.200
good look zero in that case. It will automatically consider

00:55:49.200 --> 00:55:53.280
that. Okay, fine. I have to create a C-Bow kind of a model

00:55:53.280 --> 00:55:56.440
which will perform this task. And if I'm going to write a

00:55:56.440 --> 00:55:58.600
skip gram is equal to one. It will be automatically

00:55:58.600 --> 00:56:01.680
understands that. Okay, I have to build a model for this

00:56:01.680 --> 00:56:03.180
kind of a task making sense.

00:56:08.990 --> 00:56:11.850
I want to know that sir on what basis it is generating and

00:56:11.850 --> 00:56:15.610
how so basically it is trying to generate on a basis of your

00:56:15.610 --> 00:56:20.710
data set. So it is trying to learn basically from these data

00:56:20.710 --> 00:56:24.890
set that you are trying to pass inside the model and now

00:56:24.890 --> 00:56:28.470
when I'm saying model. So model wise I'm talking about

00:56:28.470 --> 00:56:33.410
actually a neural network architecture over here. As simple

00:56:33.410 --> 00:56:37.370
as that and I like I told you that when I will start giving

00:56:37.370 --> 00:56:40.010
you explanation or when I start talking about the real

00:56:40.010 --> 00:56:42.610
network. I'll talk about a real network about a word to

00:56:42.610 --> 00:56:45.350
vector as well. It's a very small new network just with one

00:56:45.350 --> 00:56:47.910
single hidden layer. It's not at all a complex one. So at

00:56:47.910 --> 00:56:50.910
that point of a time I'll be talking about it that what is a

00:56:50.910 --> 00:56:53.130
function which has been used inside a hidden layer. What is

00:56:53.130 --> 00:56:56.130
the output function that we generally try to use how it is

00:56:56.130 --> 00:56:58.390
going to learn it in a forward propagation in a backward

00:56:58.390 --> 00:57:02.290
propagation all those things making sense to all of us guys.

00:57:05.020 --> 00:57:08.160
Yeah, now it is going to learn based out of your data set.

00:57:08.740 --> 00:57:11.440
So whatever data set that we have prepared whatever data set

00:57:11.440 --> 00:57:13.460
that we are trying to provide although it's a very small

00:57:13.460 --> 00:57:16.720
data set if data set is large, obviously it will be able to

00:57:16.720 --> 00:57:17.800
understand better.

00:57:20.580 --> 00:57:21.680
Shall we move ahead now?

00:57:26.200 --> 00:57:27.000
Yes, everyone.

00:57:32.630 --> 00:57:37.870
Okay, fine now. So I'm able to train the model right? So two

00:57:37.870 --> 00:57:42.190
model is ready now. Let's suppose I would like to know that

00:57:42.190 --> 00:57:46.950
I had Sudhanshu inside my data. Yeah, I had Sudhanshu inside

00:57:46.950 --> 00:57:50.350
my data. Now, what is a vector representation of Sudhanshu

00:57:50.350 --> 00:57:53.590
right? What is the vector representation of Sudhanshu? This

00:57:53.590 --> 00:57:56.750
is what I would like to know and guys. So whenever I'm going

00:57:56.750 --> 00:57:59.730
to print a vector representation of Sudhanshu, what should

00:57:59.730 --> 00:58:03.050
be the dimension of the vector by the way? Any idea what

00:58:03.050 --> 00:58:06.070
should be the dimension of the vector? I would like to know

00:58:06.070 --> 00:58:08.210
what is the numeric representation of Sudhanshu. That's it

00:58:08.210 --> 00:58:12.630
simple question. I just wanted to know. So my question is

00:58:12.630 --> 00:58:17.250
that what is your expectation? So what should be the size of

00:58:17.250 --> 00:58:21.190
vector that it is going to give us on shoe size saying

00:58:21.190 --> 00:58:23.290
hundred obviously it's going to be hundred is written over

00:58:23.290 --> 00:58:26.950
here, right? So for every entity for Sundari saying 10, I

00:58:26.950 --> 00:58:28.990
think you have made a mistake in times of typing. But yeah,

00:58:29.050 --> 00:58:33.390
Sundari has corrected himself. Yeah, I think everyone is

00:58:33.390 --> 00:58:38.690
saying hundred. It means that my like students are able to

00:58:38.690 --> 00:58:41.110
understand whatever I was trying to explain since last one

00:58:41.110 --> 00:58:47.170
hour. Okay, fine. That's great. Just a check. Okay, so let

00:58:47.170 --> 00:58:51.190
me print it then. Maybe I can try to call any model is Kibra

00:58:51.190 --> 00:58:54.090
model of Seba model. I can try to call. So this is the model

00:58:54.090 --> 00:59:00.350
dot WVU vector word vector and then I'm going to pass over

00:59:00.350 --> 00:59:04.970
here WV word vector and then I'm going to pass over here

00:59:04.970 --> 00:59:10.730
Sudhanshu S U D H A N S H U Sudhanshu this is a

00:59:10.730 --> 00:59:14.470
representation. Maybe you can try to call. Length of it and

00:59:14.470 --> 00:59:17.770
you will be able to see that it's a hundred or even you can

00:59:17.770 --> 00:59:19.690
manually do the calculation. So 5 into 10.

00:59:24.640 --> 00:59:28.520
No, it's it's more than 10.

01:00:23.380 --> 01:00:26.940
Yeah, so it's basically more

01:00:29.240 --> 01:00:32.400
than hundred. So what is the what is the issue guys? Why any

01:00:32.400 --> 01:00:33.660
idea if

01:00:36.440 --> 01:00:38.880
I'm going to call this model and if I'm going to check that

01:00:38.880 --> 01:00:42.220
what is the vector size that you have? So there is a

01:00:42.220 --> 01:00:47.700
function called as a vector size. So technically it's a. 100

01:00:50.100 --> 01:00:55.740
Yep. So what is the issue guys? Maybe I can try to run a

01:00:55.740 --> 01:00:56.680
length on top of it.

01:01:02.410 --> 01:01:05.970
It's hundred but why does it looks like it's more than

01:01:05.970 --> 01:01:10.110
hundred counting wise any idea guys? See I've just checked

01:01:10.110 --> 01:01:13.450
right? So now stanchio itself. So basically it's a hundred

01:01:13.450 --> 01:01:14.250
any idea?

01:01:22.660 --> 01:01:26.560
It's actually a hundred see. I have just run the length

01:01:26.560 --> 01:01:29.300
function on top of it. It's actually hundred.

01:01:42.040 --> 01:01:46.560
Yeah. So basically it's a 20 row, right? 1, 2, 3, 4, 5, 6,

01:01:46.640 --> 01:01:52.520
7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20. Yeah, so 20

01:01:52.520 --> 01:01:57.380
to 500 as simple as that fine. So it looks like because of

01:01:57.380 --> 01:02:00.180
like this printing printing into a console like sometime

01:02:00.180 --> 01:02:04.120
more than hundred. But yeah, it's actually hundred as we

01:02:04.120 --> 01:02:06.860
have confirmed. So this is our representation of the

01:02:06.860 --> 01:02:09.860
stanchio. Now, let's suppose I'm going to write maybe

01:02:09.860 --> 01:02:17.920
stanchio one over here. Now, I have written stanchio one.

01:02:18.560 --> 01:02:23.000
There is no stanchio one. Key stanchio one not present

01:02:23.000 --> 01:02:27.200
inside my data set. So it is going to give you a vector only

01:02:27.200 --> 01:02:30.560
for those data set which was actually available. It is not

01:02:30.560 --> 01:02:33.680
going to give you a vector of those data set which is not

01:02:33.680 --> 01:02:38.720
available inside my data set. As simple as that now, so we

01:02:38.720 --> 01:02:44.360
are able to print this one. Now, let's suppose I have this.

01:02:45.600 --> 01:02:50.720
Model. So I have this model. Now, I would like to check that

01:02:50.720 --> 01:02:55.380
what an all word we have inside my data set which is similar

01:02:55.380 --> 01:02:58.240
to stanchio or which is similar to maybe something else. So

01:02:58.240 --> 01:03:03.220
here what I can do is what to vector word vector dot. I can

01:03:03.220 --> 01:03:06.420
try to call basically similar.

01:03:08.240 --> 01:03:11.900
There is something called as most similar. Yeah, most

01:03:11.900 --> 01:03:15.220
similar. So I can try to call this one. I can try to pass.

01:03:15.220 --> 01:03:20.060
My data. So I am looking for a most similar word, which is

01:03:20.060 --> 01:03:24.140
like close to stanchio. So the word which is close to

01:03:24.140 --> 01:03:29.280
stanchio is basically a stack word. Weck see bow SDF W ops

01:03:29.280 --> 01:03:34.500
build bow cleaning and teach. So these are the word which is

01:03:34.500 --> 01:03:37.960
similar to or which is closer to I would say, right? Maybe I

01:03:37.960 --> 01:03:41.860
can try to even write over here that give me top three

01:03:41.860 --> 01:03:44.820
words. So this is the top three words, which is most

01:03:44.820 --> 01:03:49.220
similar. Similar to stanchio that you will be able to find

01:03:49.220 --> 01:03:53.900
out. Yeah. So again, this is for like a skibra model. I'm

01:03:53.900 --> 01:03:57.680
trying to perform if I would like to perform the same thing

01:03:57.680 --> 01:04:02.320
for maybe like a see bow. I will be able to perform that as

01:04:02.320 --> 01:04:08.300
well. So WV and then give me word vector for MSC. H I see H

01:04:08.300 --> 01:04:13.100
I any machine MSC H I any machine. So there is no machine

01:04:13.100 --> 01:04:17.240
keyword which is available. So let's try for NLP. I have

01:04:17.240 --> 01:04:21.140
taken an LP keyword. So yeah, this is the NLP keyword like a

01:04:21.140 --> 01:04:24.800
vector which it is trying to represent which is trying to

01:04:24.800 --> 01:04:28.720
give from C bar. So whether it's a C bar or whether it's a

01:04:28.720 --> 01:04:31.780
ski ground from both you will be able to get the same thing.

01:04:31.960 --> 01:04:35.260
Now, let's suppose I have to do the comparison maybe with

01:04:35.260 --> 01:04:39.220
the help of a C bar model between two words, right? I have

01:04:39.220 --> 01:04:41.980
to do the comparison. So similarity comparison I have to

01:04:41.980 --> 01:04:46.340
perform so I can try to call. Maybe a model see bow dot. I

01:04:46.340 --> 01:04:49.160
can call word vector.

01:04:51.660 --> 01:04:56.720
Dot similarity. So you can call similarity by vector or just

01:04:56.720 --> 01:05:02.720
you can call a similarity to similarity between NLP and AI

01:05:03.970 --> 01:05:05.230
AI.

01:05:08.600 --> 01:05:11.300
So yeah, this is the similarity that it is going to give it

01:05:11.300 --> 01:05:13.280
to you. So again, you will be able to check what is the

01:05:13.280 --> 01:05:15.500
similarity between these two data. So you will be able to

01:05:15.500 --> 01:05:19.460
print that similarity as well. Now. So if you would like to

01:05:19.460 --> 01:05:23.660
see that the all the words which is a part of the C bar

01:05:23.660 --> 01:05:27.280
model, so you will be able to print that as well. So word to

01:05:27.280 --> 01:05:35.520
Beck C bow and then you can try to write maybe a word vector

01:05:35.520 --> 01:05:43.600
dot index to key not same. So index to key function. So

01:05:43.600 --> 01:05:46.280
these are the word guys, which is already available inside

01:05:46.280 --> 01:05:49.560
my data set simple. So these are the total number of words

01:05:49.560 --> 01:05:52.500
which is available inside my data set. So the other word for

01:05:52.500 --> 01:05:56.540
which my model will be able to give you the vector as simple

01:05:56.540 --> 01:05:59.760
as that. This is what I'm trying to say that for all of

01:05:59.760 --> 01:06:03.480
these words, my model will be able to give you the vector

01:06:03.480 --> 01:06:05.460
fine everyone.

01:06:22.930 --> 01:06:24.930
So any question so

01:07:26.370 --> 01:07:30.250
key is showing me all the character sir. Yes, he will show

01:07:30.250 --> 01:07:34.750
you all the character. Hundred values for each word means we

01:07:34.750 --> 01:07:39.610
are trying to. Convert one single word or we are trying to

01:07:39.610 --> 01:07:43.850
represent one single word by that hundred numbers vector.

01:07:45.430 --> 01:07:51.610
This is the meaning of it. Okay now, so you must be having a

01:07:51.610 --> 01:07:55.650
curiosity that where we are going to use this entire things.

01:07:55.830 --> 01:08:00.290
A very simple thing is let's suppose I'm trying to train a

01:08:00.290 --> 01:08:06.090
model right now. Obviously I will try to pass a data. Yeah,

01:08:06.150 --> 01:08:08.990
I'll try to pass a data. Maybe I'll try to pass this kind of

01:08:08.990 --> 01:08:13.070
a data inside my model for a training right for a training

01:08:13.070 --> 01:08:17.510
purposes. For example, I'm trying to train maybe a. GPT

01:08:17.510 --> 01:08:20.170
model itself right or maybe board model or any other model

01:08:20.170 --> 01:08:23.410
in this entire world. So obviously I have to convert my data

01:08:23.410 --> 01:08:29.050
into a vector space means I have to convert my data into

01:08:29.050 --> 01:08:32.850
this particular format. So whatever data whatever token

01:08:32.850 --> 01:08:37.790
whatever word that we are going to. Pass now, so I can try

01:08:37.790 --> 01:08:41.730
to before sending that data inside a neural network. I have

01:08:41.730 --> 01:08:46.250
to represent a data in this format right this format each

01:08:46.250 --> 01:08:50.010
and every word. This is where you are going to use basically

01:08:50.010 --> 01:08:55.770
a word to vector as simple as that making sense guys.

01:09:07.240 --> 01:09:11.920
Yes. Okay now, so we have basically created two models,

01:09:12.060 --> 01:09:14.880
right? So basically we have created two models. So one model

01:09:14.880 --> 01:09:19.080
is basically what to. A CBO model and one model is basically

01:09:19.080 --> 01:09:22.260
we have created which was a script RAM model. Now we have

01:09:22.260 --> 01:09:25.580
used a same data. Yep. We have basically used a same data.

01:09:25.720 --> 01:09:28.960
Now, let's try to understand that how these two models are

01:09:28.960 --> 01:09:33.000
actually different. So here I'm going to call this model dot

01:09:33.000 --> 01:09:40.420
WV dot. I can try to call basically a most similar and I'm

01:09:40.420 --> 01:09:44.800
calling a Siva model basically right and here so I can try

01:09:44.800 --> 01:09:48.640
to give su. DH. Anshu. Slanchu fine, which was a part of my

01:09:48.640 --> 01:09:52.540
corpus and top and I'm going to give is equals to let's

01:09:52.540 --> 01:09:56.120
suppose five. It is going to generate some of that data. Now

01:09:56.120 --> 01:09:59.260
same thing. I will be doing for a script RAM model. So this

01:09:59.260 --> 01:10:04.480
dot WV dot most similar and here. So I'll be passing a same

01:10:04.480 --> 01:10:08.620
data. So the hanshu su. DH. Anshu. Slanchu and then I'll be

01:10:08.620 --> 01:10:10.480
calling a top top

01:10:13.620 --> 01:10:20.020
n is equals to five, right? So this is a data. That it is

01:10:20.020 --> 01:10:23.780
trying to give it to me from a skip gram model as well as

01:10:23.780 --> 01:10:28.860
from a seba model. So anything that you are able to observe

01:10:28.860 --> 01:10:32.640
guys in between both. Yep.

01:10:40.030 --> 01:10:44.390
So basically in case of a seba, it will always try to give

01:10:44.390 --> 01:10:50.890
you a similar word but based on the context and in case of a

01:10:50.890 --> 01:10:54.690
skip gram, it will again going to give you a similar word.

01:10:54.770 --> 01:10:58.550
It is again going to generate. A similar word but based on

01:10:58.550 --> 01:11:03.310
the prediction. So like I said difference between a seba and

01:11:03.310 --> 01:11:07.930
a skip gram is that seba always try to generate a target

01:11:07.930 --> 01:11:11.270
word. So based on the context that you are trying to give.

01:11:11.410 --> 01:11:14.290
So this is what seba try to generate and if I'll talk about

01:11:14.290 --> 01:11:17.210
the skip gram. So skip gram will always try to generate the

01:11:17.210 --> 01:11:20.890
prediction the surrounding prediction at the end of the day,

01:11:20.950 --> 01:11:24.930
but here so you will be able to observe that both is giving

01:11:24.930 --> 01:11:28.090
me exact similar. Similar kind of a result. The reason is I

01:11:28.090 --> 01:11:33.620
had a very very small data set which I have considered. Yep.

01:11:33.980 --> 01:11:36.840
If I'm going to change the data set if I'm going to take a

01:11:36.840 --> 01:11:41.920
very big data set in that case, maybe it is not going to

01:11:41.920 --> 01:11:46.160
give me the exact same for example. So let me change our

01:11:46.160 --> 01:11:50.840
data set so that I will be able to show you so I have one

01:11:50.840 --> 01:11:55.540
corpus over here. And this corpus is going to show you a

01:11:55.540 --> 01:11:59.220
different result. So let me let me show you that as well

01:11:59.220 --> 01:12:01.820
because with this data set as this data set itself is very

01:12:01.820 --> 01:12:04.340
small. It is not giving me.

01:12:07.440 --> 01:12:11.560
Let me change our data set little bit. Yeah, basically, I'm

01:12:11.560 --> 01:12:14.440
just trying to use the data set which will be able to form a

01:12:14.440 --> 01:12:15.520
lot of relevancy.

01:12:29.870 --> 01:12:35.570
I'll remove Sudhanshu from here. So maybe I will be using

01:12:35.570 --> 01:12:41.450
something called as learning. And then I'll be using a

01:12:41.450 --> 01:12:52.330
learning. L-e-a-r-l-e-a-r-n-i-n-g learning then here

01:12:52.330 --> 01:12:54.350
learning same

01:13:10.610 --> 01:13:11.850
result.

01:13:13.650 --> 01:13:17.130
Let me change a data set once again. Maybe

01:13:24.490 --> 01:13:27.410
I can try to change

01:13:29.040 --> 01:13:35.180
a context. Size itself so that it will not be able to learn.

01:13:35.280 --> 01:13:40.140
I'm just trying to show you guys that Skibram and Sebao is

01:13:40.140 --> 01:13:44.380
different. But yeah, so I just have to play with couple of

01:13:44.380 --> 01:13:48.640
parameters here. That's it. Sudhanshu wherever I have

01:13:48.640 --> 01:13:52.100
written. So I have to remove it. Because Sudhanshu is not

01:13:52.100 --> 01:13:54.900
going to work. So let me change a window size so that it

01:13:54.900 --> 01:13:59.320
will not be able to understand much. So window size. Okay,

01:13:59.380 --> 01:14:02.480
so here here here it goes algorithm

01:14:06.830 --> 01:14:14.490
transforming science NLP and network. Oh, it is giving me.

01:14:17.170 --> 01:14:22.090
Again the exact same result 30900.

01:14:26.110 --> 01:14:30.030
It's not exact same result. I can see a changes in a number.

01:14:30.770 --> 01:14:34.530
I think you must be able to observe but yeah, it's very

01:14:34.530 --> 01:14:40.390
similar because of the size of data set. So maybe what I can

01:14:40.390 --> 01:14:42.070
do is let

01:15:00.220 --> 01:15:07.360
me take some big data set guys very very big one. So I'm

01:15:07.360 --> 01:15:12.480
generating with the help of your own assist a big data set.

01:15:43.500 --> 01:15:46.600
Just wait for a minute. You don't assist is generating a use

01:15:46.600 --> 01:15:47.620
amount of data set for me.

01:15:50.720 --> 01:15:52.720
They are different now sir. Yeah, I know it's different

01:15:52.720 --> 01:15:55.580
number wise. It's actually different even before that it was

01:15:55.580 --> 01:15:58.840
like not exact same if you'll check the numbers, right? So

01:15:58.840 --> 01:16:01.780
numbers wise it is still not same. So if you'll just check

01:16:01.780 --> 01:16:05.380
the number after maybe two or three decimal point, so you

01:16:05.380 --> 01:16:07.720
will be able to find out that it's completely different. So

01:16:07.720 --> 01:16:09.920
here two. One eight eight five seven, right? So two eight

01:16:09.920 --> 01:16:12.680
two one eight eight seven one. So obviously the different

01:16:12.680 --> 01:16:16.280
but what I'm trying to show you like I'm just trying to show

01:16:16.280 --> 01:16:19.020
you very clearly or I would like to show you very clearly

01:16:19.020 --> 01:16:23.420
that even this words are not going to be same, right? So the

01:16:23.420 --> 01:16:25.820
order of arrangement that you are able to see in terms of

01:16:25.820 --> 01:16:29.000
words, even this is this should not be same. That's the

01:16:29.000 --> 01:16:32.640
reason. So I'm going to take a big data set. So let me take

01:16:32.640 --> 01:16:36.480
a large corpus. I have already generated with your own

01:16:36.480 --> 01:16:43.980
assist. So yeah, this is a big data set. My corpus. Let me

01:16:43.980 --> 01:16:45.520
replace now.

01:16:47.750 --> 01:16:48.370
Let's do it.

01:17:06.990 --> 01:17:09.570
Number is different is

01:17:12.710 --> 01:17:15.050
giving me maybe

01:17:21.190 --> 01:17:25.730
I can take some different word. Let me check with not

01:17:25.730 --> 01:17:27.450
learning machine

01:17:29.470 --> 01:17:36.110
and here machine. Seba's keep them making

01:17:38.860 --> 01:17:41.260
spoken again.

01:17:45.870 --> 01:17:48.270
Numbers are same but maybe

01:17:54.480 --> 01:17:56.820
I can use five here.

01:18:14.610 --> 01:18:18.230
Yeah, it's very close to the vector space. That's the

01:18:18.230 --> 01:18:23.590
reason. So it is not able to generate some new data for the

01:18:23.590 --> 01:18:30.210
same. So maybe I can try to use something like a deep. Let's

01:18:30.210 --> 01:18:31.270
see what it does.

01:19:21.360 --> 01:19:26.940
It's very very close vector itself is like a very close to

01:19:26.940 --> 01:19:29.740
each other. So maybe I can give 10 here.

01:19:38.280 --> 01:19:42.600
Wildlife cloud service problems. Wildlife cloud. Okay. Yeah,

01:19:42.640 --> 01:19:46.460
we are able to see the alteration over here. See guys. So

01:19:46.460 --> 01:19:49.640
I've just played with the window size. So when I'm trying to

01:19:49.640 --> 01:19:53.700
generate something from Seba model, so I'm just trying to

01:19:53.700 --> 01:19:57.160
give an input is equal to vision and in both the things. So

01:19:57.160 --> 01:19:59.880
I'm trying to give input is equal to vision and here it is

01:19:59.880 --> 01:20:05.120
giving me wildlife cloud service problem and text, right? So

01:20:05.120 --> 01:20:09.060
basically it is. It is trying to generate. Next word out of

01:20:09.060 --> 01:20:12.280
it always. So it will try to generate always a target word

01:20:12.280 --> 01:20:15.660
as a next word, which is nothing but a context word, which

01:20:15.660 --> 01:20:18.840
is similar or based on the probability. So it will always

01:20:18.840 --> 01:20:20.660
try to tell you that. Okay, fine. So this is the probability

01:20:20.660 --> 01:20:23.980
that after vision wildlife will come then cloud will come

01:20:23.980 --> 01:20:26.100
the service will come then problem will come and then text

01:20:26.100 --> 01:20:29.300
will come. Now if I'll talk about a skip gram, so when I'm

01:20:29.300 --> 01:20:32.660
trying to give a vision over here, so it is trying to

01:20:32.660 --> 01:20:35.660
generate a surrounding word that okay, so vision could be

01:20:35.660 --> 01:20:40.240
surrounded by the best probability is wildlife. Maybe after

01:20:40.240 --> 01:20:44.720
and before cloud vision problem vision service vision and

01:20:44.720 --> 01:20:47.800
again order is not the same. So wildlife cloud service

01:20:47.800 --> 01:20:51.600
problem while a cloud problem service, right? So now it has

01:20:51.600 --> 01:20:53.500
started changing the order. It simply means what I'm trying

01:20:53.500 --> 01:20:56.200
to show you over here is that that both the models are not

01:20:56.200 --> 01:20:59.540
at all. Same a purpose of both the models are completely

01:20:59.540 --> 01:21:02.240
different. See bow is completely different and the script

01:21:02.240 --> 01:21:06.440
gram is going to be completely different. Although. Both is

01:21:06.440 --> 01:21:11.180
helping you out in terms of converting a data into its

01:21:11.180 --> 01:21:16.420
vector representation, but still both the models are not at

01:21:16.420 --> 01:21:21.640
all. Same fine guys making sense. Yeah. So basically see bow

01:21:21.640 --> 01:21:24.520
like I have already mentioned that it will always try to

01:21:24.520 --> 01:21:29.480
predict a word when you are going to give a surrounding

01:21:29.480 --> 01:21:36.020
context. So maybe I can try to look into the data set. A

01:21:36.020 --> 01:21:44.540
little bit. So my corpus I can try to print corpus corpus.

01:21:52.980 --> 01:22:03.000
It should not it is just let me check guys a method by which

01:22:03.000 --> 01:22:04.720
I will be able to do the prediction.

01:23:09.970 --> 01:23:15.690
Okay. So here yeah, so prediction wise. Learning industrial.

01:23:15.810 --> 01:23:18.470
So maybe I can try to take the first one and then I can say

01:23:18.470 --> 01:23:21.730
that okay fine. So this is the most nearest one. Which I

01:23:21.730 --> 01:23:24.230
will be able to generate and maybe I can try to run it into

01:23:24.230 --> 01:23:29.110
a loop so that it will be like able to take learning and

01:23:29.110 --> 01:23:32.290
then industrial and then you can try to generate the next

01:23:32.290 --> 01:23:36.230
two industrial then next to the next one in this way. You

01:23:36.230 --> 01:23:38.470
will keep on generating it. For example, I'm able to

01:23:38.470 --> 01:23:41.310
generate learning. So here I can try to pass industrial you

01:23:41.310 --> 01:23:44.310
will be able to get what is the next word after industrial.

01:23:44.310 --> 01:23:47.990
So let's suppose the most common is wastage so I can pass

01:23:47.990 --> 01:23:51.850
wastage and then I will be able to generate industrial. So

01:23:51.850 --> 01:23:54.650
in this way, I'll keep on generating the next next next one

01:23:54.650 --> 01:23:57.250
obviously, but yeah at the end of the day, so I will be able

01:23:57.250 --> 01:24:01.190
to generate only one data means one by one one by one. I

01:24:01.190 --> 01:24:03.510
will be able to generate data with respect to its

01:24:03.510 --> 01:24:06.470
probability. So

01:24:11.430 --> 01:24:16.210
people are saying something in a group say

01:24:18.580 --> 01:24:21.760
mirror. It should be tokenized by word Deepak

01:24:26.430 --> 01:24:27.850
is saying Deepak is not present.

01:24:33.740 --> 01:24:38.040
Okay, so any question guys any doubt? I hope. We all are

01:24:38.040 --> 01:24:40.440
able to build the model. We all are able to generate our

01:24:40.440 --> 01:24:45.340
next similar data. So obviously I will be more interested

01:24:45.340 --> 01:24:48.020
into the very first one not in the second one not in the

01:24:48.020 --> 01:24:50.440
third one, but yeah, so hope all of us are able to

01:24:50.440 --> 01:24:52.920
understand this entire things

01:25:02.530 --> 01:25:05.470
any question for me guys anyone please go ahead.

01:25:15.720 --> 01:25:19.740
So like this LLM also generate the next word. No, no, not at

01:25:19.740 --> 01:25:23.260
all. See there is a generative model generative model is

01:25:23.260 --> 01:25:26.040
completely different. So over there context window size will

01:25:26.040 --> 01:25:29.400
be. Very very big here. It is just going to generate one

01:25:29.400 --> 01:25:33.500
single data. See if I'm giving going to give a wastage. So

01:25:33.500 --> 01:25:37.300
I'm just trying to say that generate top five. So one by one

01:25:37.300 --> 01:25:39.560
one by one one by one. It is trying to generate whether it's

01:25:39.560 --> 01:25:42.540
a top 5 top 50 whatever I'm going to say, but yeah, it is

01:25:42.540 --> 01:25:45.560
going to generate one by one one by one based on the

01:25:45.560 --> 01:25:50.480
similarity search that what is our next closest vector which

01:25:50.480 --> 01:25:54.360
is available to a wastage obviously wastage will be having

01:25:54.360 --> 01:25:57.180
its own vector. I can even check the. I can even try to

01:25:57.180 --> 01:26:00.400
print the vector which we have done before. So basically it

01:26:00.400 --> 01:26:04.100
will try to check a distance between this vector and the

01:26:04.100 --> 01:26:07.880
next one that okay. So close to NLP. Let's suppose if I'm

01:26:07.880 --> 01:26:10.460
trying to insert NLP. So it will always try to check that

01:26:10.460 --> 01:26:14.120
what is a vector which is very very closer to NLP and based

01:26:14.120 --> 01:26:17.160
on that distance. It is trying to give you the next data

01:26:17.160 --> 01:26:23.340
simple. So in 2D space, so simply it will try to check which

01:26:23.340 --> 01:26:27.380
word is basically closer to this one. And one by one one by

01:26:27.380 --> 01:26:31.120
one one by one only it is trying to generate but but if I'll

01:26:31.120 --> 01:26:33.700
talk about a generatively kind of a model elements kind of a

01:26:33.700 --> 01:26:37.380
model, it will never work in this way. They will be having a

01:26:37.380 --> 01:26:40.720
long context window. They will be having a long output

01:26:40.720 --> 01:26:43.640
sequences so that it will be able to generate not just one

01:26:43.640 --> 01:26:46.280
single token because it is generating or it is trying to

01:26:46.280 --> 01:26:49.180
give you only one single token at a time. Nothing more than

01:26:49.180 --> 01:26:52.040
that guys. Yeah, so don't think that it is going to be same

01:26:52.040 --> 01:26:55.820
as a generatively model. Yes, it will be able to give you

01:26:55.820 --> 01:26:59.360
the next. Next one means technically speaking more or less.

01:26:59.440 --> 01:27:03.240
It is behaving like a generation model, right and it is

01:27:03.240 --> 01:27:06.120
trying to give you a next word based on the distance

01:27:06.120 --> 01:27:09.340
calculation, but it will not be able to give you like a next

01:27:09.340 --> 01:27:12.300
of next of next of next of next because it is not able to

01:27:12.300 --> 01:27:15.480
understand all the relations which generatively model will

01:27:15.480 --> 01:27:18.640
be able to understand in a best possible way. But yeah, it

01:27:18.640 --> 01:27:22.320
is giving you are you can say that I'm able to generate some

01:27:22.320 --> 01:27:26.180
of the tokens out of the input that I'm trying to give. So

01:27:26.180 --> 01:27:28.900
in which use case this model will be used. So wherever you

01:27:28.900 --> 01:27:31.240
are trying to train the data, right? So whenever you are

01:27:31.240 --> 01:27:34.080
trying to train that data for like a long sequence

01:27:34.080 --> 01:27:38.600
generation. So over there you can try to use this model to

01:27:38.600 --> 01:27:42.540
convert your raw data set into its numerical representation

01:27:42.540 --> 01:27:45.580
which is called as embeddings and that's a use of embedding.

01:27:45.640 --> 01:27:51.620
That's not just a use of like a I would say anything else

01:27:51.620 --> 01:27:54.300
apart from this embedding and same thing. You can try to use

01:27:54.300 --> 01:27:57.980
it for like a TFID. You can try to use it a bag of word. You

01:27:57.980 --> 01:28:01.960
can try to use it. Maybe skip Gram C bow anything but yeah

01:28:01.960 --> 01:28:03.740
at the end of the day, it is just going to give you a

01:28:03.740 --> 01:28:09.640
numerical representation. That's it. So I think we can use

01:28:09.640 --> 01:28:12.960
this to build a recommended system. Even you can try to use

01:28:12.960 --> 01:28:16.000
this one for recommendation system for sentiment analysis

01:28:16.000 --> 01:28:22.540
building system. You can try to use it not just that so you

01:28:22.540 --> 01:28:25.440
will be able to use it for a machine translation. You will

01:28:25.440 --> 01:28:29.280
be able to use it even for a text classification. So for. A

01:28:29.280 --> 01:28:32.040
multipurpose is you can try to use it basically not just for

01:28:32.040 --> 01:28:36.720
one and going forward. So I'll going to teach you a better

01:28:36.720 --> 01:28:38.820
model than this one. But yeah now

01:28:41.570 --> 01:28:47.170
so we can even try to represent this entire data set into a

01:28:47.170 --> 01:28:50.610
graph basically that's also possible. So maybe I can try to

01:28:50.610 --> 01:28:54.250
show you into a 2D graph. So where you will be able to see

01:28:54.250 --> 01:28:58.350
all this vector but the problem over here is that we have a

01:28:58.350 --> 01:29:01.430
hundred dimension data set, right? We have basically

01:29:01.430 --> 01:29:04.590
hundred. I mentioned data set. So hundred dimension data set

01:29:04.590 --> 01:29:07.750
graph. Obviously, I will not be able to print it right

01:29:07.750 --> 01:29:09.970
hundred dimension data set. I will not be able to print it.

01:29:10.050 --> 01:29:12.390
So what I can do is I can try to apply a dimensionality

01:29:12.390 --> 01:29:17.690
reduction and then maybe I can try to print this entire data

01:29:17.690 --> 01:29:22.130
as a graph. So shall we print a graph guys? So that I can

01:29:22.130 --> 01:29:26.590
show you like how this word representation looks like not

01:29:26.590 --> 01:29:30.290
hundred D. Obviously, I'll try to convert. I'll do a PC or

01:29:30.290 --> 01:29:33.030
TS any and then I'll try to. Convert it or decompose it into

01:29:33.030 --> 01:29:35.270
a 2D dimension. Yeah, I will do it

01:29:39.590 --> 01:29:42.670
representation wise so that you will be able to see like

01:29:42.670 --> 01:29:46.670
this is how data set looks like. So fine from a scale learn

01:29:46.670 --> 01:29:53.150
a scale learn I can try to import main fold and from there I

01:29:53.150 --> 01:29:57.770
can try to import basically the S and E DS any so which has

01:29:57.770 --> 01:30:00.130
been used for a dimensionality reduction. So I'll just try

01:30:00.130 --> 01:30:02.370
to reduce the hundred dimension into two dimension or maybe

01:30:02.370 --> 01:30:04.890
three dimension because Max to Max. I can show you with

01:30:04.890 --> 01:30:07.450
three dimension. Beyond three dimension. No one can show you

01:30:07.450 --> 01:30:19.670
the graph. So import numpy as NP import numpy as NP then

01:30:19.670 --> 01:30:26.050
import matplotlib.piplot

01:30:26.050 --> 01:30:34.130
as plt fine. So just try to import this one then we can try

01:30:34.130 --> 01:30:39.450
to call our model. So what to vector maybe see bow. I can

01:30:39.450 --> 01:30:44.310
try to call dot wv. So it is going to give me the entire

01:30:44.310 --> 01:30:51.530
vector space. Okay. Word underscore vectors in this

01:30:51.530 --> 01:30:54.230
variable. I can try to store it. So this is basically going

01:30:54.230 --> 01:31:02.050
to give me all the vectors and then vocab is equals to what

01:31:02.050 --> 01:31:07.530
to vector cboss same model index. See. See bow.

01:31:10.910 --> 01:31:20.810
So from this word to vector wrd word vectors, I can try to

01:31:20.810 --> 01:31:29.670
import index to key. Okay. So now if I'll check my vocab, so

01:31:29.670 --> 01:31:32.270
it is giving me all the keys as simple as that. It is giving

01:31:32.270 --> 01:31:34.670
me all the keys. Now I have to just represent all the keys

01:31:34.670 --> 01:31:38.570
into two dimension. Right. So here.

01:31:41.900 --> 01:31:46.520
So here what I can do is so I can try to generate

01:31:48.120 --> 01:31:52.240
basically embeddings for all the keys which is available.

01:31:52.820 --> 01:31:56.640
Maybe even I can try to check what is the length of the key.

01:31:58.610 --> 01:32:02.690
So here let's try to keep it inside a list. Keep

01:32:08.000 --> 01:32:15.180
it inside a list. Okay. Type object not subscribed. Why?

01:32:18.490 --> 01:32:24.150
Word to vector index to key. Okay. So list as a function. I

01:32:24.150 --> 01:32:26.430
have to call. Right. I have just called like a bracket.

01:32:29.550 --> 01:32:32.890
Okay. So if I would like to know that what is the total

01:32:32.890 --> 01:32:37.250
number of unique keys or words that we have. So I can try to

01:32:37.250 --> 01:32:41.750
call vocab and total number of unique data that we have was

01:32:41.750 --> 01:32:47.130
419. Okay. So 419 data I can try to represent now for 419

01:32:47.130 --> 01:32:50.390
data. I have to extract its embeddings means its numerical

01:32:50.390 --> 01:32:54.610
representation. So I think we were able to represent or we

01:32:54.610 --> 01:32:57.510
were able to extract. The numerical representation. So we

01:32:57.510 --> 01:33:00.910
will try to do the exact same thing over here. Right. So

01:33:00.910 --> 01:33:04.190
we'll try for all the data. So here what I can do is so for

01:33:04.190 --> 01:33:12.190
I in vocab which is like a 419 words. Right. So let's try to

01:33:12.190 --> 01:33:20.910
generate basically word vector of all the I. So all the I.

01:33:21.750 --> 01:33:26.190
Let's try to generate a vector. And then keep it inside a

01:33:26.190 --> 01:33:32.450
list. So basically it's going to be this big. Yeah. So for

01:33:32.450 --> 01:33:37.970
all the 419 word that we have guys right inside this vocab.

01:33:38.010 --> 01:33:41.450
So for all the words I'm able to generate a numerical

01:33:41.450 --> 01:33:45.490
representation. So one for one word. So numerical

01:33:45.490 --> 01:33:48.130
representation is taking hundred space. Basically it's a

01:33:48.130 --> 01:33:52.090
like a hundred D or like vector size that we have given over

01:33:52.090 --> 01:33:56.670
here. And then eventually. We can try to convert this entire

01:33:56.670 --> 01:34:02.410
things into a numpy array. So in closing it within a numpy

01:34:02.410 --> 01:34:07.710
array. Okay. So numpy array we are able to create. So as you

01:34:07.710 --> 01:34:11.450
can see there is a array inside the array. So one array is

01:34:11.450 --> 01:34:16.230
representing one of the word into hundred dimension. Maybe I

01:34:16.230 --> 01:34:18.790
can try to convert it into a data frame and I can try to

01:34:18.790 --> 01:34:20.730
show you. So there are hundred number of columns. You will

01:34:20.730 --> 01:34:24.730
be able to find out just to represent one single word. I can

01:34:24.730 --> 01:34:29.290
try to call this data as X. So this is my data which I have

01:34:29.290 --> 01:34:36.850
created. Now my problem is that that this entire data set is

01:34:36.850 --> 01:34:41.490
available into a hundred dimension. So what I can do is. So

01:34:41.490 --> 01:34:48.290
maybe I can try to reduce our dimension to 2D and to reduce

01:34:48.290 --> 01:34:51.190
our dimension. I can try to use a PCA or maybe I can try to

01:34:51.190 --> 01:34:54.130
use a TSN. So TSN is something which I have done. As an

01:34:54.130 --> 01:34:59.850
import. So I can try to call TSN. TSN over here which is a

01:34:59.850 --> 01:35:03.450
dimensionality reduction technique. Number of component is

01:35:03.450 --> 01:35:08.270
equals to two. I can try to give and then maybe I can try to

01:35:08.270 --> 01:35:12.210
give. There is something called as perplexity value and I

01:35:12.210 --> 01:35:17.270
can try to pass X over here. So this TSN is going to

01:35:17.270 --> 01:35:23.550
decompose my entire data set into two dimension. Fine. So

01:35:23.550 --> 01:35:26.250
here TSN E.

01:35:29.520 --> 01:35:33.080
Fine. So this is the model which I have created. And now

01:35:33.080 --> 01:35:36.160
what I will do is. So I'll just try to pass this entire

01:35:36.160 --> 01:35:44.940
data. Right. So I'll try to pass this TSN E. TSN E dot fit

01:35:44.940 --> 01:35:51.740
transform. I can try to call. I can pass my perplexity is

01:35:51.740 --> 01:35:56.580
equals to X. I have given or. Let me remove this perplexity.

01:35:57.380 --> 01:36:02.940
So fit dot transform and then X. So it is going to convert

01:36:04.820 --> 01:36:07.900
my data as you can see into two dimensional. Right. Into two

01:36:07.900 --> 01:36:11.360
dimension. It is able to convert my entire data set. Now

01:36:11.360 --> 01:36:15.080
what I will do is. So this is basically a new embedding. So

01:36:15.080 --> 01:36:20.920
from 100D I am able to convert it to a 2D. So here X

01:36:20.920 --> 01:36:23.220
underscore new new

01:36:27.670 --> 01:36:30.450
embeddings. Right. So this is the new embeddings. Which I am

01:36:30.450 --> 01:36:33.510
able to create. Now once my data set is available into a 2D.

01:36:34.070 --> 01:36:38.890
I can happily try to plot it. So here just try to call like

01:36:38.890 --> 01:36:47.010
a PLT plot. Dot figure. And just try to draw the canvas

01:36:47.010 --> 01:36:54.050
size. So fig size I can try to say maybe 12 by 6. 12 by 6.

01:36:54.390 --> 01:36:59.410
Okay. And then I can try to do the scatter plot. So PLT dot

01:36:59.410 --> 01:37:04.290
scatter. Scatter plot. And scatter plot wise. I can try to

01:37:04.290 --> 01:37:05.130
give basically.

01:37:08.940 --> 01:37:19.680
X over here. So X new embeddings. And take all the row and

01:37:19.680 --> 01:37:25.900
zeroth column. Because we have two column. And so as a X.

01:37:26.020 --> 01:37:30.960
And X new embeddings. So take basically all the row. And

01:37:30.960 --> 01:37:33.960
then first column as a Y. So we have two dimensional data.

01:37:34.060 --> 01:37:38.440
Fine. And then maybe I can try to give a dot over here. So

01:37:38.440 --> 01:37:41.060
there is something called as marker. So marker is going to

01:37:41.060 --> 01:37:45.600
be any symbol I can try to give. Maybe O let's suppose. So

01:37:45.600 --> 01:37:51.800
marker is equals to O. Okay. So this is our data that we are

01:37:51.800 --> 01:37:57.420
able to get guys. This is the data representation. Now we

01:37:57.420 --> 01:38:00.360
don't know that which dot is what. Basically. Right. So

01:38:00.360 --> 01:38:03.700
which dot is representing what word. Anyhow we are able to

01:38:03.700 --> 01:38:06.220
represent the embeddings. And it's nothing but the same

01:38:06.220 --> 01:38:09.920
data. 2D data that we are able to get. Now we have to label

01:38:09.920 --> 01:38:14.920
all of this data. That's a bit of challenging task. And so

01:38:14.920 --> 01:38:16.960
that we will get to know that. Okay fine. So which word is

01:38:16.960 --> 01:38:21.740
representing which one by the way. Yeah. So let's do it. Not

01:38:21.740 --> 01:38:28.180
an issue at all. Right. So maybe we can try to check our

01:38:28.180 --> 01:38:32.200
data. So my data was available. Into vocabulary. Right. This

01:38:32.200 --> 01:38:37.280
vocabulary. Was having my data. And the same data I have

01:38:37.280 --> 01:38:40.940
taken. To create the embedding. So I just have to do a

01:38:40.940 --> 01:38:46.800
mapping over here. Okay. So I can try to write over here. So

01:38:46.800 --> 01:38:52.200
for. I comma. Words. I'm just trying to do an enumeration.

01:38:53.820 --> 01:39:00.180
Words in. Enumerate. And enumerate what. Vocab. The

01:39:00.180 --> 01:39:07.140
vocabulary that I have created. And then. Do one thing. So

01:39:07.140 --> 01:39:12.860
start from zero. And then. Go till the length of. Basically.

01:39:14.160 --> 01:39:18.420
Data embeddings. Okay. So just run for. This particular

01:39:18.420 --> 01:39:26.160
loop. And then once it's done. PLT dot. Annotate. So

01:39:26.160 --> 01:39:31.320
annotate all the circles. Or dots. And. Here. So it's going

01:39:31.320 --> 01:39:39.520
to be. Words. And. XY. Is equals to. So we have to give. X

01:39:39.520 --> 01:39:47.780
axis and Y axis. So. X. New embeddings. Or. I. Start from

01:39:47.780 --> 01:39:52.240
this. Go till. Zero. And then. X. New embedding is done.

01:39:53.880 --> 01:39:58.120
Column X. Basically. And then for Y. So X. New embeddings.

01:40:03.930 --> 01:40:08.710
New embeddings. And. Same for Y axis. So. Go with. Y. And

01:40:08.710 --> 01:40:08.930
then.

01:40:14.420 --> 01:40:15.280
Very small.

01:40:18.850 --> 01:40:22.810
It is able to annotate it. But. My graph. Is. Very very

01:40:22.810 --> 01:40:23.070
small.

01:41:02.180 --> 01:41:06.060
Yeah. So. This is the graph guys. As you can see. So. We are

01:41:06.060 --> 01:41:10.700
able to. Represent. All the word. So learning. And then.

01:41:11.180 --> 01:41:13.620
Closer to learning. You are able to see. We have a

01:41:13.620 --> 01:41:17.820
repetitive. And then. We have 419 data. So. Because of that.

01:41:17.880 --> 01:41:21.320
It is showing you. Like this. Clumsy. Kind of.

01:41:23.280 --> 01:41:27.640
Representation. Maybe. Just to eliminate. This. One. What I

01:41:27.640 --> 01:41:33.420
can do is. I can try to. Reduce. Our data. So. That. Things

01:41:33.420 --> 01:41:36.660
will be. Bit. Clean. To all of you. Yeah.

01:41:48.860 --> 01:41:51.480
So. Taking a very small data. So. I don't think that now.

01:41:51.600 --> 01:41:52.440
There will be a lot of.

01:41:59.060 --> 01:42:01.960
Yeah. So. Now. As you can see. So. We have less number of

01:42:01.960 --> 01:42:05.600
data. And. Let's suppose. If I am trying to search for home.

01:42:05.780 --> 01:42:08.980
It is going to give me. It's basically. If I am trying to

01:42:08.980 --> 01:42:11.340
search online. It is going to give me. Creation. So. This is

01:42:11.340 --> 01:42:13.920
nothing but. I am just trying to show you. In a visual

01:42:13.920 --> 01:42:17.660
format. So. Whenever. You are trying to call. Some simple.

01:42:17.740 --> 01:42:19.660
Or. You are going to call. A function called. As most

01:42:19.660 --> 01:42:23.480
similar. So. Obviously. Based on the. Distance between. A

01:42:23.480 --> 01:42:26.320
data. So. It is going to give you. The recommendation. Or.

01:42:26.380 --> 01:42:29.120
It is going to give you. The next word. Nothing much. It is

01:42:29.120 --> 01:42:31.360
trying to do. Apart from this one. So. What I have done. I

01:42:31.360 --> 01:42:34.160
have just converted. My hundred dimension data. Into. Two

01:42:34.160 --> 01:42:36.600
dimensions. So. That I will be able to. Represent it. Into a

01:42:36.600 --> 01:42:40.780
2D format. Making sense guys. To all of us. So. This graph

01:42:40.780 --> 01:42:43.360
plot. Is nothing. Like. I am just trying to give. X axis and

01:42:43.360 --> 01:42:44.220
Y axis. That's it.

01:42:53.860 --> 01:42:59.860
So. Now. I am going.

01:43:05.640 --> 01:43:06.200
To convert.

01:43:11.200 --> 01:43:15.200
My data. Into a. Three dimension. While. Keeping. A meaning.

01:43:15.300 --> 01:43:19.820
Of. A data. As. Much. As. Possible. Maybe. 90%. Property. It

01:43:19.820 --> 01:43:22.320
is going to retain. But. Same data. It is going to convert.

01:43:22.380 --> 01:43:27.860
Into a. Lower. Dimension. This is what. A TSN is doing.

01:43:28.020 --> 01:43:31.060
Maybe. Instead of TSN. You can try to use a. PCA. Principal

01:43:31.060 --> 01:43:33.520
component analysis. So. Even that is going to work for you.

01:43:33.620 --> 01:43:36.420
And. Even that is going to convert. So. TSN. I am not

01:43:36.420 --> 01:43:38.780
teaching as of now. Please come to my. Machine learning

01:43:38.780 --> 01:43:42.280
class. Over there. I used to teach. But. Yeah. So. I am

01:43:42.280 --> 01:43:44.860
assuming that. Like. We all understand. The meaning of.

01:43:45.340 --> 01:43:48.220
Dimensionality. Reduction. So. How we are able to. Reduce

01:43:48.220 --> 01:43:51.280
the. Dimension. And. What is the actual meaning of it. So. I

01:43:51.280 --> 01:43:53.360
am just trying to convert. My 100 dimensional data. Into 2

01:43:53.360 --> 01:43:55.640
dimensional data. So. That I can show you. Inside a. Graph.

01:43:56.760 --> 01:43:59.460
Fine guys. Everyone. Making sense. Any. Question for me.

01:44:03.320 --> 01:44:06.700
Yeah. So. We are able to build. A model. Today. Two models.

01:44:06.800 --> 01:44:09.400
Seba. Scriptgram. We are able to. Test the model.

01:44:18.640 --> 01:44:21.040
Basically. It's converting everything. Into a vector. Which

01:44:21.040 --> 01:44:25.240
is nothing but. 100 dimensional vector. Representation. Or.

01:44:25.460 --> 01:44:29.300
Just by looking into this picture. It's basically. 2D. So.

01:44:29.340 --> 01:44:31.620
Into a 2D. I am just trying to represent the data. And. It

01:44:31.620 --> 01:44:33.700
is just trying to give you the. Nearest one. That's it.

01:44:34.160 --> 01:44:38.420
Yeah. Machine learning classes in data science. Yeah. FSDS.

01:44:38.560 --> 01:44:39.380
It's basically. FSDS.

01:44:43.260 --> 01:44:48.440
Fine guys. Any doubt. With respect to. What to vector. With

01:44:48.440 --> 01:44:51.720
respect to. Embedding. Concept. I think we are. Very much

01:44:51.720 --> 01:44:56.180
clear. That. What. Exactly. Embedding. Concept. Is. And. I

01:44:56.180 --> 01:44:58.860
believe. In future. So. If I am going to. Explain you. Some

01:44:58.860 --> 01:45:01.420
other. Algorithm. With respect to. Embedding. You all will

01:45:01.420 --> 01:45:03.200
be able to. Understand it. Yeah.

01:45:06.710 --> 01:45:09.170
Need to. Revise. A class. Again. But. Great. Stuff. For.

01:45:09.170 --> 01:45:12.870
Day. I don't think that. You should. Revise. I have not.

01:45:13.190 --> 01:45:16.230
Discussed. Much. If. I'll. Go. Through. The. Entire. Class.

01:45:16.370 --> 01:45:19.090
So. I am. Just. Talking. About. A. Data. Cleaning. Which.

01:45:19.190 --> 01:45:23.170
Is. A. Normal. Python. Code. Then. Next. Step. Is. I'm.

01:45:23.230 --> 01:45:26.450
Trying. To. Call. Basically. Like. What. To. Vector. From.

01:45:26.490 --> 01:45:28.450
Gen. Sim. I'm. Trying. To. Set. Up. All. The. Parameter.

01:45:28.570 --> 01:45:31.010
Building. A. Model. Now. What. Model. Is. Trying. To. Do.

01:45:31.030 --> 01:45:32.830
So. Model. Is. Trying. To. Give. Me. The. Numerical.

01:45:33.410 --> 01:45:35.990
Representation. Of. The. Data. That's. It. For. Each. And.

01:45:36.010 --> 01:45:39.350
Every. Token. Or. I. Say. A. Word. For. Each. And. Every.

01:45:39.470 --> 01:45:41.110
Token. So. It. Is. Trying. To. Give. Me. The. Numerical.

01:45:41.650 --> 01:45:44.030
Representation. And. Then. I'm. Trying. To. Like. Generate.

01:45:44.150 --> 01:45:47.570
Or. I'm. Trying. To. Find. Out. The. Similar. One. And. How.

01:45:47.670 --> 01:45:49.770
It. Is. Trying. To. Give. Me. The. Similar. One. Nothing.

01:45:50.030 --> 01:45:52.510
Based. On. The. Distance. Calculation. Right.

01:45:59.090 --> 01:45:59.290
So.

01:46:05.620 --> 01:46:05.820
I.

01:46:09.520 --> 01:46:11.100
Believe. We. All. Know. How. To. Do. A. Distance.

01:46:11.240 --> 01:46:12.080
Calculation. Between. This.

01:46:15.780 --> 01:46:18.800
Okay. So. Now. This. Question. How. Do. You. Remember. All.

01:46:19.080 --> 01:46:22.440
This. It's. Been. Seven. To. Eight. Years. Since. I'm.

01:46:22.460 --> 01:46:25.580
Teaching. So. And. I'm. Coding. Live. In. A. Class. Itself.

01:46:25.740 --> 01:46:29.460
Apart. From. That. So. I. Used. To. Work. A. Lot. So. With.

01:46:29.520 --> 01:46:32.400
A. Developer. For. An. Entire. Seven. Days. Not. Like. Five.

01:46:32.600 --> 01:46:33.600
Days. So. Yeah.

01:46:37.260 --> 01:46:39.800
These. Concepts. Are. Nothing. These. Are. The. Not. Even.

01:46:39.860 --> 01:46:42.780
I'll. Say. A. Complete. Basic. It's. A. Basic. Or. Basic. I.

01:46:42.940 --> 01:46:43.080
Say.

01:47:00.060 --> 01:47:02.060
Once. Okay. So. Deepak. Is. Saying. So. I. Think. By. Today.

01:47:02.120 --> 01:47:04.140
All. The. Prequisite. Has. Been. Completed. And. From. Next.

01:47:04.220 --> 01:47:06.800
Class. We. Enter. Into. A. World. Of. DL. Yeah. So. Let. Me.

01:47:06.800 --> 01:47:09.500
Open. Up. A. Syllabus. Guys. Syllabus. Where. Is. The.

01:47:09.620 --> 01:47:09.860
Syllabus.

01:47:13.360 --> 01:47:20.720
Euron. So. Generative. Generative. AI. Yeah. This. The.

01:47:20.760 --> 01:47:25.400
Badge. So. Basically. Guys. I. Have. Covered. This. Section.

01:47:25.780 --> 01:47:28.260
So. Let. Me. Go. Through. This. If. I. Have. Not. Missed.

01:47:28.300 --> 01:47:31.200
Anything. So. Yeah. Basically. Introductions. Of. Course.

01:47:31.280 --> 01:47:33.780
We. Have. Already. Done. That. And. Then. I. Have. Covered.

01:47:33.780 --> 01:47:36.460
This. Entire. Part. So. What. To. Vector. T. F. I. D. A.

01:47:36.500 --> 01:47:38.740
Bag. Of. Word. One. Hot. Encoding. Parts. Of. Speech.

01:47:39.240 --> 01:47:41.900
Normalization. Text. Segmentation. Limitization. Stop. Word.

01:47:42.080 --> 01:47:44.460
Removal. And. We. Have. Used. It. Even. Multiple. Times.

01:47:44.540 --> 01:47:46.880
And. Today. I. Think. We. Have. Already. Used. This. Basic.

01:47:46.980 --> 01:47:48.840
Part. That. We. Have. Discussed. In. My. Previous. Class.

01:47:49.360 --> 01:47:52.580
Now. From. Next. Saturday. Onwards. I'm. Going. To. Start.

01:47:52.680 --> 01:47:56.560
With. This. Before. Starting. With. Because. This. Syllable.

01:47:56.660 --> 01:47:59.720
Says. That. That. I. Have. To. Start. From. RNN. Right. Now.

01:47:59.980 --> 01:48:02.220
RNN. Is. Nothing. But. A. Recurrent. Neural. Network. It's.

01:48:02.500 --> 01:48:04.600
Basically. A. Extension. Of. A. Neural. Network. Which. Has.

01:48:04.640 --> 01:48:07.580
Been. Used. For. That. Text. Or. Any. Kind. Of. Data. So.

01:48:07.660 --> 01:48:10.100
Where. To. Backward. Forward. Establish. The. Relationship.

01:48:10.120 --> 01:48:12.640
We'll. Talk. More. About. It. When. I'll. Try. To. Talk.

01:48:12.640 --> 01:48:16.340
About. The. Architecture. And. Research. Paper. So. Before.

01:48:16.580 --> 01:48:19.720
That. I'll. Be. Talking. About. A. Neural. Network. Because.

01:48:20.080 --> 01:48:23.240
Without. That. So. Let's. Suppose. Some. Of. You. Are. Not.

01:48:23.460 --> 01:48:25.580
Or. Maybe. You. Have. Gone. Through. A. Neural. Concept.

01:48:25.840 --> 01:48:27.940
Along. Back. Maybe. You. Will. Not. Be. Able. To. Recall.

01:48:27.960 --> 01:48:31.140
Each. And. Everything. So. From. Next. Class. Onwards. I'll.

01:48:31.160 --> 01:48:33.980
Try. To. Discuss. Neural. Network. And. Then. One. By. One.

01:48:34.180 --> 01:48:35.060
By. One. By.

01:48:53.100 --> 01:49:03.580
One. By. Plus. One. By. One. And. The. That. Trends. Full.

01:49:03.940 --> 01:49:08.120
So. I've. S wrightened. That's Because. Let's Renew. Have.

01:49:08.120 --> 01:49:11.660
like like you can visualize any of this model which is

01:49:11.660 --> 01:49:14.900
available into a market or maybe even inside the uranus

01:49:14.900 --> 01:49:17.740
system believe me that that is a kind of a visualization i'm

01:49:17.740 --> 01:49:20.880
going to give you right and very clearly you will be able to

01:49:20.880 --> 01:49:24.280
understand that if i have to do it right then what is

01:49:24.280 --> 01:49:27.820
required how i will be able to build the entire pipeline or

01:49:27.820 --> 01:49:31.220
maybe how which kind of a tuning which i will be able to do

01:49:31.220 --> 01:49:34.380
so that kind of understanding all of you will be able to get

01:49:34.380 --> 01:49:38.280
once i'll complete this entire portion the major part is

01:49:38.280 --> 01:49:42.660
actually this one so but yeah to understand this these

01:49:42.660 --> 01:49:45.580
things are important before that whatever we have mentioned

01:49:45.580 --> 01:49:51.700
it's important and then again back to back practical paper

01:49:53.040 --> 01:49:56.100
is by google yeah so attention is all you need it's a paper

01:49:56.100 --> 01:49:59.820
released by google in 2017 i believe yeah it's a 2017 paper

01:49:59.820 --> 01:50:03.420
so that is a research paper which i am going to discuss but

01:50:03.420 --> 01:50:07.640
in very detail you in a very simple way don't worry so don't

01:50:07.640 --> 01:50:10.580
think that uh if it is a research paper so you will not be

01:50:10.580 --> 01:50:12.620
able to understand yeah if you will go and download now

01:50:12.620 --> 01:50:15.520
maybe you will not be able to understand in a class i'll

01:50:15.520 --> 01:50:17.960
make sure that you all will be able to understand like a

01:50:17.960 --> 01:50:24.000
story and even you will be able to explain to someone uh mti

01:50:24.000 --> 01:50:26.540
is saying off topic please look into the sign-in option or

01:50:26.540 --> 01:50:28.680
you don't have we cannot log in with the email other than

01:50:28.680 --> 01:50:31.440
gmail yeah mti so actually because of the security reason so

01:50:31.440 --> 01:50:34.980
we are we have removed all the other options because there

01:50:34.980 --> 01:50:38.800
was some spam that we have seen that we have received and uh

01:50:38.800 --> 01:50:44.360
with some of the other server except a gmail server so uh

01:50:44.360 --> 01:50:47.980
it's a little bit difficult to like pass the security so

01:50:47.980 --> 01:50:51.500
keeping that in our mind we have uh removed other option

01:50:51.500 --> 01:50:58.800
actually mts yeah so we have done that purposely and like uh

01:50:58.800 --> 01:51:04.440
i'm completely aware about it in er is missed is it it's

01:51:04.440 --> 01:51:06.600
just a single library so it is going to give you like an

01:51:06.600 --> 01:51:07.140
immediate request so it is going to give you like an

01:51:07.140 --> 01:51:07.500
immediate request so it is going to give you like an

01:51:07.500 --> 01:51:07.500
immediate request so it is going to give you like an

01:51:07.500 --> 01:51:07.500
immediate request so it is going to give you like an

01:51:07.500 --> 01:51:07.500
immediate request so it is going to give you like a

01:51:07.500 --> 01:51:08.200
modernization could

01:51:11.440 --> 01:51:14.520
you please share the summarized document portion of the

01:51:14.520 --> 01:51:17.220
architecture part along with the study material don't worry

01:51:17.220 --> 01:51:19.820
so we are i am going to draw the architecture in a live

01:51:19.820 --> 01:51:22.800
class like i am writing a code in the similar way so i'll

01:51:22.800 --> 01:51:25.600
draw the things and that will be shared with you immediately

01:51:25.600 --> 01:51:31.070
inside the class so i think n er has been missed okay fine

01:51:31.070 --> 01:51:34.030
so if anywhere has been missed i'll call one single library

01:51:34.030 --> 01:51:38.510
and then i'll show you i think it's not okay見ved went

01:51:38.510 --> 01:51:39.270
missing so if there is out of any others i believe i have

01:51:39.270 --> 01:51:42.070
discussed air has been missed so we'll then obtain all the

01:51:42.070 --> 01:51:45.490
outdated cases which we can missed okay any idea guys what

01:51:45.490 --> 01:51:48.710
is NER I don't think it's a big concept it

01:52:14.220 --> 01:52:19.780
is missed I can discuss now itself let me discuss NER guys

01:52:19.780 --> 01:52:22.900
so that it will not be missed POS I think I have already

01:52:22.900 --> 01:52:27.160
discussed right yeah NER is like a very simple concept I am

01:52:27.160 --> 01:52:30.560
showing you now so with the help of NLTK itself I am going

01:52:30.560 --> 01:52:34.920
to show you so just wait so here let's suppose there is a

01:52:34.920 --> 01:52:39.020
text I am going to take and I'm saying that that my name is

01:52:39.020 --> 01:52:51.060
Sudhanshu Kumar I am working with Euron as a mentor and as a

01:52:51.060 --> 01:53:02.110
developer so where I used to where I used to where I used to

01:53:02.110 --> 01:53:12.530
teach data stack as well as well as I used to work to

01:53:16.820 --> 01:53:28.390
build a AI system in Euron okay so now this is the text data

01:53:28.390 --> 01:53:32.090
this is the text which I have taken any any simple text

01:53:32.090 --> 01:53:36.690
which I have taken over here now so basically what I can do

01:53:36.690 --> 01:53:41.950
is so I can try to call a library maybe NLTK library I can

01:53:41.950 --> 01:53:48.250
try to call and then I can try to basically like a print a

01:53:48.250 --> 01:53:51.250
name entity recognition name entity recognition means if it

01:53:51.250 --> 01:53:53.950
is a name it will try to tell you that okay this is the

01:53:53.950 --> 01:53:57.390
person if it is a place it will try to tell you that okay

01:53:57.390 --> 01:54:00.410
fine this is the place that is something called as NER like

01:54:00.410 --> 01:54:03.610
POS right POS I have already discussed so in case of POS

01:54:03.610 --> 01:54:05.930
what happens so basically it will try to tell you that okay

01:54:05.930 --> 01:54:11.630
fine it is a verb it is an adjective it is a term of POS

01:54:11.630 --> 01:54:14.630
parts of a speech I think I have already shown you the

01:54:14.630 --> 01:54:19.870
examples in my very first class of writing a code so this

01:54:21.150 --> 01:54:25.310
one right NER so let's suppose I am trying to pass a text

01:54:25.310 --> 01:54:29.370
over here what I can do is I can try to call a tokenize

01:54:29.370 --> 01:54:37.870
basically word tokenize word tokenize hope this is not a

01:54:37.870 --> 01:54:42.410
variable yeah so word tokenize is missing one record okay so

01:54:42.410 --> 01:54:45.890
I can try to pass my data over here so that I will be able

01:54:45.890 --> 01:54:48.350
to tokenize the entire data I believe we all understand what

01:54:48.350 --> 01:54:52.590
is the meaning of tokenization over here and then what I can

01:54:52.590 --> 01:54:58.590
do is so I can try to pass this entire tokenized data into

01:54:58.590 --> 01:55:02.750
NER function so how I will be able to get NER function so I

01:55:02.750 --> 01:55:08.430
can get it from NLTK so from NLTK I can say import and

01:55:08.430 --> 01:55:14.490
import any chuck so I am going to help me out to locate or

01:55:14.490 --> 01:55:18.370
to like you know tag with the respective NER name entity

01:55:18.370 --> 01:55:23.350
iconization so N E dot chuck and then I'm going to pass this

01:55:23.350 --> 01:55:27.730
data or tokenized data let's see what happens Oh

01:55:29.810 --> 01:55:41.540
it a string index out of range okay string index hour it

01:55:53.740 --> 01:55:56.500
text the

01:56:00.720 --> 01:56:02.180
experiment trading āŦ Ṧ āぎ āĦgĕаф āĦgĕāĦ āĦ āġĕ Ṧ āŠ āš ārĕ

01:56:02.180 --> 01:56:02.180
āĦ went to help me out to locate or to like you know tagged

01:56:02.180 --> 01:56:02.180
with a respective NER entity organization so any dot chuck

01:56:02.180 --> 01:56:02.180
and then I'm going to pass this data a tokenized data let's

01:56:02.180 --> 01:56:02.180
see what happens string index out of range okay string index

01:56:02.180 --> 01:56:02.180
or it text Call unnecessary semiclaudial prefix and a string

01:56:02.180 --> 01:56:08.520
index sandwich remain. why it is giving me error in dot

01:56:08.520 --> 01:56:14.040
chunk target tokens I can send any here so this is basically

01:56:14.040 --> 01:56:18.900
a correct one which I'm trying to send even

01:56:22.580 --> 01:56:26.260
as per definition word tokenize okay

01:56:39.290 --> 01:56:46.990
so let me import first let's suppose I'm going to call

01:56:46.990 --> 01:56:53.930
instead of this I will be calling POS first parts of a

01:56:53.930 --> 01:57:02.700
speech just a method guys so not a big deal I can change it

01:57:02.700 --> 01:57:08.660
and deal with it tag and then any Chuck I can try to call it

01:57:08.660 --> 01:57:12.460
on top of this one so I think we all understand POS or

01:57:14.830 --> 01:57:23.130
POS working well no module name SVG link okay why module not

01:57:23.130 --> 01:57:26.870
found error no module name SVG LI NG

01:57:28.820 --> 01:57:29.800
install

01:57:33.220 --> 01:57:36.580
SVG LI NG let's see hit

01:57:39.300 --> 01:57:45.180
now any

01:57:45.180 --> 01:57:49.100
are Chuck my name is Danshu Kumar and I'm working with a

01:57:49.100 --> 01:57:55.980
Euron as a mentor and a developer so here world and

01:57:55.980 --> 01:57:57.860
everything yeah

01:57:59.690 --> 01:58:07.210
so we are people was then what is basically objective this

01:58:20.970 --> 01:58:24.250
library is not giving me the

01:58:33.500 --> 01:58:37.520
correct one any here so basically I can try to use some

01:58:37.520 --> 01:58:42.000
other libraries as well some better library yeah this is

01:58:42.000 --> 01:58:45.140
giving me a better POS I can say so POS tagging why it's

01:58:45.140 --> 01:58:48.300
completely fine so as you can say that noun and then Beau

01:58:48.300 --> 01:58:52.000
and then this is the JJ means objective by the way now this

01:58:52.000 --> 01:58:55.180
is again a wall wall so an object added it then and this

01:58:55.180 --> 01:58:55.180
means objective by the way now this is again a word go so

01:58:55.180 --> 01:58:55.460
and then as you can see a POS IF container which is a it is

01:58:55.460 --> 01:59:00.720
able to give me a perfect POS but let me use some other

01:59:00.720 --> 01:59:04.060
library so that I can show you just as a demo that how I can

01:59:04.060 --> 01:59:09.940
try to print name entity recognition. So let me use a best

01:59:09.940 --> 01:59:17.140
one transformer library directly from trns for mer but

01:59:17.140 --> 01:59:23.080
transformer I will have to install first so pip install trns

01:59:23.080 --> 01:59:25.160
for mer transformers.

01:59:28.260 --> 01:59:31.900
This is the advanced library so from a hugging face I can

01:59:31.900 --> 01:59:35.720
try to install and then this is going to tell you like a

01:59:35.720 --> 01:59:39.240
better way because this is having way more capability as

01:59:39.240 --> 01:59:41.260
compared to the previous one.

02:00:16.230 --> 02:00:20.250
Transformer I am able to download so are you able to chat

02:00:20.250 --> 02:00:24.970
guys because by 4.30 it was off for couple of second so can

02:00:24.970 --> 02:00:29.590
you please confirm from transformers. I'm just trying to

02:00:29.590 --> 02:00:33.210
show you one like how we can try to generate basically name

02:00:33.210 --> 02:00:36.250
entity recognition like a person to person if it is a place

02:00:36.250 --> 02:00:38.370
so it will try to tell you that okay fine this is the place

02:00:38.370 --> 02:00:47.230
something like that. So from transformer import pip pipeline

02:00:47.230 --> 02:00:52.510
actually yeah so from transformer import pipeline and then

02:00:52.510 --> 02:00:57.350
inside a pipeline I can try to say that this is again one of

02:00:57.350 --> 02:01:00.970
the five functions so I can say that give me NER and then

02:01:00.970 --> 02:01:07.270
try to use some of the model to identify basically NER. So

02:01:07.270 --> 02:01:11.850
maybe model wise I can try to use one of the pre trained

02:01:11.850 --> 02:01:16.110
BERT model. So this is a pre trained BERT model I can try to

02:01:16.110 --> 02:01:24.440
use by the way. So here NER underscore G N E R A T I O N N E

02:01:24.440 --> 02:01:28.040
R generation we can't chat sir chat option is not showing so

02:01:28.040 --> 02:01:30.820
please try to refresh. So that

02:01:36.000 --> 02:01:44.720
you can do so this is a pre trained BERT model

02:01:47.310 --> 02:01:47.790
which provides for the user added username and password.

02:01:57.840 --> 02:02:04.280
Its not

02:02:13.040 --> 02:02:17.940
even being reactivated at that time so just類ve for

02:02:21.270 --> 02:02:29.790
which and obviously it is sau Rainbow

02:02:29.790 --> 02:02:33.310
Boot. is basically a organization or maybe if i'll talk

02:02:33.310 --> 02:02:36.670
about bangalore so it will tell you that okay fine bangalore

02:02:36.670 --> 02:02:40.310
is a location so this is something called as ner named

02:02:40.310 --> 02:02:43.370
entity organization so it will be able to recognize

02:02:43.370 --> 02:02:46.290
automatically that whether i'm talking about the person or

02:02:46.290 --> 02:02:51.170
organization or location this is the only thing which comes

02:02:51.170 --> 02:02:55.550
with ner i can use not just like nltk or this library maybe

02:02:55.550 --> 02:02:59.870
i can try to use spacey library as well even i can try to

02:02:59.870 --> 02:03:06.030
use maybe a stanford nlp library and i can generate this ner

02:03:06.030 --> 02:03:13.640
so collecting done currently on account pidentic installing

02:03:14.540 --> 02:03:15.100
there

02:03:19.510 --> 02:03:22.810
is some issue with my base environment guys because of that

02:03:22.810 --> 02:03:25.090
it is giving me an issue issue

02:03:32.580 --> 02:03:36.800
with my tensorflow or pytorch so pytorch

02:04:02.940 --> 02:04:07.420
is not working in my system i'll have to fix it so let me

02:04:07.420 --> 02:04:11.280
try this ner example with nlp itself let's

02:04:13.520 --> 02:04:16.140
check what is the issue that collab

02:04:41.710 --> 02:04:44.690
it's working yeah it will work i like i said right so

02:04:44.690 --> 02:04:48.890
basically it's an issue with my system nothing else let me

02:05:07.740 --> 02:05:10.580
change my sentence this

02:05:33.940 --> 02:05:37.020
is just showing me the graph let

02:05:47.160 --> 02:05:54.520
me just change something here barack obama as this is the

02:05:54.520 --> 02:05:58.640
old library is a precedent

02:06:00.880 --> 02:06:04.420
or was the president whatever it is just

02:06:10.630 --> 02:06:13.510
giving me a pos not giving me

02:06:18.950 --> 02:06:23.510
okay something is hidden it seems because

02:06:29.560 --> 02:06:33.100
ideally it should give you like a ner as well so basically

02:06:33.100 --> 02:06:38.380
it should give you a person it should give you a place and

02:06:38.380 --> 02:06:41.420
it should give you like all the detail this is what this

02:06:44.970 --> 02:06:50.030
any chuck does in general let me change this entire sentence

02:06:50.030 --> 02:06:51.910
itself uh

02:06:54.020 --> 02:06:57.260
yeah now it's it's showing so the problem with the library

02:06:57.260 --> 02:06:59.940
was a little bit there is a problem with the library so

02:06:59.940 --> 02:07:04.020
basically when i'm trying to use uh this lntky library so

02:07:04.020 --> 02:07:06.220
obviously it is not going to identify each and everything

02:07:06.220 --> 02:07:08.640
because it's an old one and it's not been trained on the new

02:07:08.640 --> 02:07:12.060
data so it's having a very limited one that's the reason now

02:07:12.060 --> 02:07:15.880
if i'm going to use maybe uh like some of the latest trained

02:07:15.880 --> 02:07:18.560
libraries based on the neural network it is going to show me

02:07:18.560 --> 02:07:21.900
the latest one so here as you can see that uh everything

02:07:21.900 --> 02:07:24.020
everything is there for me and this one is trying to show

02:07:24.020 --> 02:07:27.600
you Barack basically is a person Obama is basically a person

02:07:27.600 --> 02:07:32.860
right and it is trying to identify basically united and

02:07:32.860 --> 02:07:36.180
state is basically a geographical location GPE means

02:07:36.180 --> 02:07:39.060
geographical position or location basically so this is

02:07:39.060 --> 02:07:43.600
called as this person person and then like this GPE is

02:07:43.600 --> 02:07:48.140
actually called as NER again library to library the result

02:07:48.140 --> 02:07:51.460
is going to differ it is not going to be same right so

02:07:51.460 --> 02:07:56.300
always it's advisable to use some of the transformer based

02:07:56.300 --> 02:08:00.040
library instead of using NLTK or spacey kind of libraries

02:08:00.040 --> 02:08:03.800
because these libraries having a limitation so they don't

02:08:03.800 --> 02:08:07.180
understand all the words so the pipeline which I was trying

02:08:07.180 --> 02:08:11.500
to use or even you can try to go here the best way is if you

02:08:11.500 --> 02:08:16.000
if you have to like go for the latest one then hugging face

02:08:16.000 --> 02:08:19.280
you can try to go here and then you can try to search for

02:08:19.280 --> 02:08:24.520
NER based model so model wise you can try to call it inside

02:08:24.520 --> 02:08:31.620
the pipeline so here you can try to search for NER yeah

02:08:52.200 --> 02:08:57.100
so multilingual NER or BERT based NER as you can see so

02:08:57.100 --> 02:08:59.600
there are a lot of like a model which has been trained on a

02:08:59.600 --> 02:09:02.260
huge amount of data you will be able to find out simple

02:09:02.260 --> 02:09:05.780
inside a pipeline call NER and then like this model name

02:09:05.780 --> 02:09:09.800
automatically you will be able to load it and this will be

02:09:10.420 --> 02:09:14.360
this will be able to give you a better result even for like

02:09:14.360 --> 02:09:17.460
some of the unknown data set or maybe some of the new data

02:09:17.460 --> 02:09:20.980
set in terms of NER but yeah the meaning of NER is very

02:09:20.980 --> 02:09:27.420
simple it identify what is what that's it fine guys yeah

02:09:28.920 --> 02:09:29.920
everyone

02:09:33.850 --> 02:09:39.450
Okay! So even NER is done which I have missed I'd somehow so

02:09:39.450 --> 02:09:41.770
I was not able to show you so now I'm able to show you even

02:09:41.770 --> 02:09:45.750
that part so this topic is complete guys. now in my next

02:09:45.750 --> 02:09:47.810
class so like i said i'm going to start from the neural

02:09:47.810 --> 02:09:50.970
network and then this entire architecture so from next class

02:09:50.970 --> 02:09:54.430
onwards i think one or two class is going to be pure

02:09:54.430 --> 02:09:58.530
theoretical but with a amazing explanation of an

02:09:58.530 --> 02:10:01.890
architecture and then again we'll come back to the practical

02:10:01.890 --> 02:10:06.450
one we'll start writing the code yeah so with that thank you

02:10:06.450 --> 02:10:09.270
so much everyone hope all of you have enjoyed and all of you

02:10:09.270 --> 02:10:14.110
have learned something so if you have any kind of a doubt so

02:10:14.110 --> 02:10:16.970
you have a group you can try to ping me inside a group and

02:10:16.970 --> 02:10:20.210
obviously we all are looking at your messages so we'll try

02:10:20.210 --> 02:10:23.930
to answer we'll try to solve your doubts and yeah till then

02:10:23.930 --> 02:10:26.970
thank you so much everyone and see you again in my next

02:10:26.970 --> 02:10:30.430
class which is going to happen on saturday 2 30 p.m ist

02:10:30.430 --> 02:10:34.830
thank you so much everyone please allow us to change our

02:10:34.830 --> 02:10:38.530
subscription to a gmail address from other emails okay you

02:10:38.530 --> 02:10:42.210
can drop a mail to my team right you can drop a mail maybe

02:10:42.210 --> 02:10:45.670
you can try to cc me you put my email id just to fast track

02:10:45.670 --> 02:10:49.630
the process and just mention that this email id is having an

02:10:49.630 --> 02:10:53.210
access the previous one and then you can try to send the

02:10:53.210 --> 02:10:56.310
next one my team will check remove the access of the

02:10:56.310 --> 02:10:59.510
previous id and then give you the access of the new id yeah

02:10:59.510 --> 02:11:04.610
fine okay fine guys with that thank you so much everyone see

02:11:04.610 --> 02:11:07.490
you again next class thank you so much okay

02:11:11.140 --> 02:11:14.340
sorry i forgot to share the code file so let me share a code

02:11:14.340 --> 02:11:19.520
file as well so code file wise let

02:11:21.090 --> 02:11:27.910
me share now itself so that otherwise i'll forget gen ai and

02:11:27.910 --> 02:11:31.490
then compress to a gif file fine

02:11:41.830 --> 02:11:45.410
guys so now code file is available inside your resource

02:11:45.410 --> 02:11:48.450
section and even later stage you all will be able to access

02:11:48.450 --> 02:11:52.710
it yeah with that thank you so much everyone take care see

02:11:52.710 --> 02:11:54.910
you again in my next class saturday thank you so much

