WEBVTT

00:03:26.220 --> 00:03:29.840
Okay, so hi everyone, good morning and I believe I am

00:03:29.840 --> 00:03:32.900
audible and visible to all of you. Yeah, so please confirm

00:03:32.900 --> 00:03:34.240
if I am audible and visible.

00:03:40.220 --> 00:03:43.860
Okay, so good morning. Yeah, I think chat is open for all.

00:03:44.000 --> 00:03:47.200
You can chat even in between and directly to me.

00:03:58.580 --> 00:04:05.960
Okay, so last week we had a holiday and I believe you all

00:04:05.960 --> 00:04:10.120
have enjoyed your festival. And this week again we are going

00:04:10.120 --> 00:04:12.480
to start with the class. Class again from the same place

00:04:12.480 --> 00:04:15.480
where we left. So anyone who can tell me like what we have

00:04:15.480 --> 00:04:18.860
discussed in a previous class, not last week, last to last

00:04:18.860 --> 00:04:19.840
week, what we have discussed.

00:04:23.660 --> 00:04:24.100
Yeah.

00:04:28.080 --> 00:04:31.020
So autogen fine tuning. Yeah, autogen I believe I have

00:04:31.020 --> 00:04:34.140
discussed on Saturday and then on Sunday, I believe I was

00:04:34.140 --> 00:04:37.680
talking about different kind of fine tuning, right? So we

00:04:37.680 --> 00:04:41.580
have prepared a data set apart from preparing a data set. So

00:04:41.580 --> 00:04:47.200
we have even like tested a model. I believe so and we have

00:04:47.200 --> 00:04:49.340
seen that how we can try to do a different different kind of

00:04:49.340 --> 00:04:52.560
fine tuning like a full fine tuning Laura based fine tuning,

00:04:52.600 --> 00:04:55.200
quora based fine tuning or quantized fine tuning. So I

00:04:55.200 --> 00:04:59.680
believe this is something that we have discussed. Yeah.

00:05:00.420 --> 00:05:04.620
Okay, so we'll try to start from the same place and today

00:05:04.620 --> 00:05:07.720
we'll try to focus on the same project. I believe a code

00:05:07.720 --> 00:05:09.800
file is already available inside your dashboard. So if you

00:05:09.800 --> 00:05:12.740
will go to your last previous class, you will be able to

00:05:12.740 --> 00:05:15.020
find out a code file over there. Which I have already

00:05:15.020 --> 00:05:18.960
shared. Yes, a model is not available in that code file

00:05:18.960 --> 00:05:23.300
reason is very, very simple. So if I'll show you my system,

00:05:23.520 --> 00:05:27.140
so where I have trained a model, some model size itself is

00:05:27.140 --> 00:05:33.500
around 24 or 25 GB, which I remember. So yeah, 25, 24 GB is

00:05:33.500 --> 00:05:36.920
a huge like a data. So I have not shared a model file, but

00:05:36.920 --> 00:05:40.400
yes, all the other file is already available and you can try

00:05:40.400 --> 00:05:42.460
to run the training and then after running a training, so

00:05:42.460 --> 00:05:44.820
you will be able to do a inferencing. With the model.

00:05:50.620 --> 00:05:56.740
Yeah. Okay, so we'll see. Yeah, we'll see those things. So

00:05:56.740 --> 00:06:01.200
let me share my screen and let's start with that project. So

00:06:01.200 --> 00:06:04.240
here is my VS code fine

00:06:12.760 --> 00:06:16.920
tuning final. Okay. So, yeah, this is something that we were

00:06:16.920 --> 00:06:20.000
talking about and we have talked about a data set. So we

00:06:20.000 --> 00:06:23.100
have prepared a different different domain wise data set and

00:06:23.100 --> 00:06:26.600
we have even learned that that what kind of a data set. We

00:06:26.600 --> 00:06:29.920
have to prepare in case of a DPO or full fine tuning Laura

00:06:29.920 --> 00:06:32.880
preferred or maybe a quantize Laura. So this is something

00:06:32.880 --> 00:06:35.780
that we have already like a discussed. So data set is

00:06:35.780 --> 00:06:38.380
already available inside your code base. If you'll go and

00:06:38.380 --> 00:06:41.740
check if you're going to download it and even these fine

00:06:41.740 --> 00:06:45.180
tuning files are also available. The only file which is not

00:06:45.180 --> 00:06:49.820
available is basically this models file because this model

00:06:49.820 --> 00:06:53.520
file size is around 20 GB. It's a very very huge whenever

00:06:53.520 --> 00:06:55.820
you are going to train the model and it's not just one

00:06:55.820 --> 00:06:57.800
single model. That we are training. So we are training like

00:06:57.800 --> 00:07:02.420
a 4 to 5 model altogether and that was a reason so that

00:07:02.420 --> 00:07:04.700
model file is not available. So you can just run the

00:07:04.700 --> 00:07:08.620
training. So to run the training again. So I have also given

00:07:08.620 --> 00:07:12.520
you a file for that. So train all model dot pi apart from

00:07:12.520 --> 00:07:15.000
that. So we have a separate separate model training file,

00:07:15.100 --> 00:07:20.080
which is available over here. So deep for DPO. This one is

00:07:20.080 --> 00:07:22.340
the file for full fine tuning. This one is the file for

00:07:22.340 --> 00:07:25.320
Laura preferred and Q Laura. These are the separate files.

00:07:25.360 --> 00:07:28.060
So I'm just calling all. Of this file altogether in one

00:07:28.060 --> 00:07:30.540
single file, which which we have already discussed. So I

00:07:30.540 --> 00:07:33.500
don't think that we have to go through this discussion once

00:07:33.500 --> 00:07:37.460
again. Now we are just trying to run all those pi file

00:07:37.460 --> 00:07:40.900
altogether. That's it. So 1 2 3 4 5. So altogether we are

00:07:40.900 --> 00:07:42.840
going to run all of this file and this is going to take

00:07:42.840 --> 00:07:45.460
time. So if you are trying to do a training in your local

00:07:45.460 --> 00:07:49.960
system, maybe it is going to take around depends if you have

00:07:49.960 --> 00:07:55.040
a i7 processor without a GPU. So in that case it may take

00:07:55.040 --> 00:07:59.200
like a 2 hour of time for you to do a training in your local

00:07:59.200 --> 00:08:02.980
system. And if you are going to train this things on a GPU.

00:08:03.180 --> 00:08:05.460
So again, it is it is going to take less time. So depends

00:08:05.460 --> 00:08:09.420
upon the compute that we have by the way, and we are not

00:08:09.420 --> 00:08:13.400
training for a very big number of epochs. So just for one or

00:08:13.400 --> 00:08:17.600
two epoch only. This is going to take that much of time in

00:08:17.600 --> 00:08:21.160
your local system. If you are having i7 kind of a processor

00:08:21.160 --> 00:08:25.040
in case of i5, maybe like a depends again. It will go for

00:08:25.040 --> 00:08:29.460
maybe a 5 hour 6 hours. To train like these models and this

00:08:29.460 --> 00:08:32.780
file this zip file is already available. So you can like go

00:08:32.780 --> 00:08:35.580
and look into that zip file, which is already available

00:08:35.580 --> 00:08:38.600
inside your previous section or previous class resource

00:08:38.600 --> 00:08:42.460
section. By the way, now what we are going to do today. So

00:08:42.460 --> 00:08:46.940
today we will try to focus on basically serving all of these

00:08:46.940 --> 00:08:51.220
models. So technically if we are able to build a model, so

00:08:51.220 --> 00:08:53.560
as of now, let's suppose we are building. We have built a

00:08:53.560 --> 00:08:57.100
model in a local system. So after building. A model, so we

00:08:57.100 --> 00:09:00.400
have to do a inferencing. So we have to do and we have also

00:09:00.400 --> 00:09:04.100
tested those model, right? So we were like able to send our

00:09:04.100 --> 00:09:06.780
data and then we were able to get a response, but that's

00:09:06.780 --> 00:09:09.860
okay. So we have to do a inferencing by the way inferencing

00:09:09.860 --> 00:09:13.180
means so maybe through a API. So we have to expose all of

00:09:13.180 --> 00:09:16.840
those model to the outer world, right? So first of all in a

00:09:16.840 --> 00:09:20.320
local system and then same thing should work even on a cloud

00:09:20.320 --> 00:09:23.580
platform. So on any cloud platform where we are going to

00:09:23.580 --> 00:09:27.220
deploy and this is where this API server. What pi file comes

00:09:27.220 --> 00:09:31.440
into a picture. So here summary wise if I'll tell you so we

00:09:31.440 --> 00:09:34.220
are not doing anything like a much over here. So summary

00:09:34.220 --> 00:09:37.420
wise, so we are just trying to expose all of our trained

00:09:37.420 --> 00:09:41.660
model to the outer world with the help of fast API and I

00:09:41.660 --> 00:09:44.540
believe we all are aware about a fast API. So this is

00:09:44.540 --> 00:09:47.140
something that I am going to discuss in today's class.

00:09:47.660 --> 00:09:52.160
Second thing is we have a docker file basically. So docker

00:09:52.160 --> 00:09:54.940
is going to help me out in terms of doing a

00:09:54.940 --> 00:09:57.120
containerization. So I'll be. Talking about that

00:09:57.120 --> 00:10:00.340
containerization part as well that why we do a docker things

00:10:00.340 --> 00:10:05.300
basically so that I can build one single image and then on

00:10:05.300 --> 00:10:08.160
any cloud platform means literally any cloud platform. I

00:10:08.160 --> 00:10:11.540
will be able to deploy it. I will be able to host it and

00:10:11.540 --> 00:10:15.560
again. So if something is available over a cloud, it's

00:10:15.560 --> 00:10:19.040
always gives me a ease of use, right? So I will be able to

00:10:19.040 --> 00:10:22.880
consume it anywhere and similar kind of a system we are

00:10:22.880 --> 00:10:26.360
going to learn. So when we will get into a project. Because

00:10:26.360 --> 00:10:29.360
project is a part of your syllabus, right? So let's suppose

00:10:29.360 --> 00:10:32.180
if I'm talking about a URI like chat system, so obviously

00:10:32.180 --> 00:10:35.000
URI like chat system requires not just one single model, a

00:10:35.000 --> 00:10:38.200
multi model, right? Because inside URI, we have given you a

00:10:38.200 --> 00:10:42.200
multiple model selection by the way and around those models.

00:10:42.260 --> 00:10:45.200
So we have attached a multiple features, right? Maybe like

00:10:45.200 --> 00:10:47.840
for a file uploading feature we have attached, we have

00:10:47.840 --> 00:10:50.240
attached where you can try to give just a voice command and

00:10:50.240 --> 00:10:53.920
then you will be able to chat and many, many more things,

00:10:54.040 --> 00:10:56.640
right? But yeah. Everything is going to happen across our

00:10:56.640 --> 00:11:00.620
LLMs. Now so if I'll talk about a URI like system, so in

00:11:00.620 --> 00:11:05.240
that kind of a system, we have a proprietary model which is

00:11:05.240 --> 00:11:08.000
not an open one, right? For example, we have given you an

00:11:08.000 --> 00:11:11.320
access of chat GPT kind of a model. Those models are

00:11:11.320 --> 00:11:14.240
proprietary model, especially if I'll talk about a chat GPT

00:11:14.240 --> 00:11:17.640
4 and 5 series, right? I'm not talking about OSS like a

00:11:17.640 --> 00:11:22.520
model basically from chat GPT. So you have to go through an

00:11:22.520 --> 00:11:26.240
exact similar kind of approach. Means there won't be any

00:11:26.240 --> 00:11:28.320
kind of differences means there won't be any kind of

00:11:28.320 --> 00:11:31.680
differences. Plus majority of you when you are going to join

00:11:31.680 --> 00:11:35.640
an organization, again, if someone is going to ask you,

00:11:35.680 --> 00:11:38.840
which obviously people will, right? Because that is going to

00:11:38.840 --> 00:11:42.860
be your day to day task. So you have to train your model and

00:11:42.860 --> 00:11:45.340
at that point of time, you are not going to do anything

00:11:45.340 --> 00:11:48.300
different except this. So this one which we have discussed,

00:11:48.320 --> 00:11:51.960
so you can use a same code base in terms of data. Obviously,

00:11:51.960 --> 00:11:55.200
you are going to pass your own data. But yes, in a same

00:11:55.200 --> 00:11:59.500
format, right? Same format means I was reading a data as a

00:11:59.500 --> 00:12:03.460
key value pair into my code file, right? So data preparation

00:12:03.460 --> 00:12:06.280
is going to be same. It is not going to change at all with

00:12:06.280 --> 00:12:09.300
respect to these kind of training that we have done. Apart

00:12:09.300 --> 00:12:12.860
from that, your fine tuning script is not going to be

00:12:12.860 --> 00:12:16.840
different. Parameter wise, obviously you all are going to

00:12:16.840 --> 00:12:19.520
play with the parameter, which we have already discussed, I

00:12:19.520 --> 00:12:22.420
believe in my previous class that whatever parameter,

00:12:22.480 --> 00:12:25.780
whatever number of epoch, logging step on optimizer,

00:12:25.820 --> 00:12:28.620
whatever we have taken. So obviously you have a flexibility.

00:12:28.960 --> 00:12:31.980
So where you can change it and then eventually you will be

00:12:31.980 --> 00:12:34.720
able to train the model. That is the only changes which you

00:12:34.720 --> 00:12:39.400
will have to do if you are training a model for some of the

00:12:39.400 --> 00:12:43.940
businesses. And again, so testing wise or like, you know,

00:12:43.940 --> 00:12:47.460
hosting wise. So it's a similar kind of approach, Docker or

00:12:47.460 --> 00:12:50.480
Kubernetes approach you are going to apply. This is

00:12:50.480 --> 00:12:54.340
something that we do. And then we try to. Leave a system on

00:12:54.340 --> 00:12:57.400
auto scale so that, you know, so whenever there will be more

00:12:57.400 --> 00:13:01.460
demand, so obviously system will try to, you know, spin some

00:13:01.460 --> 00:13:03.960
additional machine and then it is going to serve the,

00:13:04.060 --> 00:13:07.300
technically it is going to serve the request or serve the

00:13:07.300 --> 00:13:10.580
traffic. Similar thing we have done even in case of a URI.

00:13:10.760 --> 00:13:14.280
So some models is a proprietary model. Some model is a self

00:13:14.280 --> 00:13:17.260
-hosted model and yeah, so we are able to call both the

00:13:17.260 --> 00:13:19.900
models. So in case of proprietary model, so we don't have to

00:13:19.900 --> 00:13:22.480
worry much. There are like a lot of providers. So we can

00:13:22.480 --> 00:13:24.880
just try to, you know. Integrate that APIs and then it will

00:13:24.880 --> 00:13:28.660
be able to serve it. In case of like a open source model. So

00:13:28.660 --> 00:13:31.920
we download the wait file and then after downloading a wait

00:13:31.920 --> 00:13:34.800
file, so we try to host it in some of our own environments

00:13:34.800 --> 00:13:38.120
and again, so whenever you chat, so it is going to hit my

00:13:38.120 --> 00:13:40.260
environment. Technically, this is something which is going

00:13:40.260 --> 00:13:43.320
to happen and around that. So all of these features we can

00:13:43.320 --> 00:13:46.180
try to attach. As simple as that. So shall we start guys?

00:13:46.340 --> 00:13:51.960
Shall we start with the API? Yeah. That's a summary. I

00:13:51.960 --> 00:13:55.160
believe. We have like a revised event. Everything. So those

00:13:55.160 --> 00:13:59.220
who are not able to like a recall things, because I know

00:13:59.220 --> 00:14:02.580
it's like, it's been 14 days for us, right? Not 14, 12 days

00:14:02.580 --> 00:14:08.060
maybe. Yeah. Okay. So here you will be able to find out one

00:14:08.060 --> 00:14:10.840
file called as API underscore server dot pi file. Now why

00:14:10.840 --> 00:14:13.540
I'm using this one. So I'm just trying to like, you know,

00:14:13.540 --> 00:14:16.660
expose my entire model to the outer world. And again,

00:14:16.720 --> 00:14:20.460
approach is very same like a fast API approach. So where we

00:14:20.460 --> 00:14:23.800
all know that just try to give a route and then at that

00:14:23.800 --> 00:14:26.360
particular route, something will be available. Some function

00:14:26.360 --> 00:14:29.980
will be called at that particular route. As simple as that.

00:14:30.100 --> 00:14:33.920
Right. So here what we are trying to do, so we are trying to

00:14:33.920 --> 00:14:36.920
do all the imports and again, in terms of deployment, all

00:14:36.920 --> 00:14:40.320
the other files will not be required. Yeah. So you just need

00:14:40.320 --> 00:14:43.560
a requirement deployment file. You just need a model file.

00:14:43.720 --> 00:14:46.040
That's it. Data set is not required at the time of

00:14:46.040 --> 00:14:49.060
deployment. This fine tuning script is not required at the

00:14:49.060 --> 00:14:51.200
time of deployment. So at the time of deployment, you can

00:14:51.200 --> 00:14:54.420
like a remove these files and you have to just keep a model

00:14:54.420 --> 00:14:57.980
file and then this API server file. And then if I'm like a

00:14:57.980 --> 00:15:00.660
deploying it as a Docker. So in that case, you just need a

00:15:00.660 --> 00:15:03.640
Docker and requirement dot txt file. That's it. The rest of

00:15:03.640 --> 00:15:05.520
the file, you can remove it. You can delete it or maybe

00:15:05.520 --> 00:15:09.380
create a separate folder and leave it over there. Right.

00:15:09.520 --> 00:15:12.660
Depends. So here what we are trying to do. So we are

00:15:12.660 --> 00:15:15.760
basically trying to like call all the fast API related

00:15:15.760 --> 00:15:18.620
things and we are just trying to call a pytorch over here so

00:15:18.620 --> 00:15:22.980
that I will be able to do a like a fine exposure of the

00:15:22.980 --> 00:15:26.520
model. And here. So we are trying to create the app. So we

00:15:26.520 --> 00:15:29.800
are trying to create basically a fast API app over here. And

00:15:29.800 --> 00:15:33.780
we have just given a title description and a version simple

00:15:33.780 --> 00:15:37.340
and with the help of this object. So I will be exposing each

00:15:37.340 --> 00:15:40.920
and everything to the outer world. Now here we are trying to

00:15:40.920 --> 00:15:44.900
add a course which is not required for my local system, by

00:15:44.900 --> 00:15:47.240
the way. But yeah, this is a required. So when I'm going to

00:15:47.240 --> 00:15:50.660
host a model on some cloud platform so that it will be able

00:15:50.660 --> 00:15:54.780
to allow every kind of a traffic. To my server, basically.

00:15:55.420 --> 00:15:58.300
So this is like this is something which I'm trying to like

00:15:58.300 --> 00:16:01.800
add, which is an optional one only for a deployment. Now

00:16:01.800 --> 00:16:05.400
here. So we are trying to make it pidentic. So here we are

00:16:05.400 --> 00:16:08.940
trying to say that that there is an inference request and

00:16:08.940 --> 00:16:11.540
there is an inference response. So whenever someone is going

00:16:11.540 --> 00:16:15.300
to request it, so what kind of a data we they are supposed

00:16:15.300 --> 00:16:18.000
to send. So this is basically a data validation that we are

00:16:18.000 --> 00:16:20.080
trying to do. And this is where this pidentic class comes

00:16:20.080 --> 00:16:23.500
into a picture. So here we are saying that okay, so user is

00:16:23.500 --> 00:16:26.280
going to send a query user is going to say like what is the

00:16:26.280 --> 00:16:28.880
maximum token, what should be the temperature and what is

00:16:28.880 --> 00:16:33.520
the top choice it should yield basically. And when model is

00:16:33.520 --> 00:16:35.900
going to give a response, so it is going to follow this one.

00:16:35.960 --> 00:16:38.880
So model then query then response and then token generated.

00:16:39.000 --> 00:16:41.400
So this is something that model will give and this is

00:16:41.400 --> 00:16:43.920
something that model will take as a input. And we have made

00:16:43.920 --> 00:16:48.260
it as a pidentic apart from that. So what we are doing? We

00:16:48.260 --> 00:16:52.100
are trying to basically. Catch. Catch the model so that you

00:16:52.100 --> 00:16:55.980
know, every time so whenever user will chat, so it is not a

00:16:55.980 --> 00:16:59.680
load a model because size is going to be maybe a little bit

00:16:59.680 --> 00:17:02.380
big. So what I'm saying that that okay, fine. So for now,

00:17:02.480 --> 00:17:07.820
just try to load a model once and then keep on hitting that

00:17:07.820 --> 00:17:12.680
particular model. But ideally it is not a good approach. I

00:17:12.680 --> 00:17:15.080
have done that so that you know, I can make my life a little

00:17:15.080 --> 00:17:18.800
bit easier. But this is not a good approach because you are

00:17:18.800 --> 00:17:23.560
trying to. Give a load on a client side in general. So we

00:17:23.560 --> 00:17:26.560
are we generally do a server side catching, not a client

00:17:26.560 --> 00:17:29.720
side catching. But yeah, this is not an ideal approach. But

00:17:29.720 --> 00:17:32.140
still we have implemented. This is one of the approach you

00:17:32.140 --> 00:17:35.940
can say for maybe like doing a inferencing in a fastest

00:17:35.940 --> 00:17:39.160
possible manner so that only once just load the model and

00:17:39.160 --> 00:17:43.260
then eventually keep on doing a inferencing with that model

00:17:43.260 --> 00:17:46.920
so that user will feel a lag only for a first time. Second

00:17:46.920 --> 00:17:50.720
time user will not feel a kind of a lag. Basically. So this

00:17:50.720 --> 00:17:54.000
is just one variable we have created. But in general, when

00:17:54.000 --> 00:17:56.520
we will try to bring something on a production, so we will

00:17:56.520 --> 00:17:59.100
try to do a server side catching, maybe we can try to use a

00:17:59.100 --> 00:18:03.180
Redis as a service over there. And then in that case, like

00:18:03.180 --> 00:18:06.320
nothing is going to happen on a client side means user side.

00:18:06.480 --> 00:18:10.480
So that user will not feel because if I'm trying to, you

00:18:10.480 --> 00:18:15.620
know, download a five GB kind of a model on a user side. So

00:18:15.620 --> 00:18:20.480
again, if user system is not that strong. They will they

00:18:20.480 --> 00:18:24.780
will feel a lot of like a lag over there. So this is one of

00:18:24.780 --> 00:18:27.340
the problem that we are going to face in this kind of

00:18:27.340 --> 00:18:30.700
approach now. So base model. So this is the model that we

00:18:30.700 --> 00:18:32.660
have trained. So by default, this model will be available.

00:18:33.140 --> 00:18:35.840
And then here. So what we are trying to do. So we are trying

00:18:35.840 --> 00:18:40.100
to like create just a configuration means we are just trying

00:18:40.100 --> 00:18:43.340
to like say that that where my model is available. So my HR

00:18:43.340 --> 00:18:46.540
model is available at this spot. So wherever I'm keeping

00:18:46.540 --> 00:18:48.620
this model. So as of now, I'm keeping all of this model

00:18:48.620 --> 00:18:51.860
inside. My model directory. So inside model. So there is a

00:18:51.860 --> 00:18:55.340
HR model, there is a healthcare model, marketing model or

00:18:55.340 --> 00:18:58.580
let's suppose we have a sales model. So yeah, so we are just

00:18:58.580 --> 00:19:02.060
trying to like give a path and then what is the type of the

00:19:02.060 --> 00:19:05.140
model and little bit of description just for my

00:19:05.140 --> 00:19:08.860
understanding purposes. So this is just like a key value,

00:19:08.900 --> 00:19:11.920
key value, key value pair we have created for HR for finance

00:19:11.920 --> 00:19:16.020
for sales for a healthcare and same let's suppose for

00:19:16.020 --> 00:19:19.160
marketing. So let's suppose. If I have a model in some

00:19:19.160 --> 00:19:22.500
different places in that case, just change the path. Nothing

00:19:22.500 --> 00:19:25.280
much, right? So wherever your model file is available, wait

00:19:25.280 --> 00:19:28.260
files are available. Just try to change because this is the

00:19:28.260 --> 00:19:32.620
only area of interest for me. I need just a path wherever my

00:19:32.620 --> 00:19:36.460
model is stored. That's it. So this is our model

00:19:36.460 --> 00:19:39.400
configuration. Now what I will do. So I'll just try to like,

00:19:39.420 --> 00:19:43.560
you know, uh, like, uh, we'll call a load model. So whenever

00:19:43.560 --> 00:19:46.600
I'm trying to load a model. So first of all, it will try to

00:19:46.600 --> 00:19:49.720
check in. To a catch. So this is the catch variable which we

00:19:49.720 --> 00:19:52.800
have created, right? So it will try to look into the catch.

00:19:52.980 --> 00:19:56.360
If model is available, the model which I'm looking for,

00:19:56.420 --> 00:19:59.180
right? If I'm going to pass a model name and if model is

00:19:59.180 --> 00:20:02.400
available inside a cache, so obviously it is supposed to

00:20:02.400 --> 00:20:07.860
return a model from a cache itself. If not right, if not,

00:20:07.940 --> 00:20:12.160
then it is supposed to go for a model config by the way. So

00:20:12.160 --> 00:20:15.880
this one, right? Model configuration with a model name. And

00:20:15.880 --> 00:20:20.120
then. Then it should give me basically a model path and a

00:20:20.120 --> 00:20:23.500
model type basically. So what is the path of the model so

00:20:23.500 --> 00:20:27.500
that I will be able to, uh, like, uh, discover the model and

00:20:27.500 --> 00:20:32.040
what is the type of the model? If not, then this 404, so

00:20:32.040 --> 00:20:35.500
basically model not found, right? So if, uh, that model is

00:20:35.500 --> 00:20:39.040
not available, so in that case, just give me 404 over here

00:20:39.040 --> 00:20:42.840
now. So again, we are trying to do a check on a model path

00:20:42.840 --> 00:20:46.060
as you can see. So if model path is not available. So again,

00:20:46.120 --> 00:20:48.300
give me. Give me 404. So this is just an error handling that

00:20:48.300 --> 00:20:51.460
we have done that if model is not available, then what

00:20:51.460 --> 00:20:54.640
should do if model path is not available, then what you are

00:20:54.640 --> 00:20:59.380
supposed to do over there as simple as that. Now once this

00:20:59.380 --> 00:21:04.000
is done, right? Once this entire thing is done, then what we

00:21:04.000 --> 00:21:07.580
are trying to do. So obviously as a user, you are going to

00:21:07.580 --> 00:21:11.520
pass an input and you are going to pass an input into a text

00:21:11.520 --> 00:21:15.060
basically in an NLP manner, right? You are going to pass a

00:21:15.060 --> 00:21:18.180
data. You are going to pass an input. So obviously we have

00:21:18.180 --> 00:21:21.840
to tokenize those data because at the end of the day, system

00:21:21.840 --> 00:21:24.340
is not going to understand your English language or any

00:21:24.340 --> 00:21:27.180
other languages. System is going to understand just a token,

00:21:27.300 --> 00:21:31.060
a numbers, right? So we are supposed to use a same tokenizer

00:21:31.060 --> 00:21:35.000
that we have used at the time of training of the model. It

00:21:35.000 --> 00:21:37.840
should not be a different tokenizer and I believe this

00:21:37.840 --> 00:21:41.500
understanding is clear to all of us. We have like a seen

00:21:41.500 --> 00:21:44.960
these kinds of a scenario, not just one or two times. But a

00:21:44.960 --> 00:21:47.680
multiple times, right? So we are trying to call a same

00:21:47.680 --> 00:21:52.940
tokenizer from the model path and basically we are going to

00:21:52.940 --> 00:21:56.840
tokenize our data set. So whatever data which like someone

00:21:56.840 --> 00:22:00.120
is going to. So this is just a tokenizer object which we are

00:22:00.120 --> 00:22:03.620
trying to load so that I can use it at the time of, you

00:22:03.620 --> 00:22:06.240
know, sending or doing an inference with respect to the

00:22:06.240 --> 00:22:11.160
model. Now so load model based on the type. So here, which

00:22:11.160 --> 00:22:14.660
model I would like to call? LoRa prefect, QLoRa or maybe a

00:22:14.660 --> 00:22:20.480
DPO. So here it is going to load a model for me. So base

00:22:20.480 --> 00:22:24.440
model name and then device map and task type. It's a float

00:22:24.440 --> 00:22:27.980
16 bit model for a preferred model. We are going to load it.

00:22:28.080 --> 00:22:32.480
Now once model is, so this is for a base model just for the

00:22:32.480 --> 00:22:35.780
base model, right. And otherwise, so from a pre-trained

00:22:35.780 --> 00:22:39.640
model, we are going to like load a model over here. This is

00:22:39.640 --> 00:22:43.040
just a model checks or model load. We are trying to. like a

00:22:43.040 --> 00:22:47.260
load for full fine tune model so this is approach means this

00:22:47.260 --> 00:22:50.780
function is going to be same it's not like a very different

00:22:50.780 --> 00:22:54.080
we are just trying to like a call a model one by one one by

00:22:54.080 --> 00:22:58.760
one one by one that's it now so here again model dot cache

00:22:58.760 --> 00:23:02.500
so tokenizer config everything is fine so technically it is

00:23:02.500 --> 00:23:06.620
going to return me a model as simple as that it is going to

00:23:06.620 --> 00:23:10.360
return me the model now so this is one of the function just

00:23:10.360 --> 00:23:13.360
to load the model now another function we have over here so

00:23:13.360 --> 00:23:16.380
which is going to generate the response once it will be able

00:23:16.380 --> 00:23:20.280
to load the model. So now to generate our like a response we

00:23:20.280 --> 00:23:23.560
need a model name we have to pass the query right we have to

00:23:23.560 --> 00:23:26.440
even define what is the maximum token it should like a yield

00:23:26.440 --> 00:23:31.640
and temperature and then a top responses right so here load

00:23:31.640 --> 00:23:34.520
model wise so we are going to call a load model function so

00:23:34.520 --> 00:23:39.020
that model will be loaded and then once model will be loaded

00:23:39.020 --> 00:23:44.900
over here we are. Going to basically tokenize the data so we

00:23:44.900 --> 00:23:48.120
are going to call the tokenizer over here and then we are

00:23:48.120 --> 00:23:51.220
going to prepare a prompt so we are trying to pass a prompt

00:23:51.220 --> 00:23:56.300
inside a tokenizer and it is going to return me a input now

00:23:56.300 --> 00:24:00.120
here so we have basically like a approach where we are

00:24:00.120 --> 00:24:03.300
trying to check whether we have a CUDA available or not

00:24:03.300 --> 00:24:07.500
means GPU available or not for a fast inferencing so if GPU

00:24:07.500 --> 00:24:11.180
is available so in that case like this is going to be true

00:24:11.180 --> 00:24:14.280
and if that is going to be true so in that case it is going

00:24:14.280 --> 00:24:19.080
to basically like take this input the input that we have a

00:24:19.080 --> 00:24:21.780
key value pair kind of an input that we have and then it is

00:24:21.780 --> 00:24:26.680
going to basically prepare the final input now if we don't

00:24:26.680 --> 00:24:31.880
have a CUDA available so in that case basically it again so

00:24:31.880 --> 00:24:35.780
whether we have a CUDA available or not basically like a it

00:24:35.780 --> 00:24:38.760
will try to come to the next section and it will be able to

00:24:38.760 --> 00:24:41.780
do inferencing even with respect to them. Okay. CPU so here

00:24:41.780 --> 00:24:44.640
we are trying to call model.generate we are trying to pass a

00:24:44.640 --> 00:24:48.440
input inside the model maximum token temperature so all this

00:24:48.440 --> 00:24:51.020
parameter parameter that we have passed over here inside

00:24:51.020 --> 00:24:53.720
this function so we are just trying to call it we are just

00:24:53.720 --> 00:24:57.260
trying to call it and this is going to yield me an output so

00:24:57.260 --> 00:25:00.040
once I will be having an output I will just try to do a

00:25:00.040 --> 00:25:03.620
reverse tokenization so where I will be able to convert my

00:25:03.620 --> 00:25:08.420
output into a English by the way and yeah so I am going to

00:25:08.420 --> 00:25:12.660
showcase that particular response. To my user so this

00:25:12.660 --> 00:25:15.220
function is going to run two things one is a response

00:25:15.220 --> 00:25:18.560
English response and then a token generated it is going to

00:25:18.560 --> 00:25:22.760
create so this is the two major function that we have

00:25:22.760 --> 00:25:27.500
basically created only two major function now once this

00:25:27.500 --> 00:25:31.340
function is ready I have to expose it I have to expose it to

00:25:31.340 --> 00:25:35.200
the outer world so here what we are trying to do so we are

00:25:35.200 --> 00:25:38.420
trying to call our app and at a route location at a home

00:25:38.420 --> 00:25:42.400
location so we are like a executing this function so where

00:25:42.400 --> 00:25:45.940
you will be able to see just a information yeah so if you

00:25:45.940 --> 00:25:49.100
are going to hit the route you will be able to see just this

00:25:49.100 --> 00:25:53.440
information so whatever information that we have mentioned

00:25:53.440 --> 00:25:56.740
over here so these are the end points which is available

00:25:56.740 --> 00:25:59.800
plus if you will go and check inside the docs so you will be

00:25:59.800 --> 00:26:04.700
able to see these paths and on this path you will be able to

00:26:04.700 --> 00:26:06.680
access a different different different different kind of a

00:26:06.680 --> 00:26:11.740
models. So this is just for the home directory. Now so if

00:26:11.740 --> 00:26:14.580
you are going to hit this particular path API slash model

00:26:14.580 --> 00:26:18.240
then you will be able to list down all the models simple

00:26:18.240 --> 00:26:21.060
this function will be come as a return I will show it to you

00:26:21.060 --> 00:26:24.700
that that part I will show you so just wait for some time if

00:26:24.700 --> 00:26:28.520
you are going to hit this path health path so you will be

00:26:28.520 --> 00:26:33.100
able to get a health of the model and if GPU is available

00:26:33.100 --> 00:26:36.880
model is loaded or not all of these things so whatever I

00:26:36.880 --> 00:26:39.160
have mentioned over here so you will be able to see that. If

00:26:39.160 --> 00:26:42.400
you are going to hit this particular path means you will end

00:26:42.400 --> 00:26:46.300
up hitting my HR model basically so you will go and do a HR

00:26:46.300 --> 00:26:49.800
inferencing. So it will call generate response and generate

00:26:49.800 --> 00:26:54.000
response is calling a load model by default yeah so you are

00:26:54.000 --> 00:26:57.640
supposed to provide these like inputs query maximum token

00:26:57.640 --> 00:27:00.220
temperature and top p and then it is going to give you the

00:27:00.220 --> 00:27:04.640
final responses as simple as that and it is going to give

00:27:04.640 --> 00:27:08.360
you the final response. If someone is going to hit my API

00:27:08.360 --> 00:27:12.700
slash model. So in that case finance model will be called so

00:27:12.700 --> 00:27:15.640
template is exactly same there is no differences in a

00:27:15.640 --> 00:27:19.540
templates you will be able to find out now so if someone is

00:27:19.540 --> 00:27:23.120
going to hit API slash sales so in that case sales model

00:27:23.120 --> 00:27:26.700
will be invoked it is the same thing over here if someone is

00:27:26.700 --> 00:27:30.180
going to like a hit this path API slash healthcare so in

00:27:30.180 --> 00:27:33.260
that case healthcare model will be invoked if someone is

00:27:33.260 --> 00:27:35.760
going to hit my API slash marketing so in that case

00:27:35.760 --> 00:27:41.200
marketing model will be invoked. And then again so here API

00:27:41.200 --> 00:27:44.500
dot infer model name so in that case generic inferences will

00:27:44.500 --> 00:27:51.080
be invoked over here as simple as that. So here whenever we

00:27:51.080 --> 00:27:53.660
are going to start so on startup what we are trying to do so

00:27:53.660 --> 00:27:56.740
we are trying to preload the model so this is just another

00:27:56.740 --> 00:27:59.940
additional function I was testing for my testing purpose so

00:27:59.940 --> 00:28:02.240
I have created these functions although it was not like

00:28:02.240 --> 00:28:06.080
important by the way but yeah so default model we are trying

00:28:06.080 --> 00:28:10.380
to load. Just another function over here and then like just

00:28:10.380 --> 00:28:14.260
run it yeah so just run it and after running it you will be

00:28:14.260 --> 00:28:19.360
able to see the inferencing. So hope this makes sense to

00:28:19.360 --> 00:28:24.120
most of us yes everyone I will take a doubt so don't worry I

00:28:24.120 --> 00:28:28.060
will do that but before that let me run these things and

00:28:28.060 --> 00:28:31.580
then show you the output like how we can do a testing with

00:28:31.580 --> 00:28:35.000
the help of APIs let me show it to you that particular part.

00:28:35.000 --> 00:28:40.060
Now I have also prepared I think this file was not prepared

00:28:40.060 --> 00:28:44.500
in my previous upload so I have just prepared yesterday

00:28:44.500 --> 00:28:49.120
night where one file and you will be able to see a summary

00:28:49.120 --> 00:28:52.340
readme file was available but yeah still I have just

00:28:52.340 --> 00:28:56.240
modified a lot of things inside my readme file right. So

00:28:56.240 --> 00:29:00.640
here basically if I have to like run this entire things even

00:29:00.640 --> 00:29:03.180
maybe after an year right so that you don't have to

00:29:03.180 --> 00:29:06.620
struggle. Keeping that in a mind I have prepared a readme

00:29:06.620 --> 00:29:09.880
file so that in a easiest possible manner you all will be

00:29:09.880 --> 00:29:14.100
able to do a training. So first of all you have to like do a

00:29:14.100 --> 00:29:18.440
requirement.txt installation. Second this is for API so I

00:29:18.440 --> 00:29:22.000
have created two requirement.txt one at the time of training

00:29:22.000 --> 00:29:25.360
and one for a inferencing training so we need little bit

00:29:25.360 --> 00:29:30.160
more files we need little bit more like a installation for

00:29:30.160 --> 00:29:33.420
testing if we are just like a doing inferencing. So we don't

00:29:33.420 --> 00:29:37.680
need much we just need like a fast API over here pidentic

00:29:37.680 --> 00:29:42.580
and related libraries that's it. So if I have I'm just doing

00:29:42.580 --> 00:29:45.120
a testing over here. So this is something which I have

00:29:45.120 --> 00:29:47.760
mentioned then if you have to call all the training so you

00:29:47.760 --> 00:29:51.720
can try to basically call this train all model.py files

00:29:51.720 --> 00:29:55.940
automatically it is calling my fine tuning file. So I'm

00:29:55.940 --> 00:29:59.400
doing all this training all together if I just wanted to do

00:29:59.400 --> 00:30:03.480
a DPO training so in that case. Simple call python go inside

00:30:03.480 --> 00:30:07.380
this directory or give a fully qualified path and then just

00:30:07.380 --> 00:30:12.120
try to call python dpo fine tuning.py it will execute simple

00:30:12.120 --> 00:30:15.820
very very simple. So for any given fine tuning if you would

00:30:15.820 --> 00:30:19.040
like to call all the fine tuning together then this is the

00:30:19.040 --> 00:30:22.840
command and I think this is just a pythonic command right. I

00:30:22.840 --> 00:30:26.520
have also created a bat file so where technically it is

00:30:26.520 --> 00:30:31.220
calling this python file itself but it's a window compatible

00:30:31.220 --> 00:30:34.660
file so without even running a python code just double click

00:30:34.660 --> 00:30:38.480
it will work. Those who are working in a Linux or maybe into

00:30:38.480 --> 00:30:41.160
a Mac machine so you can just try to call this sh file

00:30:41.160 --> 00:30:43.840
right. So script file you can try to call and then it is

00:30:43.840 --> 00:30:46.960
going to start the training. Technically it is trying to

00:30:46.960 --> 00:30:50.080
call this one as simple as that if you will go and check my

00:30:50.080 --> 00:30:53.660
file that I have created so here it is not doing anything

00:30:53.660 --> 00:30:56.820
different so starting all the model training and one by one

00:30:56.820 --> 00:30:59.840
it is trying to call all the training it's the same thing it

00:30:59.840 --> 00:31:03.040
is trying to do right it's just a clickable file I have

00:31:03.040 --> 00:31:06.180
created a bat file or you can say sh file for a Linux kind

00:31:06.180 --> 00:31:09.520
of a system. So individual model training I have also given

00:31:09.520 --> 00:31:13.740
you this one command but yeah it's just a python command so

00:31:13.740 --> 00:31:19.020
python and then call that py file. Now here basically so if

00:31:19.020 --> 00:31:21.960
you would like to do a training into h100 kind of a GPU

00:31:21.960 --> 00:31:25.460
environment right GPU kind of environment so in that case

00:31:25.460 --> 00:31:29.860
just try to call this sh file and it is going to basically

00:31:29.860 --> 00:31:33.300
do the training. So once training will be done means once

00:31:33.300 --> 00:31:36.580
your model will be available here so once this models are

00:31:36.580 --> 00:31:40.000
available over here then we have to do a inferencing. To do

00:31:40.000 --> 00:31:42.880
a inferencing simple api server dot py file which I was

00:31:42.880 --> 00:31:47.060
talking about just now so call that particular py file and

00:31:47.060 --> 00:31:50.820
at this location you will be able to see all of your model

00:31:50.820 --> 00:31:53.560
plus end point that I have created. This is called as end

00:31:53.560 --> 00:31:56.780
point right this is how you were even able to access your

00:31:56.780 --> 00:32:00.520
URI right. Right. This is the end point. So these are the

00:32:00.520 --> 00:32:03.200
end point which I have created for finance sales marketing

00:32:03.200 --> 00:32:07.480
healthcare hr model and to just get a health of the model.

00:32:07.780 --> 00:32:12.340
So you can just try to hit the end points after running this

00:32:12.340 --> 00:32:14.900
particular file. So let me run this file as we are here

00:32:14.900 --> 00:32:20.780
right and let's see. So python api server dot py file yeah

00:32:37.550 --> 00:32:40.310
so as you can see my server has already started and my

00:32:40.310 --> 00:32:43.410
server is up and running at 8000 port number now let me go

00:32:43.410 --> 00:32:47.790
to my 8000 port number and let's hit it okay.

00:32:54.270 --> 00:32:57.810
So here I am able to see something right credit print and

00:32:57.810 --> 00:33:03.470
yeah so this is a base message which I have set which I am

00:33:03.470 --> 00:33:09.270
able to see over here. So now this is coming from my route

00:33:10.740 --> 00:33:14.660
by the way yeah so at my route these

00:33:17.800 --> 00:33:21.000
are the messages which I am trying to see right which I am

00:33:21.000 --> 00:33:24.100
trying to showcase by the way. So yeah all of these messages

00:33:24.100 --> 00:33:30.320
is visible. At my route yeah and GPU so CPU only so in a CPU

00:33:30.320 --> 00:33:33.960
only mode it is like running my entire model it is not able

00:33:33.960 --> 00:33:39.360
to detect a CPU inside my system for now basically right and

00:33:39.360 --> 00:33:42.300
these are the end point. So whatever whatever I was like

00:33:42.300 --> 00:33:44.780
trying to print so all of these things are available over

00:33:44.780 --> 00:33:51.000
here. Now if I have to go inside docs because fast api

00:33:51.000 --> 00:33:54.820
provides you that so I can try to go to this route. Let me

00:33:54.820 --> 00:33:58.680
go see this right so that I will be able to see a swagger UI

00:33:58.680 --> 00:34:04.600
over here. So here is my swagger UI so where all of these

00:34:04.600 --> 00:34:08.400
routes are available and one by one one by one so I can try

00:34:08.400 --> 00:34:12.440
to even test these models right from my swagger UI itself.

00:34:12.780 --> 00:34:16.580
So for example if I have to test a HR model so maybe I can

00:34:16.580 --> 00:34:20.760
try to select this HR. I can click on try this out and query

00:34:20.760 --> 00:34:26.200
wise I can try to provide a query that what is. Leave policy

00:34:26.200 --> 00:34:28.460
leave policy

00:34:31.300 --> 00:34:36.460
and maybe I can send this like a data to my model and let's

00:34:36.460 --> 00:34:41.920
see so what is the response I will be able to get okay

00:34:55.270 --> 00:35:02.810
so it is telling me JSON invalid. I think I have missed this

00:35:02.810 --> 00:35:09.090
double quotes here while sending a string execute once

00:35:09.090 --> 00:35:14.070
again. Yeah. Now it's like a loading my model and even I can

00:35:14.070 --> 00:35:16.030
come back and check over here.

00:35:24.580 --> 00:35:27.700
Okay so yeah what is the leave policies that was my query

00:35:27.700 --> 00:35:31.440
right to HR model by the way and these are the responses

00:35:31.440 --> 00:35:36.240
based on the training I'm able to get. So this path is

00:35:36.240 --> 00:35:40.460
working fine for me. Simple right. So anyone and everyone

00:35:40.460 --> 00:35:44.020
can call a HR model path is up and running it is able to

00:35:44.020 --> 00:35:47.640
like I'm able to test it basically. Similarly you can try

00:35:47.640 --> 00:35:49.900
to. Test your finance model you can try to test your sales

00:35:49.900 --> 00:35:54.600
model or any other model at the end of the day what I need I

00:35:54.600 --> 00:35:59.820
just need a path simple. So I just need this end point

00:35:59.820 --> 00:36:05.260
right. So as of now this is my local end point and it is

00:36:05.260 --> 00:36:09.520
working. So if I'm going to host it over a cloud platform so

00:36:09.520 --> 00:36:13.000
this local host eight thousand will become my global end

00:36:13.000 --> 00:36:16.560
point so slash API slash HR so anyone can hit this end point

00:36:16.560 --> 00:36:20.660
and then they will be able to talk to my HR model. So I

00:36:20.660 --> 00:36:26.480
believe this part we all understand. Right guys. Yeah. So I

00:36:26.480 --> 00:36:29.900
don't think that like we should talk this talk about this

00:36:29.900 --> 00:36:32.560
one because we have started our entire generative class with

00:36:32.560 --> 00:36:36.600
API. Right so we all understand that API part we all

00:36:36.600 --> 00:36:38.740
understand that what is the meaning of this girl. We all

00:36:38.740 --> 00:36:41.940
understand that if we are going to host this entire solution

00:36:41.940 --> 00:36:46.860
on some cloud platform. So this is this is going to change.

00:36:46.980 --> 00:36:49.800
Right. This will become IP address or maybe a domain name

00:36:49.800 --> 00:36:52.560
that we are going to map. And this is how even URI is

00:36:52.560 --> 00:36:55.520
working. Right. So this is how even URI is working. So

00:36:55.520 --> 00:36:58.800
whenever you try to select a particular model right it goes

00:36:58.800 --> 00:37:01.880
in the same direction is go it goes in the same way. Right.

00:37:01.960 --> 00:37:04.360
If it is not a proprietary model if it is a proprietary

00:37:04.360 --> 00:37:07.600
model then we are directly calling their API. So from our

00:37:07.600 --> 00:37:11.540
system to their system we are trying to like make a call

00:37:11.540 --> 00:37:15.960
back and forth over there. So I'm able to test my model in

00:37:15.960 --> 00:37:19.540
local. And that too even through an API I'm able to test it.

00:37:19.760 --> 00:37:24.260
So if it is working here it should work and it must work

00:37:24.260 --> 00:37:27.980
everywhere. Right. In whatever cloud platform I would like

00:37:27.980 --> 00:37:32.740
to like deploy these kind of a solution. But obviously as my

00:37:32.740 --> 00:37:38.720
model size is very very huge it's big. So again any free

00:37:38.720 --> 00:37:42.460
platform is not going to help me out in terms of doing a

00:37:42.460 --> 00:37:46.280
model inferencing. They are not going to provide you. That

00:37:46.280 --> 00:37:49.880
much of compute which is required for this kind of

00:37:49.880 --> 00:37:54.300
inferencing. Even with maybe like a 10 request per second.

00:37:54.500 --> 00:37:59.700
So we need a platform so where we can like deploy the way we

00:37:59.700 --> 00:38:02.620
try to deploy it on a production system. And this is where

00:38:02.620 --> 00:38:07.080
like a next to next to I think next chapter comes into a

00:38:07.080 --> 00:38:12.540
picture. So if you will check your syllabus part here. We

00:38:12.540 --> 00:38:19.000
have mentioned. All three cloud platform. So if you will go

00:38:19.000 --> 00:38:23.200
in this section so Gen AI on cloud. So here hosting model on

00:38:23.200 --> 00:38:28.680
Azure, Vortex and AWS bedrock. So we will try to understand

00:38:28.680 --> 00:38:32.160
even this part that how we can host a model all on three

00:38:32.160 --> 00:38:36.320
cloud platform GCP or Azure or AWS. If I talk about our

00:38:36.320 --> 00:38:40.300
solution so we are using AWS by the way AWS is a default

00:38:40.300 --> 00:38:43.280
stack that we are using. But in class I will be talking

00:38:43.280 --> 00:38:47.460
about all three that how we can like you know deploy. Even I

00:38:47.460 --> 00:38:51.420
can use H100 or something like that A100 or H200 kind of a

00:38:51.420 --> 00:38:54.680
system somewhere and then we can try to go for the

00:38:54.680 --> 00:38:57.440
deployment that is that is also possible. But yeah that

00:38:57.440 --> 00:39:00.620
those chapters are already here. So let's not talk about

00:39:00.620 --> 00:39:06.820
much now. So fine we are able to test this model right. We

00:39:06.820 --> 00:39:09.400
are able to test this model and everything is working

00:39:09.400 --> 00:39:13.340
everything is up and running. Now what we have to do. So we

00:39:13.340 --> 00:39:19.560
have to send this model basically for a deployment. So here

00:39:19.560 --> 00:39:23.560
we can try to use a docker approach or we can try to use

00:39:23.560 --> 00:39:27.500
basically a Kubernetes approach. So any idea guys about a

00:39:27.500 --> 00:39:29.820
containerization. So what what is the meaning of

00:39:29.820 --> 00:39:35.420
containerization. Any idea anyone yeah anyone.

00:39:39.850 --> 00:39:42.570
So anyone who can tell me like what is the benefit I will be

00:39:42.570 --> 00:39:45.530
able to get. See I am able to test a model even in local

00:39:45.530 --> 00:39:50.370
right. Without using any kind of a docker just fast API. So

00:39:50.370 --> 00:39:53.930
I can I can even like you know host this model in this way.

00:39:54.010 --> 00:39:56.610
Like the way I have tested it. So what is the need of a

00:39:56.610 --> 00:39:59.270
docker over here. Any idea guys yeah.

00:40:14.610 --> 00:40:17.930
So people are saying that creating a fixed environment yeah

00:40:17.930 --> 00:40:21.610
true building executable with inbuilt environment that is

00:40:21.610 --> 00:40:24.270
also true it will help you to keep all the dependency in one

00:40:24.270 --> 00:40:28.050
container means yeah one single like a machine you can say a

00:40:28.050 --> 00:40:32.770
logical machine. Worsening okay easy to share and run on any

00:40:32.770 --> 00:40:35.670
system. It's a image which is independent of the system

00:40:35.670 --> 00:40:39.250
anyone can use it once yes that that is actually a correct

00:40:39.250 --> 00:40:42.590
answer. So see generally what happens so I'm trying to run

00:40:42.590 --> 00:40:45.490
something in my system and maybe you are trying to run

00:40:45.490 --> 00:40:50.150
something on your system right. Our machine will be or when

00:40:50.150 --> 00:40:53.610
I say machine right so our my operating system can be

00:40:53.610 --> 00:40:56.250
different your operating system can be different maybe you

00:40:56.250 --> 00:40:58.990
are running same thing into a Windows machine. Maybe into a

00:40:58.990 --> 00:41:02.010
Linux maybe into a Mac machine even in case of a Linux

00:41:02.010 --> 00:41:05.170
machine there is a chances that your Linux is different mine

00:41:05.170 --> 00:41:08.490
is different your Mac OS version is different mine is

00:41:08.490 --> 00:41:13.330
different Windows machine wise so my Windows environment is

00:41:13.330 --> 00:41:15.610
different yours is different. So even with respect to our

00:41:15.610 --> 00:41:19.710
Windows there is a chance that it will not work right. If my

00:41:19.710 --> 00:41:22.410
Windows version is going to change if my Linux version is

00:41:22.410 --> 00:41:26.010
going to change if my OS version is going to change right.

00:41:26.430 --> 00:41:30.210
So keeping that in a mind. There is a universal solution you

00:41:30.210 --> 00:41:32.650
will be able to find out which is called as Dockerization

00:41:33.030 --> 00:41:36.170
right which is called as basically Dockerization. So what it

00:41:36.170 --> 00:41:40.290
does so basically it is going to help us out to create one

00:41:40.290 --> 00:41:44.890
single unified solution or system you can say right a

00:41:44.890 --> 00:41:47.250
system. So although I am running these things in my Windows

00:41:47.250 --> 00:41:50.870
machine and maybe you will end up running the same thing

00:41:50.870 --> 00:41:56.690
into a Linux machine or maybe into a Mac OS but but so here

00:41:56.690 --> 00:42:00.870
just go to a Docker file. Right Docker file and let's try to

00:42:00.870 --> 00:42:04.610
understand at least once for all because doesn't matter

00:42:04.610 --> 00:42:06.870
which project you are doing whether you are doing project

00:42:06.870 --> 00:42:09.770
with respect to our data science or big data or maybe data

00:42:09.770 --> 00:42:13.430
analytics or maybe a web development meaning of Docker is

00:42:13.430 --> 00:42:16.130
going to be same right and everywhere we are using a Docker.

00:42:16.290 --> 00:42:19.870
Now what it does by the way so as you can see so we are

00:42:19.870 --> 00:42:24.290
trying to right so we are trying to set a image of our

00:42:24.290 --> 00:42:28.250
container means OS of our container. Yeah. Container means a

00:42:28.250 --> 00:42:31.850
machine let's suppose a piece of hardware right. So even I

00:42:31.850 --> 00:42:34.770
have a machine and its piece of the hardware inside that so

00:42:34.770 --> 00:42:38.790
we have operating system so in my like a hardware so I have

00:42:38.790 --> 00:42:42.770
a Windows machine but here for this particular project I

00:42:42.770 --> 00:42:47.130
have defined this particular version. So this is a Linux

00:42:47.130 --> 00:42:51.590
version which I am trying to use. Now if I am using this

00:42:51.590 --> 00:42:55.290
version and if I am able to run in my system so whenever you

00:42:55.290 --> 00:42:58.990
will run in your system. This version will be same right and

00:42:58.990 --> 00:43:02.230
this is where it will create a huge differences it will

00:43:02.230 --> 00:43:06.070
become a system independent means even I am running from a

00:43:06.070 --> 00:43:09.590
Windows machine but still I am using Ubuntu right. Even if

00:43:09.590 --> 00:43:11.570
you will run from a Mac machine but you will end up using

00:43:11.570 --> 00:43:14.570
Ubuntu and even if you are running maybe some other like

00:43:14.570 --> 00:43:18.030
some Linux version if you are trying to use maybe Fedora

00:43:18.030 --> 00:43:22.370
maybe Suzy kind of operating system Linux machine but still

00:43:22.370 --> 00:43:26.030
for this particular project this is going to be the version.

00:43:26.030 --> 00:43:29.490
So this is a big differentiator you will be able to find it

00:43:29.490 --> 00:43:32.310
and this is something called as dockerization or

00:43:32.310 --> 00:43:36.350
containerization right that we are trying to create one box

00:43:36.350 --> 00:43:39.910
one container now inside that box I have already defined

00:43:39.910 --> 00:43:42.730
that okay this is going to be my operating system layer

00:43:42.730 --> 00:43:46.330
right and then we are installing all those dependencies

00:43:46.330 --> 00:43:50.150
inside it. So whenever even you are going to run it right

00:43:50.150 --> 00:43:53.970
the box is going to be same doesn't matter where you are

00:43:53.970 --> 00:43:56.050
running that box. So this is what a dockerization. A docker

00:43:56.050 --> 00:44:00.310
is going to provide it to you by the way right. So here we

00:44:00.310 --> 00:44:03.970
can set our own like a operating system. So here we are

00:44:03.970 --> 00:44:07.830
using Ubuntu every one of us are going to use like some of

00:44:07.830 --> 00:44:12.030
the operating system right and once this is like done so

00:44:12.030 --> 00:44:14.630
then we are trying to set our working directory. So where

00:44:14.630 --> 00:44:19.370
our like you know entire application will go. So this is

00:44:19.370 --> 00:44:22.610
going to be a machine inside this machine. So we are trying

00:44:22.610 --> 00:44:26.470
to set this app as a directory. And then we are trying to

00:44:26.470 --> 00:44:30.210
install all the dependency just like a fresh machine right.

00:44:30.290 --> 00:44:32.890
So let's suppose if I am going to give you a new computer a

00:44:32.890 --> 00:44:36.110
new laptop you will do some python installation you will do

00:44:36.110 --> 00:44:38.910
all those dependencies installation. So similarly we are

00:44:38.910 --> 00:44:43.250
trying to do all of these installation over here. Then CUDA

00:44:43.250 --> 00:44:46.310
is required right. So CUDA is required GPU support is

00:44:46.310 --> 00:44:48.470
required. So we are trying to install basically torch torch

00:44:48.470 --> 00:44:51.150
within torch audio and even a CUDA over here as you can see

00:44:51.150 --> 00:44:54.330
link is already mentioned. So this is basically the

00:44:54.330 --> 00:44:57.050
dockerization. So basically inside this directory in this is

00:44:57.050 --> 00:45:00.570
a directory available inside my docker basically the new

00:45:00.570 --> 00:45:03.930
machine which I have like a created by the way this one

00:45:03.930 --> 00:45:08.630
yeah. Now inside this then what we are trying to do. So we

00:45:08.630 --> 00:45:12.170
are trying to copy the requirement.txt file to the home

00:45:12.170 --> 00:45:14.150
location to app location that's the reason it's been written

00:45:14.150 --> 00:45:16.770
as a dot. So if you would like to copy your requirement.txt

00:45:16.770 --> 00:45:19.450
in some other location so you can do that. So what it will

00:45:19.450 --> 00:45:21.970
do so it will try to take this requirement.txt the

00:45:21.970 --> 00:45:24.990
requirement.txt which I have over here. And then it will try

00:45:24.990 --> 00:45:28.390
to copy into that box the box that we are trying the virtual

00:45:28.390 --> 00:45:31.030
box you can imagine the box that we are trying to create. So

00:45:31.030 --> 00:45:34.210
inside that it is trying to copy that requirement.txt and

00:45:34.210 --> 00:45:37.670
then story is going to be same. So even in your local system

00:45:37.670 --> 00:45:39.890
what you will do. Even in local system you will do pip

00:45:39.890 --> 00:45:43.210
install requirement.txt. So we are trying to do pip install

00:45:43.210 --> 00:45:48.030
requirement.txt. So now till this part our system will be

00:45:48.030 --> 00:45:50.510
ready right. Our system will be ready with all the

00:45:50.510 --> 00:45:54.370
dependencies with this operating system. Okay. Basically it

00:45:54.370 --> 00:45:58.130
will be ready. Now once it is ready. So we are trying to

00:45:58.130 --> 00:46:01.870
copy the application code. Basically so whatever like

00:46:01.870 --> 00:46:04.990
application code that we have over here whatever code base

00:46:04.990 --> 00:46:07.990
that we are like a sign to send. So it is going to copy all

00:46:07.990 --> 00:46:11.650
of those code and then we are trying to open up 8000 port to

00:46:11.650 --> 00:46:14.990
the outer world that okay. So whatever like you do so just

00:46:14.990 --> 00:46:21.190
just try to expose it to the outer world for me now. So once

00:46:21.190 --> 00:46:24.990
this exposure will be done. Right. So here we have we are

00:46:24.990 --> 00:46:29.110
just trying to like a set something for our GPUs. So CUDA

00:46:29.110 --> 00:46:32.170
visible device is equal to 0. PyTorch CUDA location

00:46:32.170 --> 00:46:37.350
configuration is equals to maximum split MB 512. So

00:46:37.350 --> 00:46:41.330
basically we are trying to like allocate our memory for our

00:46:41.330 --> 00:46:45.470
GPU. Right. And we are trying. So here we are what we are

00:46:45.470 --> 00:46:48.390
saying CUDA visible device 0. It simply means that that we

00:46:48.390 --> 00:46:53.370
are trying to restrict our container to use only GPU 0. So.

00:46:53.370 --> 00:46:56.370
If multiple GPUs are available we are saying that okay just

00:46:56.370 --> 00:47:01.030
use a GPU number 0 over here. Don't try to use any other GPU

00:47:01.030 --> 00:47:07.470
basically. Right and now and then we are trying to run this

00:47:07.470 --> 00:47:10.930
one. So we are saying that that try to run Python command

00:47:10.930 --> 00:47:15.670
and then with that try to run API server. The things that we

00:47:15.670 --> 00:47:19.490
have just executed for our testing purposes. Right. So API

00:47:19.490 --> 00:47:23.350
server.py file. So technically we are not doing much.

00:47:24.270 --> 00:47:27.370
Difference. Right. A much different thing over here. It's

00:47:27.370 --> 00:47:29.730
simply means that that we are trying to create a separate

00:47:29.730 --> 00:47:33.490
box and inside that box we are trying to configure each and

00:47:33.490 --> 00:47:37.250
everything so that if it will run in my system it will run

00:47:37.250 --> 00:47:40.950
on every system every system in this entire universe. This

00:47:40.950 --> 00:47:44.330
is what we are trying to like say this is what like we are

00:47:44.330 --> 00:47:50.170
trying to do over here. So this is my docker file now. So

00:47:50.170 --> 00:47:53.090
this is going to help me out in terms of building the entire

00:47:53.090 --> 00:47:58.650
universe. Right. So we have a docker compose file. Right. So

00:47:58.650 --> 00:48:03.350
we have a docker compose file and again so we can have so

00:48:03.350 --> 00:48:07.870
okay before moving to this compose file. So let me know guys

00:48:07.870 --> 00:48:11.510
if we have any kind of a issue with this. Yeah.

00:48:14.050 --> 00:48:15.690
So any question for me.

00:48:27.140 --> 00:48:31.160
For OS how it works. So okay I will tell you that part. I

00:48:31.160 --> 00:48:33.740
will tell you that part. So we will have to like install a

00:48:33.740 --> 00:48:36.960
docker and then we have to like a build it. So technically

00:48:36.960 --> 00:48:41.040
it is going to like a build a image with the help of our

00:48:41.040 --> 00:48:41.620
docker app.

00:49:29.310 --> 00:49:32.310
Why we put CUDA is equal to zero. So we are putting CUDA is

00:49:32.310 --> 00:49:36.190
equal to zero so that we are just trying to give like a kind

00:49:36.190 --> 00:49:41.550
of instruction that use a GPU number zero only if I have a

00:49:41.550 --> 00:49:45.190
multiple GPUs in my system. So in that case just try to use

00:49:45.190 --> 00:49:51.570
only one. So zeroth GPU because GPUs will be numbered in

00:49:51.570 --> 00:49:52.830
that way. Okay

00:49:57.600 --> 00:50:02.720
now so this is one of the file. Now we have another file

00:50:02.720 --> 00:50:07.680
called as docker compose. Now many people like you know will

00:50:07.680 --> 00:50:11.920
be confused between a docker file and a docker compose over

00:50:11.920 --> 00:50:13.780
here. What is what is the difference between these two

00:50:13.780 --> 00:50:19.340
files. So see this docker file you can think as like a kind

00:50:19.340 --> 00:50:23.080
of a file which will create one single machine. Right. Or

00:50:23.080 --> 00:50:26.400
you can say in a technical term. It is going to create one

00:50:26.400 --> 00:50:28.880
of them. It is going to create one of the images. Now here

00:50:28.880 --> 00:50:32.860
there is something called as docker compose that we have and

00:50:32.860 --> 00:50:39.200
this is going to help me out in terms of or in terms of you

00:50:39.200 --> 00:50:43.760
can say running a multiple or managing a multiple images at

00:50:43.760 --> 00:50:46.400
a time. So this is a two differences you will be able to

00:50:46.400 --> 00:50:49.820
find out between a docker compose yaml file and this

00:50:49.820 --> 00:50:52.820
particular docker file. So with the help of this yaml file

00:50:52.820 --> 00:50:57.480
you will be able to run one container or one. One images you

00:50:57.480 --> 00:51:00.100
will be able to execute or maybe you will be able to execute

00:51:00.100 --> 00:51:04.600
more images all together for a different different purposes.

00:51:04.980 --> 00:51:07.340
This is the reason. So we have a yaml is nothing but just a

00:51:07.340 --> 00:51:10.880
configuration file you can say. Yeah. Just a configuration

00:51:10.880 --> 00:51:16.220
file that we are technically talking about. Okay. Now so

00:51:16.220 --> 00:51:21.600
moving to the next how we will be able to build a docker

00:51:21.600 --> 00:51:27.800
image now so to build any docker image. In any machine. So

00:51:27.800 --> 00:51:33.060
first of all you have to go to a docker and you have to

00:51:33.060 --> 00:51:37.240
download a docker for windows linux or mac. It is available

00:51:37.240 --> 00:51:40.820
for all three machines right and this should be available

00:51:40.820 --> 00:51:44.940
inside your system. If it is not available you will not be

00:51:44.940 --> 00:51:49.260
able to build a docker inside your system. So this is the

00:51:49.260 --> 00:51:52.680
application which is already available right so you can go

00:51:52.680 --> 00:51:54.940
and you can download it for a windows machine. You can go

00:51:54.940 --> 00:51:57.200
and download it for a mac machine you can go and download it

00:51:57.200 --> 00:52:01.300
for the linux machine very very simple but yeah after

00:52:01.300 --> 00:52:05.760
download only you will be able to perform any of like these

00:52:05.760 --> 00:52:10.520
operations. So let me ping you this piece of link.

00:52:13.640 --> 00:52:18.120
So first download it and even so we are not going to use

00:52:18.120 --> 00:52:20.760
this docker just now. So we are going to use this docker

00:52:20.760 --> 00:52:23.500
things in almost all the project that is listed down inside

00:52:23.500 --> 00:52:26.900
your syllabus. Right. It will be required right it will be

00:52:26.900 --> 00:52:28.740
required. So we are going to use it again and again and

00:52:28.740 --> 00:52:32.800
again. So simple just go download install it and once you

00:52:32.800 --> 00:52:37.960
are going to install it you are you will be able to see

00:52:37.960 --> 00:52:40.840
something like this. So I have already installed right I

00:52:40.840 --> 00:52:44.180
have already installed it over here. Couple of images are

00:52:44.180 --> 00:52:47.160
running in my system in my machine. So even I was testing

00:52:47.160 --> 00:52:49.700
like yesterday night. So yeah couple of things are running

00:52:49.700 --> 00:52:55.240
over here. This is this LLM APIs. I have created. So just

00:52:55.240 --> 00:53:00.700
look at the size of one of my like you know images it's a 42

00:53:00.700 --> 00:53:04.800
.96 GB of image that I had ended up like a creating

00:53:04.800 --> 00:53:09.680
yesterday night itself yesterday night when I was like

00:53:09.680 --> 00:53:12.720
trying to test this entire piece of the code in my system.

00:53:13.020 --> 00:53:17.700
So here a first thing is that that you have to basically

00:53:17.700 --> 00:53:22.560
install a docker inside your machine and then start it.

00:53:22.700 --> 00:53:26.540
Yeah. Install it and then start it. Once you are going to

00:53:26.540 --> 00:53:30.780
start then only you will be able to build an image with

00:53:30.780 --> 00:53:34.620
respect to your project inside your system. Otherwise you

00:53:34.620 --> 00:53:37.440
will not be able to build it. So how you will be able to

00:53:37.440 --> 00:53:40.660
build an images. Let me show you that part as well. So if

00:53:40.660 --> 00:53:44.060
you'll go to your readme I have clearly mentioned over here

00:53:44.060 --> 00:53:47.240
that if you have to build an image a command is very very

00:53:47.240 --> 00:53:48.980
simple where is a command.

00:53:53.800 --> 00:53:58.860
Yeah. So here. Right. So if I have to build a image docker

00:53:58.860 --> 00:54:02.480
image so basically you have to like go to your console and

00:54:02.480 --> 00:54:06.180
then docker build hyphen D LLM API latest or this is just a

00:54:06.180 --> 00:54:09.220
name I have given right so that it will start building your

00:54:09.220 --> 00:54:13.360
like a docker images and this is going to take quite a time.

00:54:13.480 --> 00:54:16.500
Right. This is going to take quite a time for you. Now once

00:54:16.500 --> 00:54:20.060
your image will be built right then you can try to just

00:54:20.060 --> 00:54:23.500
simply run. So start all the services so docker compose up

00:54:23.500 --> 00:54:27.300
and then D. So even docker compose up is going to work but

00:54:27.300 --> 00:54:30.760
it will keep on giving you a log. So D is nothing but D is

00:54:30.760 --> 00:54:33.640
for running some processes in a background. This is the

00:54:33.640 --> 00:54:37.460
meaning of a D by the way in a background right. So and

00:54:37.460 --> 00:54:40.300
again if you have to like you know stop the services so in

00:54:40.300 --> 00:54:44.080
that case you can just say docker compose down and it is

00:54:44.080 --> 00:54:48.220
going to stop the service. Now let me run this command

00:54:48.220 --> 00:54:50.980
docker compose up D docker

00:54:54.470 --> 00:54:59.330
compose up D so yeah it will start loading the images for

00:54:59.330 --> 00:55:03.630
me. And you will not find any kind of a differences by the

00:55:03.630 --> 00:55:08.930
way. So you will be able to see that you know like it's

00:55:08.930 --> 00:55:13.190
pretty much like a same the way we were able to test a fast

00:55:13.190 --> 00:55:16.510
API. It's the exact same thing right. You all will be able

00:55:16.510 --> 00:55:17.630
to see it even over here.

00:55:31.150 --> 00:55:35.290
I'm using I3 does it work. Yeah docker will work basically

00:55:35.290 --> 00:55:39.570
but yes it will it will take a lot of time. And before that

00:55:39.570 --> 00:55:43.090
so you have to like a train this models. Right. Because I

00:55:43.090 --> 00:55:46.630
have this model in my system but when I have shared a file

00:55:46.630 --> 00:55:49.790
with all of you I haven't shared a model because if I'll

00:55:49.790 --> 00:55:54.170
show you the size of only this particular file it was around

00:55:54.170 --> 00:55:55.770
20 GB. There

00:56:11.260 --> 00:56:16.920
is a property option okay yeah yeah 29 GB. So 29 GB was the

00:56:16.920 --> 00:56:21.780
total like a model size by the way. So I haven't shared this

00:56:21.780 --> 00:56:26.480
it's not possible to share basically so and that's the

00:56:26.480 --> 00:56:28.300
reason so I think I have asked you in the previous class

00:56:28.300 --> 00:56:31.640
that before coming to the next class just try to train your

00:56:31.640 --> 00:56:34.900
model which will take time even for my system it was taking

00:56:34.900 --> 00:56:38.080
like a one or two hour of time. So for sure for most of you

00:56:38.080 --> 00:56:41.420
it is going to take like a more than two hour of time. So

00:56:41.420 --> 00:56:44.980
maybe you can just like you know start the training and then

00:56:44.980 --> 00:56:48.100
go for a sleep and then next morning you will be able to get

00:56:48.100 --> 00:56:48.500
your model.

00:56:53.810 --> 00:56:56.690
Oh it is like a building once again.

00:57:10.130 --> 00:57:13.170
So it will it will take some time. 15-20 minutes. I mean it

00:57:13.170 --> 00:57:17.070
is a minimum time it is like going to take from

00:57:28.080 --> 00:57:30.800
where we need to download this model you don't have to

00:57:30.800 --> 00:57:34.340
download the model you have to run basically this file

00:57:34.340 --> 00:57:39.720
called as train all model.py file this one train file so

00:57:39.720 --> 00:57:43.500
that it is going to create a model directory and file inside

00:57:43.500 --> 00:57:46.160
your system automatically it will create it everything is

00:57:46.160 --> 00:57:49.700
already configured so that it will create a file for you

00:57:49.700 --> 00:57:50.600
because

00:57:53.510 --> 00:57:56.330
you are training it with your own data. So anyone guys. Who

00:57:56.330 --> 00:57:59.930
have done a training of these this model the fine tune model

00:57:59.930 --> 00:58:05.100
which I have discussed anyone yeah because I have asked you

00:58:05.100 --> 00:58:08.280
to do that that's the reason I have shared a code file in my

00:58:08.280 --> 00:58:10.020
previous class yeah

00:58:12.550 --> 00:58:18.770
no no one okay maybe you can try today it was hanging in my

00:58:18.770 --> 00:58:23.090
system okay I mean like for most of you so yes it is like

00:58:23.090 --> 00:58:26.790
going to give you a issue better thing will be that go ahead

00:58:26.790 --> 00:58:33.030
with only one model at a time. Okay. One single epoch. So

00:58:33.030 --> 00:58:36.710
maybe instead of running this all maybe just run one maybe

00:58:36.710 --> 00:58:39.130
like a Cora system

00:58:42.870 --> 00:58:49.290
not supported so fine tuned in open AI platform open AI

00:58:49.290 --> 00:58:54.610
platform. So again there will be a bottleneck if you are

00:58:54.610 --> 00:58:57.150
doing a fine tuning with respect to open AI platform because

00:58:57.150 --> 00:59:04.810
like it is like going to be available on open AI platform it

00:59:04.810 --> 00:59:09.250
will not be like a data which will be available in your

00:59:09.250 --> 00:59:13.370
environment so data is also going to open AI so everything

00:59:13.370 --> 00:59:18.720
will be like going into the open AI so it's better because

00:59:18.720 --> 00:59:22.220
ideally this is what we do and again so if your system is

00:59:22.220 --> 00:59:25.800
not supported that's completely fine we have already talked

00:59:25.800 --> 00:59:29.880
about H100 right hyper stack cloud platform I believe you

00:59:29.880 --> 00:59:33.200
all have like you know used those platform for a model

00:59:33.200 --> 00:59:36.920
inferencing right in my previous classes do it over there

00:59:36.920 --> 00:59:40.580
very simple at least for one hour at least for one hour spin

00:59:40.580 --> 00:59:45.460
your machine in H100 do a training there simple when

00:59:50.610 --> 00:59:53.270
dockerizing we are uploading a fine tuned model as well as a

00:59:53.270 --> 01:00:00.150
entire folder yes yeah means a complete image we will try to

01:00:00.150 --> 01:00:04.550
ship basically that is one of the way so we can ship the

01:00:04.550 --> 01:00:07.470
entire image that this is the reason so we are saying that

01:00:07.470 --> 01:00:10.050
it will be system independent right so whatever is working

01:00:10.050 --> 01:00:12.350
in my system it will work in other system as well. So we

01:00:12.350 --> 01:00:16.010
will ship the entire like a so we will not ship the entire

01:00:16.010 --> 01:00:19.890
images I would say we will try to build a image over there

01:00:19.890 --> 01:00:24.390
so in whatever platform we are going to deploy we will

01:00:24.390 --> 01:00:27.630
simply run the same command docker compose upd it will build

01:00:27.630 --> 01:00:33.810
a image over there for me for fine tune any model normally

01:00:33.810 --> 01:00:37.930
what should be the system configuration I mean like in

01:00:37.930 --> 01:00:42.870
general obviously you need like a configuration where you

01:00:42.870 --> 01:00:47.250
have models. So more than 16 GB of RAM at least and then

01:00:47.250 --> 01:00:52.190
core i7 or core iron processor which is required a latest

01:00:52.190 --> 01:00:53.870
generation if

01:00:55.720 --> 01:00:58.580
you are going to do a model fine tuning even for a small

01:00:58.580 --> 01:01:02.200
models for a big one even this configuration will not be

01:01:02.200 --> 01:01:05.440
sufficient you have to go for a cloud option cloud is the

01:01:05.440 --> 01:01:06.360
only option that we have.

01:01:20.100 --> 01:01:24.060
For creating a docker images is it required to have a model

01:01:24.060 --> 01:01:28.800
folder locally no technically it is not required. Yeah. I

01:01:28.800 --> 01:01:32.800
would say but yeah at the time at the end of the day you are

01:01:32.800 --> 01:01:36.100
trying to build a doc basically it depends right so for what

01:01:36.100 --> 01:01:39.520
purpose I am building a docker. So if I am building a docker

01:01:39.520 --> 01:01:44.020
for doing inferencing and after for like a running this api

01:01:44.020 --> 01:01:49.100
dot server dot pi file so if it is not available then what

01:01:49.100 --> 01:01:51.780
what we will like build for

01:01:59.040 --> 01:02:01.880
running with the CPU what changes docker file needed nothing

01:02:01.880 --> 01:02:05.740
so if CUDA is available it will take it as I am doing a

01:02:05.740 --> 01:02:08.880
installation dependency installation for a CUDA if CUDA is

01:02:08.880 --> 01:02:10.900
not available it will not take it as simple as that.

01:02:14.150 --> 01:02:18.450
So still it is like taking some time it will take like time

01:02:18.450 --> 01:02:19.970
15-20 minute is a minimum.

01:02:59.350 --> 01:03:03.730
So sometime docker is going to take less amount of time for

01:03:03.730 --> 01:03:07.330
you to you know build it and sometime you will be able to

01:03:07.330 --> 01:03:11.050
see that it is taking more time and if you will observe

01:03:11.050 --> 01:03:15.130
closely so where it has taken a more amount of time. Any any

01:03:15.130 --> 01:03:19.850
guesses guys. Why it is taking more amount of time for me. I

01:03:19.850 --> 01:03:22.090
think we all can see the reason over here the highlighted

01:03:22.090 --> 01:03:23.350
one so

01:03:27.700 --> 01:03:32.140
basically it is trying to transfer or copy all the files and

01:03:32.140 --> 01:03:36.640
my model file is around 29gb or 30gb so this is the place

01:03:36.640 --> 01:03:40.840
where it has taken most of the time right almost like a 10

01:03:40.840 --> 01:03:43.660
-20 minute of time then you can see that it is copying my

01:03:43.660 --> 01:03:46.480
requirement dot txt file requirement dot txt file and now it

01:03:46.480 --> 01:03:49.300
is running the pip installation. Which is a small process.

01:03:49.300 --> 01:03:53.220
Which is not going to take much of time right and even I

01:03:53.220 --> 01:03:56.640
have shown you that my image size is very very big. So my

01:03:56.640 --> 01:04:00.180
image size is big because the data which it has copied was

01:04:00.180 --> 01:04:04.040
very very huge. Technically that data was my nothing but my

01:04:04.040 --> 01:04:07.860
model yeah it was nothing but my model and this is the place

01:04:07.860 --> 01:04:13.000
it has taken most amount of the time. So if my like a data

01:04:13.000 --> 01:04:16.540
file is small the file which I am transferring from local to

01:04:16.540 --> 01:04:20.220
that box basically. It will not take. So sometime you will

01:04:20.220 --> 01:04:24.300
see that docker is like taking couple of minute of time only

01:04:24.300 --> 01:04:28.220
or may be second of time to build it. In my cases it is

01:04:28.220 --> 01:04:31.800
taking more time because of the model. So model is basically

01:04:31.800 --> 01:04:36.800
a main culprit over here. After that you will be able to

01:04:36.800 --> 01:04:41.800
observe everything port is ok so here port is already

01:04:41.800 --> 01:04:49.280
allocated. So my 8000 is already occupied. My

01:04:59.240 --> 01:05:04.660
8000 is already occupied and because of that failed

01:05:37.420 --> 01:05:40.800
what so basically it is telling me that my port is my 8000

01:05:40.800 --> 01:05:44.480
is already occupied and even inside a docker if you will go

01:05:44.480 --> 01:05:48.200
and check so I have exposed to a 8000 port number. This is

01:05:48.200 --> 01:05:54.180
what it is trying to tell. Let me fix that issue yeah so

01:05:54.180 --> 01:05:58.640
this is running at 8000 ok that is fine.

01:07:01.720 --> 01:07:03.300
Ok. Maybe

01:07:07.010 --> 01:07:11.130
I can change this port itself 8001 ok

01:07:20.890 --> 01:07:22.510
save it. Port

01:07:27.500 --> 01:07:31.840
is already allocated where is my yaml ok

01:07:34.080 --> 01:07:37.180
so port 8002

01:07:39.040 --> 01:07:41.320
8000 ok

01:07:52.550 --> 01:07:56.270
so now it is telling me that fine tuning lm api container is

01:07:56.270 --> 01:08:00.950
started ok that's cool. So if it is started I can go inside

01:08:00.950 --> 01:08:05.630
my docker I can go inside the container and here so I will

01:08:05.630 --> 01:08:10.250
be able to basically see if it has started. So as you can

01:08:10.250 --> 01:08:13.230
see I have started this one on 8000 port number 1 my 8000

01:08:13.230 --> 01:08:19.130
was occupied and here so I will be able to observe something

01:08:19.130 --> 01:08:25.390
if not I will have to build it once again. Now on 8001 it is

01:08:25.390 --> 01:08:26.630
uvcon

01:08:32.740 --> 01:08:33.580
let

01:09:31.280 --> 01:09:41.720
me change my host port and my container port. Ok. So my host

01:09:41.720 --> 01:09:45.500
port is this my container port I will leave it as 8000 only

01:09:45.500 --> 01:09:46.560
ok

01:10:06.690 --> 01:10:10.410
now it is working. So I have just changed my mapping as my

01:10:10.410 --> 01:10:13.710
port number was like. So here if you will come inside my

01:10:13.710 --> 01:10:16.350
docker so you will be able to see that this fine tuning

01:10:16.350 --> 01:10:19.970
final is up and running and I have just changed the port

01:10:19.970 --> 01:10:23.930
mapping for now because in my 8000 so something was already

01:10:23.930 --> 01:10:28.010
running so it was occupied so I have just mapped it docker

01:10:28.010 --> 01:10:32.810
8000 to my 8001. Ok. So basically here we are doing two port

01:10:32.810 --> 01:10:36.110
mapping so the right hand side is basically this docker port

01:10:36.110 --> 01:10:40.750
number and this 8001 is nothing but my local system port

01:10:40.750 --> 01:10:43.950
number so that whatever is running in docker exposed to the

01:10:43.950 --> 01:10:47.550
outer world on this particular port. So this mapping is like

01:10:47.550 --> 01:10:51.470
that is something that we do all the time in docker. Now so

01:10:51.470 --> 01:10:54.030
exact same thing you will be able to find out nothing

01:10:54.030 --> 01:10:57.530
different right nothing different the only thing is this

01:10:57.530 --> 01:11:00.690
time we are running a complete dockerize solution. I can

01:11:00.690 --> 01:11:06.030
even like check our docs over here right and I can even hit

01:11:06.030 --> 01:11:09.270
my models and I will be able to do an inferencing but every

01:11:09.270 --> 01:11:13.010
time this time so everything is running inside my docker

01:11:13.010 --> 01:11:17.790
container as we can see right as we can see and even you

01:11:17.790 --> 01:11:21.110
will be able to see like all the logs over here or maybe you

01:11:21.110 --> 01:11:24.330
can try to call a docker compose logs command and you will

01:11:24.330 --> 01:11:28.310
be able to see the entire log. So this is how we have like

01:11:28.310 --> 01:11:32.850
you know. Basically like dockerize the entire solution. Now

01:11:32.850 --> 01:11:35.850
if I am able to dockerize this entire solution it simply

01:11:35.850 --> 01:11:40.670
means that that it will run everywhere right it will run

01:11:40.670 --> 01:11:44.650
everywhere and inside a docker. So when I was like a

01:11:44.650 --> 01:11:48.550
building it so I have even mentioned a CUDA right. So here

01:11:48.550 --> 01:11:53.430
you will be able to see that my docker is able to identify a

01:11:53.430 --> 01:11:57.930
CUDA. So GPU available true what GPU is available. So in my

01:11:57.930 --> 01:12:01.810
system. So I have RTX 4090 right this is the GPU which is

01:12:01.810 --> 01:12:05.790
available inside my local system and this is something that

01:12:05.790 --> 01:12:10.250
it is able to list down. So if you will go to my system

01:12:10.250 --> 01:12:12.650
basically you

01:12:19.210 --> 01:12:23.810
will be able to find out a GPU and what is my GPU by the

01:12:23.810 --> 01:12:28.490
way. So RTX 4090 is a GPU which is available but that is

01:12:28.490 --> 01:12:32.650
available at GPU 1 I have a GPU 0. So by default. I have

01:12:32.650 --> 01:12:35.990
given that. I have given that that try to invoke a GPU 0. So

01:12:35.990 --> 01:12:40.290
it is not going to invoke my GPU 1 but yeah even GPU 0 it is

01:12:40.290 --> 01:12:45.030
going to like invoke from there listing wise. So CUDA is

01:12:45.030 --> 01:12:49.650
able to CUDA will be able to direct always a NVIDIA GPU by

01:12:49.650 --> 01:12:52.750
the way. So that's the reason. So it is not a AMD or some

01:12:52.750 --> 01:12:57.250
other GPUs it will be able to invoke because CUDA is a

01:12:57.250 --> 01:13:01.030
library created by NVIDIA. So it is like listing now on my

01:13:01.030 --> 01:13:05.610
NVIDIA GPU over here. Okay. Simple. Right. So this is

01:13:05.610 --> 01:13:07.670
something that you will be able to find out inside the log.

01:13:07.890 --> 01:13:11.750
Now you will be able to hit your model the way you hit your

01:13:11.750 --> 01:13:17.610
model anywhere in this entire like a world by the way and if

01:13:17.610 --> 01:13:20.710
I have to make it available to the entire world. So maybe I

01:13:20.710 --> 01:13:23.790
can try to use a ngrok and with the help of this I can even

01:13:23.790 --> 01:13:28.290
expose my entire system as a server to the entire world or

01:13:28.290 --> 01:13:31.350
maybe I can go ahead with cloud deployment and then I can

01:13:31.350 --> 01:13:35.690
try to make this model available. To all of us. Yeah. Can I

01:13:35.690 --> 01:13:39.810
run any data size any data size in docker. See docker is

01:13:39.810 --> 01:13:44.430
just a software right. Everything depends upon where I am

01:13:44.430 --> 01:13:49.090
running a docker simple it is just a software right just a

01:13:49.090 --> 01:13:55.330
box basically it is giving you a way to provision any kind

01:13:55.330 --> 01:13:59.430
of operating system into it right and then any kind of a

01:13:59.430 --> 01:14:02.410
service installation or configuration you will be able to do

01:14:02.410 --> 01:14:05.210
it. This is the only place docker comes into a picture. So

01:14:05.210 --> 01:14:09.430
it is trying to make your solution universal right that if

01:14:09.430 --> 01:14:12.710
something is running in your system it will run in others

01:14:12.710 --> 01:14:15.690
system as well because generally we face this kind of issues

01:14:15.690 --> 01:14:18.930
right that some libraries some dependencies are working in

01:14:18.930 --> 01:14:22.050
my system but it's not working in your system then someone

01:14:22.050 --> 01:14:25.530
with maybe a Linux operating system will come and then they

01:14:25.530 --> 01:14:28.650
will say it's not working in my system then someone with Mac

01:14:28.650 --> 01:14:31.450
will come and they say it's not running in my system. So

01:14:31.450 --> 01:14:34.170
this is where docker comes into a picture as I have already

01:14:34.170 --> 01:14:37.390
mentioned that whenever we are trying to create a docker

01:14:37.390 --> 01:14:40.590
file. So we try to define a image that which operating

01:14:40.590 --> 01:14:44.290
system you should choose yeah. Now this operating system is

01:14:44.290 --> 01:14:46.230
going to be same so doesn't matter which operating system I

01:14:46.230 --> 01:14:49.970
have in my local this is the operating system in which you

01:14:49.970 --> 01:14:52.530
will be running this application only this application I

01:14:52.530 --> 01:14:56.510
will be running this entire application. So yeah. So size

01:14:56.510 --> 01:15:00.730
depends upon where I am running that docker. Basically. Now

01:15:00.730 --> 01:15:03.910
I am saying 8000 to 8001 please explain port mapping see

01:15:03.910 --> 01:15:06.910
here I have just changed the port mapping the reason was

01:15:06.910 --> 01:15:09.210
very simple you must have seen that I was getting an error

01:15:09.210 --> 01:15:12.310
why I was getting an error I was getting an error because my

01:15:12.310 --> 01:15:18.910
8000 port number was basically occupied where in my host

01:15:18.910 --> 01:15:22.850
system my host system see we have two system now technically

01:15:22.850 --> 01:15:27.490
we are talking about two systems. So one system is my host

01:15:27.490 --> 01:15:33.450
system. Host is my windows machine windows machine and then

01:15:33.450 --> 01:15:37.610
we have basically another system which is inside docker

01:15:37.610 --> 01:15:40.410
right where I am running the solution this is nothing but

01:15:40.410 --> 01:15:45.610
this is a ubuntu ubuntu machine. Now here the port number

01:15:45.610 --> 01:15:49.010
that you are able to see so this port number belongs to host

01:15:49.010 --> 01:15:53.330
machine so this 8001 belongs to host machine and this is

01:15:53.330 --> 01:15:57.530
basically belongs to this machine. So inside my docker. 8000

01:15:57.530 --> 01:16:00.430
port number was available this is also a machine right

01:16:00.430 --> 01:16:03.250
docker is nothing but it's a machine itself a box in itself

01:16:03.250 --> 01:16:06.090
right just like you have a windows so this is completely

01:16:06.090 --> 01:16:09.310
fine but in my host machine 8000 port number was not

01:16:09.310 --> 01:16:12.690
available because I have I was running fast api right and

01:16:12.690 --> 01:16:15.570
even I was running fast api at 8000 so it was giving me an

01:16:15.570 --> 01:16:18.970
error saying that that this port is basically occupied. So

01:16:18.970 --> 01:16:23.150
what I have done I have just changed it so here docker gives

01:16:23.150 --> 01:16:26.110
you a provision so you can try to change this one or this

01:16:26.110 --> 01:16:29.630
one. It depends so 8000 inside a docker was fine it was

01:16:29.630 --> 01:16:35.110
running now here right so here I have just changed this like

01:16:35.110 --> 01:16:38.790
a port number simple even inside a docker if you would like

01:16:38.790 --> 01:16:43.410
to change a port number you can do that as well. So again so

01:16:43.410 --> 01:16:49.230
you just have to go inside your like a fast api here where

01:16:49.230 --> 01:16:54.270
is my api server yeah so here we are doing a 8000 right. So

01:16:54.270 --> 01:16:57.390
wherever this code will run. By default it is it will take

01:16:57.390 --> 01:17:00.470
8000. So if it is running inside a docker it will take 8000

01:17:00.470 --> 01:17:04.970
right if I am going to make it 8001 in that case or 8002 or

01:17:04.970 --> 01:17:08.850
8000 something 8000x in that case even inside a docker it

01:17:08.850 --> 01:17:12.010
will run on that particular port number and then I can like

01:17:12.010 --> 01:17:14.070
a do a mapping with the outer world.

01:17:17.100 --> 01:17:22.100
So 8001 is a docker file is hosted inside docker nested port

01:17:22.100 --> 01:17:25.060
8000 is available. So we are exactly yeah docker is also a

01:17:25.060 --> 01:17:28.120
machine think that as a machine. Not think that as a machine

01:17:28.120 --> 01:17:34.120
it is actually a machine a box right. So the way port works

01:17:34.120 --> 01:17:36.860
we are just like doing a mapping that okay fine whatever is

01:17:36.860 --> 01:17:41.160
running on 8000 inside a docker when it will come outside

01:17:41.160 --> 01:17:44.760
because I am trying to hit my docker image from my local

01:17:44.760 --> 01:17:48.740
browser right this browser which I am trying to use so this

01:17:48.740 --> 01:17:51.300
browser is not available inside my docker it is outside

01:17:51.300 --> 01:17:54.800
basically so from outside world I am trying to hit my docker

01:17:54.800 --> 01:17:59.100
images. Yeah. So if in my outside world port is occupied so

01:17:59.100 --> 01:18:01.740
it will give me an issue and just to fix it I have changed

01:18:01.740 --> 01:18:05.200
the port number if you are running for the first time you

01:18:05.200 --> 01:18:08.020
will not get that issue by the way I am already like running

01:18:08.020 --> 01:18:11.500
lot of things on different port so 8000 especially so that

01:18:11.500 --> 01:18:14.120
is the reason I got an issue and I fixed it so this is the

01:18:14.120 --> 01:18:20.740
meaning of this port number by the way okay. So this is this

01:18:20.740 --> 01:18:25.220
now let me share this fresh file once again with all of you.

01:18:26.240 --> 01:18:29.760
So control a. I will not be selecting this model file

01:18:29.760 --> 01:18:33.860
otherwise file size will be very very big compressed to zip

01:18:33.860 --> 01:18:38.120
final oh final

01:18:52.360 --> 01:18:53.780
model

01:18:55.640 --> 01:19:09.210
fine tuning with docker image okay fine.

01:19:12.790 --> 01:19:14.970
So let me share okay

01:19:36.340 --> 01:19:39.060
guys. So I am like going to upload this one. This is the one

01:19:39.060 --> 01:19:43.620
in a previous lecture itself where I have uploaded a

01:19:43.620 --> 01:19:48.160
previous file inside your dashboard I

01:19:53.190 --> 01:19:57.330
have just changed the file name means our zip file name yeah

01:19:59.540 --> 01:20:05.620
so if you will go to your like previous session live

01:20:16.180 --> 01:20:19.520
lecture 12th October session which was the previous one so

01:20:19.520 --> 01:20:22.200
you will be able to see this file which I have uploaded just

01:20:22.200 --> 01:20:25.520
now and this is the file which I have uploaded last week.

01:20:26.200 --> 01:20:29.140
Last week. When I was like taking your very first class so

01:20:29.140 --> 01:20:32.960
this is updated file so over here only model is not

01:20:32.960 --> 01:20:36.700
available you have to create a model model size itself is

01:20:36.700 --> 01:20:41.260
like a 30 GB so can't like a transfer it so even zip will

01:20:41.260 --> 01:20:44.540
take like a two three hours of time okay.

01:20:51.100 --> 01:20:56.380
So now if something is available as a docker right inside my

01:20:56.380 --> 01:21:01.040
system so now I am like a good and now I will be able to

01:21:01.040 --> 01:21:04.420
host it anywhere. Okay. Now when we talk about a hosting

01:21:04.420 --> 01:21:07.760
right when we talk about a hosting so again a couple of

01:21:07.760 --> 01:21:10.720
things comes into a picture so we are we are trying to host

01:21:10.720 --> 01:21:16.260
it but how things will go on technically a scale what kind

01:21:16.260 --> 01:21:20.800
of a traffic it will be able to handle how many number of

01:21:20.800 --> 01:21:24.360
machines we are supposed to spawn or we are supposed to you

01:21:24.360 --> 01:21:27.620
know basically a spin over there what should be the bare

01:21:27.620 --> 01:21:33.300
minimum number of machines. Then. Basically. Like. What what

01:21:33.300 --> 01:21:37.500
kind of you know inferencing time it is supposed to take or

01:21:37.500 --> 01:21:40.580
maybe you can say a latency it should like a give it to me.

01:21:40.740 --> 01:21:45.240
So all to manage all of these things what do we do. So we

01:21:45.240 --> 01:21:50.900
try to go one step ahead and instead of taking all of these

01:21:50.900 --> 01:21:55.380
headache by ourselves so we can try to basically deploy this

01:21:55.380 --> 01:21:59.220
entire solution into a Kubernetes cluster with a GPU

01:21:59.220 --> 01:22:02.140
obviously. Right. So if you will go to this hyper stack

01:22:02.140 --> 01:22:06.180
cloud or any other cloud platform even inside AWS or Azure

01:22:06.180 --> 01:22:09.800
or GCP you will be able to find out that they have already

01:22:09.800 --> 01:22:14.220
given you a managed Kubernetes services right where you

01:22:14.220 --> 01:22:17.560
don't have to do much you just have to go and set up bare

01:22:17.560 --> 01:22:21.180
minimum number of the machine a default machine that okay I

01:22:21.180 --> 01:22:24.340
just want maybe one worker node and one like a master node

01:22:24.340 --> 01:22:29.000
this is the configuration right and then when traffic will

01:22:29.000 --> 01:22:31.640
come. So let's suppose if thousand people are trying to hit

01:22:31.640 --> 01:22:33.980
my machine or 10,000 people are trying to hit my machine

01:22:33.980 --> 01:22:37.880
then try to add this main number of a machine. So maximum

01:22:37.880 --> 01:22:40.560
machine or pool size basically the technically it's called

01:22:40.560 --> 01:22:43.740
as pool size right. So we try to define a pool size that

01:22:43.740 --> 01:22:48.080
okay let's let's like let's suppose like thousands of people

01:22:48.080 --> 01:22:51.000
are trying to hit on a single second at the concurrent like

01:22:51.000 --> 01:22:54.940
a user are trying to hit my model hit my services. So in

01:22:54.940 --> 01:22:59.280
that case you can go till this much of a maximum pool. And

01:22:59.280 --> 01:23:02.600
if it is going to exceed that your server is going to crash

01:23:02.600 --> 01:23:05.240
your server will say that okay fine I'm not able to give a

01:23:05.240 --> 01:23:08.900
response because my limit is over. Now all of these things

01:23:08.900 --> 01:23:12.500
in a easiest possible way right in a easiest possible way we

01:23:12.500 --> 01:23:15.700
will be able to manage with the help of Kubernetes. So

01:23:15.700 --> 01:23:20.440
Kubernetes is one of the you will say ideal solution in this

01:23:20.440 --> 01:23:23.620
entire world you will be able to find out. So where most of

01:23:23.620 --> 01:23:27.820
the solution we try to deploy as a Kubernetes services even

01:23:27.820 --> 01:23:30.260
the Euron website that you are able to see. So it doesn't

01:23:30.260 --> 01:23:32.920
matter whether like a thousand people are trying to hit or

01:23:32.920 --> 01:23:35.620
maybe ten thousand people are trying to hit it will be able

01:23:35.620 --> 01:23:39.040
to take a load because we have configured it till a certain

01:23:39.040 --> 01:23:41.540
load that okay this is the maximum load which I will be able

01:23:41.540 --> 01:23:45.140
to handle and if it will go beyond that it will crash for

01:23:45.140 --> 01:23:48.960
sure because we have created our pool size and obviously if

01:23:48.960 --> 01:23:52.580
like a load is going to exceed that pool size no one will be

01:23:52.580 --> 01:23:56.100
able to save your server right and we must have seen that

01:23:56.100 --> 01:23:59.200
even big big server crashes because of only that reason

01:23:59.200 --> 01:24:01.660
because I can't like a give a pool size equal to infinite it

01:24:01.660 --> 01:24:05.680
will cost me right I have to create a reserve pool. So here

01:24:05.680 --> 01:24:10.440
right so here we have a Kubernetes approach we will not do

01:24:10.440 --> 01:24:13.380
this as of now right we will try to do this Kubernetes

01:24:13.380 --> 01:24:17.780
things in our URI. So because similar kind of approach we

01:24:17.780 --> 01:24:21.220
have to follow so we have to basically like a hosted on a

01:24:21.220 --> 01:24:25.200
Kubernetes services we have to see that whether my server is

01:24:25.200 --> 01:24:29.900
scaling our itself or not. And then a beautiful UI interface

01:24:29.900 --> 01:24:33.340
obviously because API is fine as a API I will be able to

01:24:33.340 --> 01:24:37.800
expose on the other end I need a beautiful UI interface as

01:24:37.800 --> 01:24:42.100
well so that you know I will be able to build a URI that is

01:24:42.100 --> 01:24:45.000
that is something so here I have just mentioned just to give

01:24:45.000 --> 01:24:48.160
you a heads up because in couple of weeks I believe in three

01:24:48.160 --> 01:24:51.400
four weeks right we will start the project we are very near

01:24:51.400 --> 01:24:56.260
to our like you know final projects as our syllabus as soon

01:24:56.260 --> 01:24:58.400
as syllabus will be over. So we'll get into that. Section

01:24:58.400 --> 01:25:03.700
right so there Kubernetes services we are going to use it

01:25:03.700 --> 01:25:07.860
maybe you can try to like a start exploring but it is not at

01:25:07.860 --> 01:25:11.460
all difficult it's very very easy for example if I'll talk

01:25:11.460 --> 01:25:15.060
about AWS so they are giving you EKS services right so

01:25:15.060 --> 01:25:17.340
managed Kubernetes services they are already giving you and

01:25:17.340 --> 01:25:20.680
like you don't have to do anything just go spin the machine

01:25:20.680 --> 01:25:25.760
and you know solve it as simple as that so we'll be we'll be

01:25:25.760 --> 01:25:30.140
doing that this will be the. Final like you know this will

01:25:30.140 --> 01:25:33.640
be the final thing that we'll be doing now here so I have

01:25:33.640 --> 01:25:39.080
created some performance benchmark as well right that what

01:25:39.080 --> 01:25:42.900
in all like what kind of a training time it is going to take

01:25:42.900 --> 01:25:46.100
and what will be the memory user so you can look into this

01:25:46.100 --> 01:25:47.900
particular table as well this is going to be very very

01:25:47.900 --> 01:25:50.540
useful because last time some of you have asked me a

01:25:50.540 --> 01:25:53.700
question that if I'm trying to do this kind of a training

01:25:53.700 --> 01:25:56.640
then what will be the time it should take. What should be

01:25:56.640 --> 01:25:59.220
the. The memory consumption keeping that in the mind I have

01:25:59.220 --> 01:26:02.500
updated this readme file and where I have mentioned that

01:26:02.500 --> 01:26:06.480
that if model is basically this full fine tuning if you are

01:26:06.480 --> 01:26:09.240
trying to do right so in that case 45 to 60 minute it is

01:26:09.240 --> 01:26:13.180
going to take again epoch wise right so if I'm going to

01:26:13.180 --> 01:26:16.540
change the epoch it is going to change and again the data

01:26:16.540 --> 01:26:19.760
that I have taken memory consumption is going to be 12 to 16

01:26:19.760 --> 01:26:23.840
GB of a virtual RAM and model size will be around 2.2 GB or

01:26:23.840 --> 01:26:27.320
something or so. LoRa wise 20 to 30 minutes. minute right

01:26:27.320 --> 01:26:33.100
and which is like one of the like a model where we are not

01:26:33.100 --> 01:26:36.360
changing the entire weights 6 to 8 GB of RAM and then this

01:26:36.360 --> 01:26:38.620
is the model size preferred wise it is going to take this

01:26:38.620 --> 01:26:40.820
much of time and then this is going to be the tentative

01:26:40.820 --> 01:26:44.000
memory consumption quantized LoRa wise so it is going to

01:26:44.000 --> 01:26:47.520
take 15 to 20 minute and model size is going to be 25 MB one

01:26:47.520 --> 01:26:50.560
of the smallest one DPO so direct preference optimization

01:26:50.560 --> 01:26:53.940
wise so it is going to take like 30 to 40 minute of time. So

01:26:53.940 --> 01:26:56.740
full fine tuning is the only one which is going to take most

01:26:56.740 --> 01:27:00.800
amount of the time after DPO by the way. So this is the

01:27:00.800 --> 01:27:06.180
tentative like a training time it is going to take and the

01:27:06.180 --> 01:27:08.560
memory consumption and finally you will be able to see a

01:27:08.560 --> 01:27:12.980
model final model size right in general. Now inference

01:27:12.980 --> 01:27:17.820
performance wise so here basically if I will talk about HR

01:27:17.820 --> 01:27:21.000
full fine tuning model so load time is going to be 5 second

01:27:21.000 --> 01:27:24.600
to 10 second and first query is 2 to 3 second. Again this is

01:27:24.600 --> 01:27:26.840
a benchmarking which is been created with respect to my

01:27:26.840 --> 01:27:29.960
system right so whatever my system was like taking time and

01:27:29.960 --> 01:27:32.620
again if I will talk about my system configuration so I

01:27:32.620 --> 01:27:37.680
believe this is my system configuration so where I have a

01:27:37.680 --> 01:27:43.600
CPU so where I have you can see like a 16 core CPU we have I

01:27:43.600 --> 01:27:47.380
have 32 core logical over here and if I will talk about a

01:27:47.380 --> 01:27:50.300
memory so RAM wise I have a 96 GB of a RAM so this is a

01:27:50.300 --> 01:27:53.240
benchmarking with respect to my system where I have. I have

01:27:53.240 --> 01:27:57.060
trained it in your system it is not going to be same so

01:27:57.060 --> 01:27:59.940
whenever someone say this benchmarking with respect to what

01:27:59.940 --> 01:28:04.100
configuration my configuration 96 GB of RAM like and again

01:28:04.100 --> 01:28:10.560
RTX 4090 GPUs and 4 TB or 6 TB of hard disk so that is a i9

01:28:10.560 --> 01:28:14.520
processor so with that kind of a configuration this is the

01:28:14.520 --> 01:28:17.560
benchmarking. So in my system it takes around 5 to 10 second

01:28:17.560 --> 01:28:21.720
of time first query like takes 2 to 3 second of time.

01:28:21.720 --> 01:28:24.380
Because we know that that we are doing a catching catching

01:28:24.380 --> 01:28:27.980
of the model so when I am going to fire a second query it

01:28:27.980 --> 01:28:30.180
will not take same amount of time because it has already

01:28:30.180 --> 01:28:34.200
cached it in my client side right. Token per second which it

01:28:34.200 --> 01:28:37.280
will be able to yield is 100 to 150 token again so LoRa

01:28:37.280 --> 01:28:40.100
model which is like a healthcare model I have created so

01:28:40.100 --> 01:28:43.460
load time 3 to 5 second first query this much and then

01:28:43.460 --> 01:28:46.100
subsequent query is going to take this much. Sales marketing

01:28:46.100 --> 01:28:48.680
finance so for all of these things I have created a

01:28:48.680 --> 01:28:53.480
benchmark but again with respect to my system as simple as

01:28:53.480 --> 01:29:00.800
that yeah with respect to my system. Now so yeah rest is

01:29:00.800 --> 01:29:07.690
just a story by the way so you can maybe like a read it

01:29:07.690 --> 01:29:10.050
through this one use

01:29:12.390 --> 01:29:16.210
cases integration example so chatbot where you can try to

01:29:16.210 --> 01:29:19.110
integrate it and I believe we all know if something is

01:29:19.110 --> 01:29:22.090
available as an API I can integrate it anywhere mobile apps

01:29:22.090 --> 01:29:25.870
website. Any kind of integration. You all will be able to do

01:29:25.870 --> 01:29:28.630
it. Yeah one more amazing thing I do not know whether you

01:29:28.630 --> 01:29:32.510
have like tested that or not. So have you have you tested

01:29:32.510 --> 01:29:34.730
URI in WhatsApp yeah.

01:30:07.320 --> 01:30:10.940
So recently what we have done again so just an extension of

01:30:10.940 --> 01:30:14.800
our services. So here you will be able to see that if you

01:30:14.800 --> 01:30:17.280
will go and ping to this number the way you try to ping to

01:30:17.280 --> 01:30:20.320
your friends I think I have released couple of like a real

01:30:20.320 --> 01:30:23.140
video for that I do not know whether you have experienced

01:30:23.140 --> 01:30:25.560
that or not but yeah if you have not experienced it. You can

01:30:25.560 --> 01:30:29.100
experience it now maybe so this is the mobile number right

01:30:29.100 --> 01:30:32.420
where you can try to ping basically and even you can go from

01:30:32.420 --> 01:30:37.160
here so on our like your own website. So we have done this

01:30:37.160 --> 01:30:40.000
integration. So here right URI WhatsApp assistance we have

01:30:40.000 --> 01:30:43.080
added this is this mobile number is different from this

01:30:43.080 --> 01:30:45.780
number. So we have dedicated one number mobile number for

01:30:45.780 --> 01:30:50.200
this and what it does so just AI is available URI is

01:30:50.200 --> 01:30:52.480
available in your WhatsApp and you can ask any kind of a

01:30:52.480 --> 01:30:56.100
question in any of your languages. Plus you can try to even

01:30:56.100 --> 01:30:59.260
upload an image you can try to even give a voice command

01:30:59.260 --> 01:31:02.600
over here not just that so that is that is basically

01:31:02.600 --> 01:31:07.160
something that it gives from a general model right plus we

01:31:07.160 --> 01:31:11.200
have even changed its tone a little bit. So the way you will

01:31:11.200 --> 01:31:14.140
behave so this model the system will start behaving in that

01:31:14.140 --> 01:31:17.380
way. So we have created a specific agent for that and apart

01:31:17.380 --> 01:31:22.660
from this we have also exposed the Euron system. Means

01:31:22.660 --> 01:31:26.780
basically. A course and LMS system that we have so we have

01:31:26.780 --> 01:31:29.820
created an agent. So let's suppose here if I am going to ask

01:31:29.820 --> 01:31:36.760
a question that tell me the latest launch from Euron right.

01:31:37.120 --> 01:31:41.920
So it is not just going to like you know like pull the as

01:31:41.920 --> 01:31:44.520
you can see it has started typing it's very very fast right.

01:31:44.640 --> 01:31:48.260
So it has started like generating a response and it is not

01:31:48.260 --> 01:31:52.960
just giving me an output. So here generic certification boot

01:31:52.960 --> 01:31:55.860
camp 2.1. And all right this is the launch. So it is giving

01:31:55.860 --> 01:31:59.000
me all the detail. So it is it is able to understand

01:31:59.000 --> 01:32:03.680
everything technically and whether it's a part of Euron or

01:32:03.680 --> 01:32:06.840
maybe outside Euron even real time search. So if I am going

01:32:06.840 --> 01:32:15.360
to ask that tell me about AI latest news from India right.

01:32:15.600 --> 01:32:18.580
So if I am going to ask a question so it will go and do a

01:32:18.580 --> 01:32:21.640
Google search it will be able to understand if I am asking

01:32:21.640 --> 01:32:24.220
something from Euron. So it is understanding. Even that and

01:32:24.220 --> 01:32:27.640
even from Euron even I can ask about lecture right. So

01:32:27.640 --> 01:32:31.280
latest news and today is what 25th October yeah. So as you

01:32:31.280 --> 01:32:34.660
can see 25th October and these are the major news and these

01:32:34.660 --> 01:32:37.760
are the sources of that particular news. So it is like

01:32:37.760 --> 01:32:41.200
giving me even that kind of responses. If I am going to ask

01:32:41.200 --> 01:32:49.780
a question that give me for loop Python lecture

01:32:51.320 --> 01:32:55.220
something like this if I am going to ask. So it will go to

01:32:55.220 --> 01:33:00.540
our LMS system it will try to search and then yeah so it is

01:33:00.540 --> 01:33:04.820
given giving me a lecture basically a particular lecture

01:33:04.820 --> 01:33:09.380
itself. So basically why I am building this the reason is

01:33:09.380 --> 01:33:13.960
very simple so obviously in next one year or so we will be

01:33:13.960 --> 01:33:16.680
having so many lectures right we are like working we are

01:33:16.680 --> 01:33:19.720
creating on like we are creating so many like lectures on

01:33:19.720 --> 01:33:22.760
every segment in Hindi English and maybe going forward other

01:33:22.760 --> 01:33:26.920
languages as well. So as our library will grow it will be

01:33:26.920 --> 01:33:29.180
very difficult and again for like same thing we will be

01:33:29.180 --> 01:33:32.760
having multiple courses as well right. So here anyone can

01:33:32.760 --> 01:33:35.080
come and anyone can ask that okay I am looking for lecture

01:33:35.080 --> 01:33:38.460
in this in that and if that lecture is available it will

01:33:38.460 --> 01:33:42.740
bring a direct lecture link right. And obviously if you are

01:33:42.740 --> 01:33:45.220
going to click on this lecture link if you have a plus

01:33:45.220 --> 01:33:47.760
access then only you will be able to otherwise it will throw

01:33:47.760 --> 01:33:50.680
you to the payment page right. So anyone can come over here

01:33:50.680 --> 01:33:54.140
talk to the system. And then. They will be able to search

01:33:54.140 --> 01:33:58.620
the entire library instead of you know going through like

01:33:58.620 --> 01:34:04.180
courses and searching through our courses. So we have given

01:34:04.180 --> 01:34:08.080
this complete agentic capabilities just inside our WhatsApp.

01:34:08.280 --> 01:34:10.940
It works very good with images. Even I have uploaded this

01:34:10.940 --> 01:34:14.680
kind of images and then I have said that okay so enhance my

01:34:14.680 --> 01:34:17.740
pictures and make it like a colorful. It was able to do it

01:34:17.740 --> 01:34:21.720
everything everything. So almost. Black. Like all kind of a

01:34:21.720 --> 01:34:24.560
general purpose agent and a specific agent we have added

01:34:24.560 --> 01:34:28.640
inside this one and yeah running on WhatsApp simple. So no

01:34:28.640 --> 01:34:32.320
one has to you know come to Bev and then like ask a

01:34:32.320 --> 01:34:35.860
question. So everything can be asked here itself. So you can

01:34:35.860 --> 01:34:38.720
you can try this out those who have not tried maybe just

01:34:38.720 --> 01:34:42.440
ping and then start chatting with it right. Even we have

01:34:42.440 --> 01:34:45.200
given a capability that if I would like to give a feedback

01:34:45.200 --> 01:34:52.640
so I want to give a. Feedback. About. Gen. A. I. Lecture.

01:34:53.400 --> 01:34:57.260
It's okay. Yeah. So something like this. So it will be able

01:34:57.260 --> 01:34:59.180
to understand that you are trying to give a feedback

01:34:59.180 --> 01:35:02.900
basically. So here what it does automatically it logs your

01:35:02.900 --> 01:35:06.040
feedback in our databases we can we can see your feedbacks

01:35:06.040 --> 01:35:09.340
by the way we can we can see your feedbacks we can see your

01:35:09.340 --> 01:35:12.760
complaints we can see a new users everything we can we can

01:35:12.760 --> 01:35:18.570
like a bifurcate from here. Can you update my resume with

01:35:18.570 --> 01:35:21.130
the prompt and JD and no we have not added the resume

01:35:21.130 --> 01:35:24.630
capability so far. Maybe in future we'll do it but yeah for

01:35:24.630 --> 01:35:29.570
not with it and no not at all. See anything is good it's

01:35:29.570 --> 01:35:33.710
it's lucrative it's tempting but for building this kind of

01:35:33.710 --> 01:35:37.710
application where we need a latency in seconds right or

01:35:37.710 --> 01:35:42.130
maybe not in second even a microsecond millisecond anything

01:35:42.130 --> 01:35:45.750
will not work basically it will create a bottleneck. So it's

01:35:45.750 --> 01:35:50.350
not anything by the way it's a custom agent. Basically we

01:35:50.350 --> 01:35:54.870
have created. And yeah so back end code is written in a node

01:35:54.870 --> 01:35:57.610
by the way some piece of the code is written in Python as

01:35:57.610 --> 01:36:01.090
well for an agent basically which has been called here

01:36:01.090 --> 01:36:04.830
right. And we have hosted all of these agent on our server.

01:36:04.970 --> 01:36:09.310
So this is this was the actual approach in it and is good in

01:36:09.310 --> 01:36:11.610
terms of doing orchestration in terms of doing automation

01:36:11.610 --> 01:36:18.070
but yeah so where ever latency is a concern then I never

01:36:18.070 --> 01:36:21.650
advise anyone. To go ahead with any tool available in this

01:36:21.650 --> 01:36:25.410
entire world I always advise people to go ahead with the

01:36:25.410 --> 01:36:28.370
custom coding because that is the only way which will give

01:36:28.370 --> 01:36:31.990
you a flexibility no other way means literally no other way.

01:36:36.330 --> 01:36:40.290
How it is built which model for NLP I mean like model wise

01:36:40.290 --> 01:36:44.370
it is calling OpenAI model itself model wise if I'll say I'm

01:36:44.370 --> 01:36:48.710
calling agent by the way. So agent means basically a piece

01:36:48.710 --> 01:36:51.110
of the functions we are trying to call in a back end and

01:36:51.110 --> 01:36:55.110
then. That is trying to like understand and decide that

01:36:55.110 --> 01:36:57.030
whether I should do a Google search whether I should you

01:36:57.030 --> 01:37:00.130
know choose for this particular tone polite or maybe the

01:37:00.130 --> 01:37:03.530
aggressive tone and model is trying to decide that whether I

01:37:03.530 --> 01:37:06.510
should go and search your own database for something a model

01:37:06.510 --> 01:37:11.070
is so my agents are deciding basically that that whether

01:37:11.070 --> 01:37:13.550
it's a feedback query or something which which I can go and

01:37:13.550 --> 01:37:17.050
log somewhere in a back end. So yeah it's it's a custom

01:37:17.050 --> 01:37:20.590
agent that we have written same kind of agent that we have

01:37:20.590 --> 01:37:23.670
seen in our. Classes right same same agent we have created

01:37:23.670 --> 01:37:24.790
even

01:37:26.840 --> 01:37:29.220
it is showing a source of news as well yeah obviously it

01:37:29.220 --> 01:37:31.460
will show a source of news we have configured in that way.

01:37:32.640 --> 01:37:35.240
So that you will be able to check the authenticity of the

01:37:35.240 --> 01:37:37.360
news right otherwise automation

01:37:41.900 --> 01:37:45.620
using a smart brain. Yeah how can I build a voice agent with

01:37:45.620 --> 01:37:49.400
the same logic. See voice agent is nothing but it's a text

01:37:49.400 --> 01:37:51.980
to speech and a speech to text. So it's basically TTS and

01:37:51.980 --> 01:37:59.980
SST SPS STS sorry it's speech to text. STT yeah so basically

01:37:59.980 --> 01:38:05.100
like you try to convert your speech into like a text and

01:38:05.100 --> 01:38:08.980
then you try to pass your text after doing a tokenization

01:38:08.980 --> 01:38:11.760
model will give you a response and then you convert those

01:38:11.760 --> 01:38:18.130
responses into a speech again this is what it does. So are

01:38:18.130 --> 01:38:20.310
you connected or WhatsApp with your that's the reason you

01:38:20.310 --> 01:38:23.310
are able to access it yes obviously. So we have configured a

01:38:23.310 --> 01:38:26.730
complete like a WhatsApp that's the reason so all of you can

01:38:26.730 --> 01:38:28.670
ping on this number the number which I have pinged you

01:38:28.670 --> 01:38:31.970
right. So anyone can ping to this number and anyone can

01:38:31.970 --> 01:38:37.410
start doing a chat with this like a WhatsApp and I believe

01:38:37.410 --> 01:38:40.150
this is something that you guys can try creating because

01:38:40.150 --> 01:38:44.250
this is a need for any organization. See it is it is able to

01:38:44.250 --> 01:38:46.370
create even a beautiful you know pictures I have tested a

01:38:46.370 --> 01:38:49.410
lot and even you can see I had a lot of like conversation in

01:38:49.410 --> 01:38:52.190
Hindi language right and it was able to understand and the

01:38:52.190 --> 01:38:56.170
tone tone is amazing yeah tone is amazing. So maybe you can

01:38:56.170 --> 01:38:59.650
try creating because as we all know that. How agent works

01:38:59.650 --> 01:39:02.670
right we can try to like a create a agent so you can take

01:39:02.670 --> 01:39:06.590
inspiration from URI WhatsApp AI right. So we have URI API

01:39:06.590 --> 01:39:09.450
we have a URI WhatsApp AI we have a URI AI for a web

01:39:09.450 --> 01:39:12.490
interface. So maybe you can start creating it and this is

01:39:12.490 --> 01:39:15.330
something which every organization needs everyone right

01:39:15.330 --> 01:39:19.970
everyone. So where system should give a response this is one

01:39:19.970 --> 01:39:22.590
of the system which is which everyone can mention their

01:39:22.590 --> 01:39:25.490
resume the another one is this fine-tuning which I am

01:39:25.490 --> 01:39:28.890
talking about yeah. This is one thing that you can mention

01:39:28.890 --> 01:39:32.030
inside your resume many people have already mentioned it and

01:39:32.030 --> 01:39:35.330
yeah so you can go and you can explain inside your like an

01:39:35.330 --> 01:39:37.730
interview. So this is something which directly you can write

01:39:37.730 --> 01:39:38.270
into a resume.

01:39:45.560 --> 01:39:49.860
You have increases the interview limits as well yeah. So

01:39:49.860 --> 01:39:52.240
basically see if you are going to mention this kind of a

01:39:52.240 --> 01:39:54.940
project right so no one will cross question because every

01:39:54.940 --> 01:39:59.260
organization is doing this right even URI kind of a WhatsApp

01:39:59.260 --> 01:40:02.200
system every organization needs it whether they have it or

01:40:02.200 --> 01:40:04.660
not it is a different case because. Most of them is not

01:40:04.660 --> 01:40:07.860
having that kind of a system right otherwise every

01:40:07.860 --> 01:40:11.380
organization would love basically to expose their world to

01:40:11.380 --> 01:40:15.580
the outer world right but the only bottleneck is they are

01:40:15.580 --> 01:40:18.080
not able to create it right they are not able to create it.

01:40:18.180 --> 01:40:20.260
So if you are going to mention something like this into your

01:40:20.260 --> 01:40:23.580
resume believe me they will hire you without asking much of

01:40:23.580 --> 01:40:25.140
question this will happen.

01:40:31.490 --> 01:40:34.550
How your code is deciding where we get the information from

01:40:34.550 --> 01:40:37.190
the model Google or social site or you know okay. Are you

01:40:37.190 --> 01:40:40.350
going to learn that code as well. No no. See it is an agent

01:40:40.350 --> 01:40:45.030
right. Your entire generative AI session was based on what

01:40:45.030 --> 01:40:51.090
agent right even in it and what we have done created agent

01:40:51.090 --> 01:40:54.250
language chain what we have done created agent everywhere we

01:40:54.250 --> 01:40:57.170
have created agents itself right even in auto gen last class

01:40:57.170 --> 01:40:59.910
I believe on Saturday class right. So we have done what

01:40:59.910 --> 01:41:03.370
created agent. Now what is agent a smart brain right which

01:41:03.370 --> 01:41:05.810
can decide so technically agent is something which can

01:41:05.810 --> 01:41:10.490
understand my language NLP. We are calling agents over there

01:41:10.490 --> 01:41:14.630
and agent is having access of all the tool it is having

01:41:14.630 --> 01:41:17.930
access of your own database it is having access of a server

01:41:17.930 --> 01:41:20.650
API which can like go for the Google search and other APIs

01:41:20.650 --> 01:41:23.930
for a real time search it is having access of a speech to

01:41:23.930 --> 01:41:26.950
text it is having access of a image generation model so that

01:41:26.950 --> 01:41:30.950
it can understand so it can so if you will upload a image

01:41:30.950 --> 01:41:35.350
inside that it will generate the image part number one it

01:41:35.350 --> 01:41:38.690
can enhance the image part number two. And it can even

01:41:38.690 --> 01:41:41.230
understand the image means if you are going to ask a

01:41:41.230 --> 01:41:45.510
question that what is there inside that images right. It

01:41:45.510 --> 01:41:48.430
will try to even explain you the image. So even in image

01:41:48.430 --> 01:41:51.710
part we have three funnels if you will check this WhatsApp

01:41:51.710 --> 01:41:55.170
UD AI. So we have three funnels just for our images. So we

01:41:55.170 --> 01:41:57.850
are calling technically three different different agents or

01:41:57.850 --> 01:42:00.410
you can say a tool calling we are trying to do just for the

01:42:00.410 --> 01:42:03.410
image part. So agent is trying to understand that which tool

01:42:03.410 --> 01:42:05.890
I should call like enhancement tool I should call. I should

01:42:05.890 --> 01:42:08.930
call or maybe I should call basically like a generation tool

01:42:08.930 --> 01:42:11.450
or whether I should call basically a understanding tool

01:42:11.450 --> 01:42:14.510
image understanding tool and that tool is nothing but a

01:42:14.510 --> 01:42:18.170
function. This is what we understood in our last like a two

01:42:18.170 --> 01:42:21.930
three month of the classes. So it is a pure agentic system

01:42:21.930 --> 01:42:24.130
we have created.

01:42:32.390 --> 01:42:36.110
Can you give a bit idea about how you did this documentation

01:42:36.110 --> 01:42:37.690
you are talking about.

01:42:46.720 --> 01:42:50.080
So basically like first of all your project should be ready.

01:42:50.260 --> 01:42:53.480
Like once your project is ready then try to you know collect

01:42:53.480 --> 01:42:56.180
all the information that I am doing this this this this this

01:42:56.180 --> 01:42:58.680
with respect this this this. So I have created the rough

01:42:58.680 --> 01:43:03.120
sketch of this and then I have like given to a UD AI it has

01:43:03.120 --> 01:43:06.300
created the entire MD file for me basically. I have done

01:43:06.300 --> 01:43:09.260
little bit of I am coding as well over here but yes input is

01:43:09.260 --> 01:43:12.240
mine because based on my input it should do whatever you are

01:43:12.240 --> 01:43:15.100
able to see right. So complete input I have to give.

01:43:20.710 --> 01:43:24.810
How it can pick a slang is it like how it can because pick a

01:43:24.810 --> 01:43:26.770
slang. I am not able to understand your question if you can

01:43:26.770 --> 01:43:33.990
like I mean like if we can create it everyone can create it

01:43:33.990 --> 01:43:36.550
very simple right. In a tech world nothing is hidden.

01:43:40.090 --> 01:43:44.130
So if we can do it and I think everyone should do it and we

01:43:44.130 --> 01:43:47.950
are here for learning right. So obviously we should hide as

01:43:47.950 --> 01:43:52.490
less as possible if we have built something. So that

01:43:52.490 --> 01:43:55.010
everyone can. Tomorrow we will create a new things. Thank

01:43:55.010 --> 01:43:59.010
you. Right this this is not the end of the world right

01:43:59.010 --> 01:44:01.670
tomorrow we will create new project new things very simple

01:44:01.670 --> 01:44:07.470
we will discuss something new. But yeah try creating this

01:44:07.470 --> 01:44:11.750
right try creating this maybe after this URI project right

01:44:11.750 --> 01:44:15.470
after this URI project if you guys will be interested. So I

01:44:15.470 --> 01:44:18.450
will just try to like invite one of my developer into your

01:44:18.450 --> 01:44:24.290
class not now after your URI project. And then I will just

01:44:24.290 --> 01:44:28.510
ask him. Ask him to you know not him even personally I will

01:44:28.510 --> 01:44:31.910
do it I can do it to give you a walk through walk through of

01:44:31.910 --> 01:44:35.250
this kind of a system right that what is the agent which is

01:44:35.250 --> 01:44:38.170
running because majority of code we have written in a node

01:44:38.170 --> 01:44:43.150
and we have not used any automation tool it's a proper pure

01:44:43.150 --> 01:44:47.670
custom code. The reason was very very simple I love init in

01:44:47.670 --> 01:44:52.450
right but I don't love it in a production very simple. In

01:44:52.450 --> 01:44:56.430
production. I know like. Like it is not going to like give

01:44:56.430 --> 01:44:59.470
me the kind of a latency which I am looking for for this

01:44:59.470 --> 01:45:02.910
real time system. Yeah for batch system init is very good.

01:45:03.090 --> 01:45:06.010
So you can automate your task once in a while you can run it

01:45:06.010 --> 01:45:10.250
once in a day you can run it those kind of a tools. But I

01:45:10.250 --> 01:45:16.750
don't rely on tools for like a real time like a things like

01:45:16.750 --> 01:45:17.250
this one.

01:45:20.250 --> 01:45:23.630
In hyper stack we just run a data yeah just in a hyper stack

01:45:23.630 --> 01:45:29.120
just like a run this entire things. Notorious language in

01:45:29.120 --> 01:45:35.000
WhatsApp agent how the slang is picked up slang we don't

01:45:35.000 --> 01:45:37.560
have to do much because nowadays the model that we have

01:45:37.560 --> 01:45:41.300
right for example a GPT model itself it's it's like a pretty

01:45:41.300 --> 01:45:44.860
much like a mature. So anyhow whatever input that you are

01:45:44.860 --> 01:45:47.720
giving we are sending it to LLMs right. So if I'll send into

01:45:47.720 --> 01:45:51.000
chat GPT by default it will be able to discard all those

01:45:51.000 --> 01:45:57.140
like a slangs a plus. Yeah. Tone. So for toning basically we

01:45:57.140 --> 01:46:00.240
have done like a little bit of engineering over there so

01:46:00.240 --> 01:46:04.480
that you know if you are going to like talk to this

01:46:04.480 --> 01:46:07.720
particular one in your language maybe in a fun way so it

01:46:07.720 --> 01:46:10.980
will behave in a fun way. So we have done a little bit of

01:46:10.980 --> 01:46:15.760
prompting over there to be honest with everyone. And yeah so

01:46:15.760 --> 01:46:21.140
basically it is able to adopt your like tone. It is kind of

01:46:21.140 --> 01:46:23.740
an instruction we have given you can say a kind of

01:46:23.740 --> 01:46:26.080
instruction. Because LLM can understand everything right.

01:46:30.010 --> 01:46:33.170
Okay so fine guys what is the next step. So next step is

01:46:33.170 --> 01:46:37.070
just run build a model test it right test it into a docker

01:46:37.070 --> 01:46:41.950
because this is not the end of this chapter. So till this

01:46:41.950 --> 01:46:45.290
chapter we are able to build and do everything till docker.

01:46:46.790 --> 01:46:49.630
Almost similar kind of a things you will be able to see

01:46:49.630 --> 01:46:53.250
again when I will be talking about a URI API. And when I

01:46:53.250 --> 01:46:56.070
will be talking about a hosting inside a cloud platform.

01:46:56.070 --> 01:46:58.230
Which is a part of your syllabus as we will progress we will

01:46:58.230 --> 01:47:01.390
try to like you know do that we will do a cloud deployment.

01:47:01.850 --> 01:47:05.730
But in between at least test it. If you are not able to run

01:47:05.730 --> 01:47:08.630
it if you are not able to train it in your local system not

01:47:08.630 --> 01:47:11.970
an issue run it inside a hyper stack that is the reason I

01:47:11.970 --> 01:47:15.410
taught you hyper stack very simple right. I taught you like

01:47:15.410 --> 01:47:18.190
how to access a GPU so that you will be able to you know

01:47:18.190 --> 01:47:21.050
train any kind of a model and we all are aware about it. So

01:47:21.050 --> 01:47:24.210
just train it for one hour test it and then you know shut

01:47:24.210 --> 01:47:27.190
down your machine so that it is not going to cost you like a

01:47:27.190 --> 01:47:29.970
more than one one and half dollar on hyper stack or

01:47:29.970 --> 01:47:34.370
somewhere else yeah. I think it's worth spending one two

01:47:34.370 --> 01:47:37.110
dollar to see a training because that is something which you

01:47:37.110 --> 01:47:41.410
are going to write into your resume right. And then we will

01:47:41.410 --> 01:47:48.230
try to attach these APIs with our UI interface. Brief first

01:47:48.230 --> 01:47:51.030
November new batch yeah. So new batch we are going to start

01:47:51.030 --> 01:47:55.170
first November batch I think next week weekend we are going

01:47:55.170 --> 01:47:58.650
to do the inauguration of that batch. So those who will be

01:47:58.650 --> 01:48:01.250
interested they can join if you have some friends and family

01:48:01.250 --> 01:48:03.370
they would like to learn this entire generative way things

01:48:03.370 --> 01:48:06.970
they can like join. And obviously like syllabus is same but

01:48:06.970 --> 01:48:13.550
it's a live class so I mean like most of the time in my

01:48:13.550 --> 01:48:20.390
history so I barely repeat a project or examples. So I keep

01:48:20.390 --> 01:48:23.130
on giving a new new example so if you have a time maybe you

01:48:23.130 --> 01:48:26.850
spend those like a Saturday Sunday with me. Simple yeah

01:48:26.850 --> 01:48:29.670
sometime you just have to spend time that's it. So if you

01:48:29.670 --> 01:48:33.810
are free you can just come to class will this course we

01:48:33.810 --> 01:48:39.030
ended up no this course is going in its own like this one

01:48:39.030 --> 01:48:42.310
right timeline. So timing is different for both the batches

01:48:42.310 --> 01:48:45.650
this will go in its own timeline that will go in its own

01:48:45.650 --> 01:48:52.120
timeline is it a last class why do you think in that way we

01:48:52.120 --> 01:48:54.980
have not completed the syllabus right timing is not same. So

01:48:54.980 --> 01:48:57.740
that batch timing is four o'clock. And this is like morning

01:48:57.740 --> 01:49:01.320
nine o'clock. So timing is completely different there is no

01:49:01.320 --> 01:49:06.600
overlap at all so asking for example a system if we need to

01:49:06.600 --> 01:49:10.040
get some information from a user for user for a request how

01:49:10.040 --> 01:49:12.800
to handle that example create some request in a ticker

01:49:12.800 --> 01:49:15.840
system through a chatbot I need to get some input from a

01:49:15.840 --> 01:49:21.720
user user chatbot must get a mandatory data to move forward

01:49:21.720 --> 01:49:27.200
ok so here like we are able to like. You know register a

01:49:27.200 --> 01:49:29.940
feedback see I want to give a feedback so system has

01:49:29.940 --> 01:49:32.720
responded your feedback has been received and forward to our

01:49:32.720 --> 01:49:37.600
support team. Now so whenever you will start chatting we are

01:49:37.600 --> 01:49:40.560
not asking you for your number or anything right because

01:49:40.560 --> 01:49:42.800
your number and everything will be available inside a

01:49:42.800 --> 01:49:46.120
session. So whether you are registered with your own you are

01:49:46.120 --> 01:49:50.000
a paid user you are a non paid user everything is available

01:49:50.000 --> 01:49:54.300
with us because in your own system so we are like basically

01:49:54.300 --> 01:49:58.140
asking you for your mobile number. Right and even here then

01:49:58.140 --> 01:50:00.520
that was the reason so if you if you remember six months

01:50:00.520 --> 01:50:03.840
back so we were giving you two options one is a mail id

01:50:03.840 --> 01:50:08.420
option one was the mobile number option but now even though

01:50:08.420 --> 01:50:11.800
if you will log in through a mail id we are asking you for

01:50:11.800 --> 01:50:14.100
your mobile number because we were building this kind of a

01:50:14.100 --> 01:50:19.580
system since then the idea was very simple that here let us

01:50:19.580 --> 01:50:23.380
not ask like a more of detail again and again right let us

01:50:23.380 --> 01:50:26.280
make it clear and simple. So. So you will log in with your

01:50:26.280 --> 01:50:29.740
number and system so system will be able to check

01:50:29.740 --> 01:50:31.860
automatically that whether you are registered when you have

01:50:31.860 --> 01:50:33.900
registered from which country which location you have

01:50:33.900 --> 01:50:38.360
registered basically if you are going to like give a

01:50:38.360 --> 01:50:41.580
location to your like whatsapp so we will try to like pull

01:50:41.580 --> 01:50:45.180
all of those information and automatically system will be

01:50:45.180 --> 01:50:48.680
able to understand that okay and again so here so when you

01:50:48.680 --> 01:50:52.260
say feedback right it will log the feedback and when it is

01:50:52.260 --> 01:50:55.080
logging a feedback we know like which courses you have

01:50:55.080 --> 01:50:58.180
accessed. Basically everything everything like whatever

01:50:58.180 --> 01:51:01.420
interaction that you have done with system we can we can see

01:51:01.420 --> 01:51:03.700
it and then we can respond back to you that okay fine this

01:51:03.700 --> 01:51:06.960
was your query so let us try to resolve it or any other

01:51:06.960 --> 01:51:11.760
issues if you are facing so yeah this is how like we have

01:51:11.760 --> 01:51:14.980
done if you would like to pop up some sort of a forms in

01:51:14.980 --> 01:51:17.320
that case what you can do so we are sending this response

01:51:17.320 --> 01:51:21.200
right instead of sending this response pop up a form kind of

01:51:21.200 --> 01:51:23.300
a structure say that okay fine select this this this or

01:51:23.300 --> 01:51:26.600
maybe field this this this so as a form user will fill the

01:51:26.600 --> 01:51:29.780
data it will again go back and then like you know you will

01:51:29.780 --> 01:51:33.280
be able to capture it if you have to do it right in our

01:51:33.280 --> 01:51:36.220
cases it is not required and I think it is in most of the

01:51:36.220 --> 01:51:38.000
cases it is not required we do not have to take much of

01:51:38.000 --> 01:51:41.640
detail because if user is interacting with me I have already

01:51:41.640 --> 01:51:45.700
all those details even we observe in python teaching also

01:51:45.700 --> 01:51:48.280
the way teaching is different you serve for every exactly

01:51:48.280 --> 01:51:50.920
yeah so every class is not good it is a live class right

01:51:50.920 --> 01:51:56.020
that is a beauty of live classes by the way. So maybe just

01:51:56.020 --> 01:51:58.480
by spending time in a live class you will be able to learn a

01:51:58.480 --> 01:52:01.300
lot even if you are not doing something but still you are

01:52:01.300 --> 01:52:05.320
learning a lot. Are you using guardrail because I am

01:52:05.320 --> 01:52:09.040
scolding in whatsapp and asked to scold me back still

01:52:09.040 --> 01:52:10.620
responding in a very polite way.

01:52:14.690 --> 01:52:18.970
So guardrail instruction we have we have all also given so

01:52:18.970 --> 01:52:24.510
you can go and scold more and then let me know. Any fraud

01:52:24.510 --> 01:52:27.230
detection project? I think fraud detection project is

01:52:27.230 --> 01:52:29.370
already available you can go and check inside a project

01:52:29.370 --> 01:52:33.940
section so it is there I believe and we can ask you earlier

01:52:33.940 --> 01:52:37.620
exactly that is a beauty right of live classes yeah working

01:52:37.620 --> 01:52:41.480
on recording as well but yeah so because some people loves

01:52:41.480 --> 01:52:47.100
recording fine. So this is it guys this is it from my side

01:52:47.100 --> 01:52:51.260
by the way just try it test it and then let me know now what

01:52:51.260 --> 01:52:53.960
we are going to discuss tomorrow so tomorrow I am going to

01:52:53.960 --> 01:52:55.980
talk about a MCP by the way. What is MCP? MCP is a model

01:52:55.980 --> 01:53:02.840
context protocol. So one of the you can say kind of a rule

01:53:02.840 --> 01:53:07.540
right or it is a protocol protocol means rule by which you

01:53:07.540 --> 01:53:11.520
will be able to give an access of any system to your LLMs

01:53:11.520 --> 01:53:14.020
for example I can give an access of my gmail system my

01:53:14.020 --> 01:53:18.160
calendar or maybe my database access I can try to give to

01:53:18.160 --> 01:53:21.880
LLMs. So that depends upon the request depends upon the

01:53:21.880 --> 01:53:24.580
query depends upon the input so it will be able to go and

01:53:24.580 --> 01:53:27.600
you know select all those. Like tools automatically or

01:53:27.600 --> 01:53:30.200
servers automatically. So my gmail server might like a

01:53:30.200 --> 01:53:34.520
calendar server or maybe it will be able to access my entire

01:53:34.520 --> 01:53:37.920
databases and then it will be able to respond. So that is

01:53:37.920 --> 01:53:41.160
something that we are going to discuss believe me it is not

01:53:41.160 --> 01:53:44.300
going to be different or very different or you are not going

01:53:44.300 --> 01:53:46.940
to find it difficult at all you will find it like a very

01:53:46.940 --> 01:53:50.160
very easy right you will find it like we are just using one

01:53:50.160 --> 01:53:54.280
decorator MCP dot tool. And then like our functions are

01:53:54.280 --> 01:53:57.800
getting converted into MCP. That is that is something that

01:53:57.800 --> 01:54:01.800
you like it will be more like a fast API yeah so in case of

01:54:01.800 --> 01:54:04.940
fast API what we have done I think we all are well versed

01:54:04.940 --> 01:54:09.060
with fast API right guys yeah I think we all have a very

01:54:09.060 --> 01:54:14.580
good understanding about APIs right because this is API was

01:54:14.580 --> 01:54:17.540
the very first like a point where we have started class and

01:54:17.540 --> 01:54:21.980
till now we have used a lot right. So here API so at this

01:54:21.980 --> 01:54:24.320
root this function is available this is the meaning of API

01:54:24.320 --> 01:54:27.000
yeah. So similarly to 11 that is my one question which I

01:54:27.000 --> 01:54:27.000
really wanted to ask let us say a you need to do like a then

01:54:27.000 --> 01:54:30.700
it will be able to find out even inside MCPs so MCP at the

01:54:30.700 --> 01:54:33.080
rate tool and then like something will be exposed to the

01:54:33.080 --> 01:54:34.620
LLMs.

01:54:40.340 --> 01:54:43.680
How it is different from agent let us discuss that that

01:54:43.680 --> 01:54:46.360
question is going to be little bit interesting how it is

01:54:46.360 --> 01:54:48.640
different from the agent okay

01:54:50.790 --> 01:54:53.990
just as to tell you like a difference now so technically in

01:54:53.990 --> 01:54:58.210
case of MCPs you give a tool access means function access

01:54:58.210 --> 01:55:02.210
directly right and then LLMs like decide that okay fine so

01:55:02.210 --> 01:55:07.170
which tool it is supposed to like a call right in case of

01:55:07.170 --> 01:55:11.290
agent the starting entry point itself is like a going to be

01:55:11.290 --> 01:55:13.910
your prompt or whatever system configuration that you have

01:55:13.910 --> 01:55:20.550
given for a agent and then that prompt so more or less you

01:55:20.550 --> 01:55:26.010
will be able to see a similar kind of a flow but it is a

01:55:26.010 --> 01:55:29.730
different I would say it is different in a way that in case

01:55:29.730 --> 01:55:34.370
of MCPs I am exposing our functions tools even in agent we

01:55:34.370 --> 01:55:39.650
were using tools right so agent is a one single unit where

01:55:39.650 --> 01:55:43.770
it can hold a multiple tools and then it can hold a memory

01:55:43.770 --> 01:55:47.350
then it can hold a LLMs right and all together it is called

01:55:47.350 --> 01:55:51.390
as agent to make a decision automatically in case of tools

01:55:51.390 --> 01:55:53.950
it's a individual individual function access we try to

01:55:53.950 --> 01:55:59.730
attach that's a difference so the it's very different when

01:56:03.310 --> 01:56:06.970
is keyword searching getting keyword searching getting

01:56:06.970 --> 01:56:11.930
launched we are working on that so we are even like a you

01:56:11.930 --> 01:56:16.110
know working on a couple of like a project where you know we

01:56:16.110 --> 01:56:19.810
are trying to parse the entire lectures we have so many

01:56:19.810 --> 01:56:21.850
lectures so many live lectures so many like a recorded

01:56:21.850 --> 01:56:25.730
lecture in Hindi and English so we are even working on a

01:56:25.730 --> 01:56:26.270
project so

01:56:31.750 --> 01:56:34.670
basically like a conversation which has happened in that

01:56:34.670 --> 01:56:38.970
lecture and then like a whenever someone is going to like do

01:56:38.970 --> 01:56:43.930
a prompt so system will be able to you know identify that at

01:56:43.930 --> 01:56:48.870
which what time stamp this topic was taught and you will be

01:56:48.870 --> 01:56:52.690
able to get basically like even seconds and minutes that

01:56:52.690 --> 01:56:55.770
okay at this minute this second this topic was covered in

01:56:55.770 --> 01:56:59.510
this lecture so we are even working on that that

01:57:01.670 --> 01:57:07.370
it's a very like a intensive system intensive process that

01:57:07.370 --> 01:57:10.670
we are like a dealing with so we are just like a figuring

01:57:10.670 --> 01:57:15.130
out some optimized approach for that one in which lecture

01:57:15.130 --> 01:57:20.270
hyper stack is covered also as we it said not available so

01:57:20.270 --> 01:57:24.210
hyper stack is covered in your live lectures right but yeah

01:57:24.210 --> 01:57:27.590
so see it can understand only as of now the capability that

01:57:27.590 --> 01:57:30.510
we have built so basically it can understand only the main

01:57:30.510 --> 01:57:34.390
main headings and the lectures point it can't go into the

01:57:34.390 --> 01:57:38.450
lecture we are working on those features as well so

01:57:40.850 --> 01:57:43.590
I wish to learn ML flow ML flow I think lectures already

01:57:43.590 --> 01:57:46.770
available from BAPI yeah full course is available for ML

01:57:46.770 --> 01:57:48.690
flow I believe okay

01:57:54.160 --> 01:57:57.580
so fine guys this is it from my side tomorrow like I said

01:57:57.580 --> 01:58:01.800
agenda is clear I'm going to teach you like a MCP so what is

01:58:01.800 --> 01:58:04.820
MCP how we can create it in a practical manner and like I

01:58:04.820 --> 01:58:07.360
said you will love it you will enjoy it today's code I have

01:58:07.360 --> 01:58:10.900
already uploaded inside your previous lectures so if you

01:58:10.900 --> 01:58:10.900
have any questions please feel free to ask them in the

01:58:10.900 --> 01:58:12.000
comment section and we will try to build a model test it

01:58:12.000 --> 01:58:15.180
leave it for now because again we are going to open up this

01:58:15.180 --> 01:58:19.580
particular chapter so with that that's it guys from my side

01:58:19.580 --> 01:58:22.160
now if you have any kind of a question so please raise your

01:58:22.160 --> 01:58:26.820
hand we can unmute each other and then we can talk yeah

01:58:26.820 --> 01:58:29.920
Mohit go ahead Sumit go ahead with the question if anyone of

01:58:29.920 --> 01:58:35.060
you have a question please raise your hand guys yeah yeah

01:58:35.060 --> 01:58:40.300
please go ahead anyone yeah please go ahead yes sir first of

01:58:40.300 --> 01:58:42.320
all I would like to thank you for your time and uh happy

01:58:42.320 --> 01:58:46.540
diwali and uh yeah happy bal interview now chat is coming so

01:58:46.540 --> 01:58:53.180
yes yes yes that is coming and from Monday I think yeah yeah

01:58:53.180 --> 01:58:58.100
so uh sir I just wanted to one questions actually I have the

01:58:58.880 --> 01:59:03.680
puzzled in the something that one time you who asked that

01:59:03.680 --> 01:59:08.640
the internal is not for the production nice point of view

01:59:08.640 --> 01:59:10.800
it's a tool no no that's not a fight Amazon website though

01:59:10.800 --> 01:59:10.800
all the ways to invest in new product or to wouldn't do it

01:59:10.800 --> 01:59:12.840
and assign us anything useless it can So I said that that if

01:59:12.840 --> 01:59:16.820
you are looking for a real time processing, for example, if

01:59:16.820 --> 01:59:18.880
I'm building a chat system, which should give me a response

01:59:18.880 --> 01:59:22.880
immediately, right? In that case, I'll not recommend any

01:59:22.880 --> 01:59:25.440
framework, any prebuilt framework, any kind of a framework.

01:59:25.600 --> 01:59:28.480
But let's suppose if I'm trying to do an automation, mail

01:59:28.480 --> 01:59:31.320
automation, let's suppose, right? So where it should go and

01:59:31.320 --> 01:59:33.520
it should, you know, every hour it should go to my mail,

01:59:33.660 --> 01:59:37.060
search, like access all the mails and then prepare a

01:59:37.060 --> 01:59:41.600
response, respond back to user. In that case, any kind of a

01:59:41.600 --> 01:59:44.620
tool is good because time is not going to be the constraint.

01:59:44.860 --> 01:59:47.420
I'm not looking for a response in a fraction of second,

01:59:47.620 --> 01:59:51.560
right? So over there, yes, those orchestration tools are

01:59:51.560 --> 01:59:54.840
very good, right? Because it's see why I'm telling you very

01:59:54.840 --> 01:59:57.180
good. The reason is very simple. It's easy to implement,

01:59:57.380 --> 01:59:59.960
right? Easy to implement, easy to automate. And obviously

01:59:59.960 --> 02:00:02.220
it's a day to day task. It's a redundant task, right? So

02:00:02.220 --> 02:00:04.620
every day you have to go and check your mail, maybe check

02:00:04.620 --> 02:00:07.380
your calendar, maybe check our minutes of meetings over

02:00:07.380 --> 02:00:10.720
there, right? Maybe check a lead funnel or maybe check your

02:00:10.720 --> 02:00:14.480
finances or maybe check a reporting. Now in that, so over

02:00:14.480 --> 02:00:16.680
there, time is not a constraint. Time is not a constraint

02:00:16.680 --> 02:00:19.600
means even if it is going to give me a response five minutes

02:00:19.600 --> 02:00:23.740
later, I can like a bear with it, right? But the URI

02:00:23.740 --> 02:00:26.440
WhatsApp system that we have created, if system will not

02:00:26.440 --> 02:00:28.560
give you a response within 10 seconds or maybe less than

02:00:28.560 --> 02:00:33.420
that, you will, you will feel frustrated, right? So in this

02:00:33.420 --> 02:00:36.460
kind of a system, obviously I'm using agent and even in

02:00:36.460 --> 02:00:39.160
batch kind of a system, I will be using agent. But here I

02:00:39.160 --> 02:00:44.560
can't use this orchestration framework because latency wise,

02:00:44.660 --> 02:00:48.440
I can't control much. In my code, I can control, right? It's

02:00:48.440 --> 02:00:51.320
completely under my control. I can optimize it till any

02:00:51.320 --> 02:00:54.540
extent I want. But in case of any 10, I won't be having much

02:00:54.540 --> 02:00:57.400
of flexibility. So that is the reason I said that, that in

02:00:57.400 --> 02:00:59.800
real time, wherever time is a constant, latency is a

02:00:59.800 --> 02:01:04.300
constraint, no, but yeah, wherever I just have to go with

02:01:04.300 --> 02:01:06.840
the automation. So day to day task automation and all those

02:01:06.840 --> 02:01:09.760
things. It's very, very good. Right. Understood, sir.

02:01:09.980 --> 02:01:13.260
Because I have, I just by creating all these things,

02:01:13.280 --> 02:01:18.120
according to my organizations that like a GM level of person

02:01:18.120 --> 02:01:23.240
is wanted to some automate his task day to day, per day

02:01:23.240 --> 02:01:27.580
task. So according to that, we can use the NATN, but like a

02:01:27.580 --> 02:01:32.180
URI point of view that you have just explained us. So in

02:01:32.180 --> 02:01:35.800
that case, how we can handle, because in that case, we can't

02:01:35.800 --> 02:01:42.400
use the NATN, right? So from where we are using the multiple

02:01:42.400 --> 02:01:46.640
models, and apart from that, we are using the social sites

02:01:46.640 --> 02:01:51.400
and Google sites as well, that we can, we want to provide

02:01:51.400 --> 02:01:56.920
the information. So how we can handle that. So basically,

02:01:57.000 --> 02:01:59.340
this is the reason we talked about a framework, for example,

02:01:59.460 --> 02:02:01.820
land graph, we have talked about Lang chain, we have talked

02:02:01.820 --> 02:02:04.680
about autogen, we have talked about crew AI, we have talked

02:02:04.680 --> 02:02:07.940
about right now. These. These are the frameworks. So which

02:02:07.940 --> 02:02:10.360
is going to help me out in terms of building an agent,

02:02:10.500 --> 02:02:15.600
coding wise. Okay. Yeah, that is the reason. So we have gone

02:02:15.600 --> 02:02:18.880
through all of those classes, right? So why, why we have

02:02:18.880 --> 02:02:21.040
gone through those classes, why we talked about a Lang

02:02:21.040 --> 02:02:23.200
chain, why we talked about a land graph, why we talked about

02:02:23.200 --> 02:02:25.940
a crew AI, why we talked about the autogen. We have done a

02:02:25.940 --> 02:02:27.960
lot of coding, right? We have done a lot of coding over

02:02:27.960 --> 02:02:32.920
there. You mean that we can use the agent with the help of

02:02:32.920 --> 02:02:35.920
coding, right? Coding. Yes. Yes. Yeah. This is what I said,

02:02:36.020 --> 02:02:39.260
right? Because over there I will be having the ability of

02:02:39.260 --> 02:02:41.580
doing optimization. That is the only flexibility which I

02:02:41.580 --> 02:02:44.360
need, right? I will be having control over each and every

02:02:44.360 --> 02:02:47.360
piece of the code, right? And then I will be able to

02:02:47.360 --> 02:02:49.060
understand, I will be able to debug, I will be able to

02:02:49.060 --> 02:02:52.220
optimize it. I will be able to check that which function is

02:02:52.220 --> 02:02:55.300
taking more time. I can just focus on that piece of the

02:02:55.300 --> 02:02:58.920
function. I can try to optimize it, right? Or maybe I can

02:02:58.920 --> 02:03:02.080
try to, you know, host it somewhere where like our instances

02:03:02.080 --> 02:03:05.900
is very, very high, right? So that is the reason we talked

02:03:05.900 --> 02:03:08.600
about those coding framework. And this is the reason I'm

02:03:08.600 --> 02:03:12.440
telling you that wherever you have a latency issue, go with

02:03:12.440 --> 02:03:15.240
the coding framework, never go with like a pre-built

02:03:15.240 --> 02:03:17.760
orchestration and it is orchestration framework, drag and

02:03:17.760 --> 02:03:22.580
drop, build it. So it's not good for a low latency system.

02:03:23.200 --> 02:03:25.840
Understood, sir. Thank you. Thank you so much. Yeah. Yeah.

02:03:26.140 --> 02:03:28.780
Does the race you asking me question does latency comes from

02:03:28.780 --> 02:03:31.360
a model response or third party API is called. How framework

02:03:31.360 --> 02:03:35.300
is responsible for a. Yeah. Yeah. Latency. See, but it

02:03:35.300 --> 02:03:38.080
depends upon multiple things, not just, I would say like a

02:03:38.080 --> 02:03:41.680
model. Yeah. Model is one of the part. For example, let's

02:03:41.680 --> 02:03:45.240
suppose I'm not using my self hosted model. I'm using model

02:03:45.240 --> 02:03:49.180
from open AI, right? So again, if I'm like a filing a

02:03:49.180 --> 02:03:52.300
queries that depends upon open AI response. So if open AI

02:03:52.300 --> 02:03:54.720
claiming that, that, okay, fine for this much token, I'm

02:03:54.720 --> 02:03:56.980
going to take this much of time. So I have to wait for that

02:03:56.980 --> 02:04:01.980
part number one, part number two. So basically even before

02:04:01.980 --> 02:04:04.580
hitting that model. So let's suppose I'm trying to hit some

02:04:04.580 --> 02:04:06.940
other model. So which is trying to make a decision for me

02:04:06.940 --> 02:04:08.760
decision for me, that what I should do, whether I should

02:04:08.760 --> 02:04:11.540
generate a response, I should go to the model. I should go

02:04:11.540 --> 02:04:14.080
to the real time search model, something like this. So

02:04:14.080 --> 02:04:16.900
again, one bottleneck, you will be able to find out even at

02:04:16.900 --> 02:04:21.900
a, that a particular like a places other bottleneck is going

02:04:21.900 --> 02:04:25.880
to be. So wherever you are hosting your own server, right?

02:04:25.980 --> 02:04:30.660
Your own server. So let's suppose it is going to be good for

02:04:30.660 --> 02:04:32.880
maybe first hundred user. But let's suppose immediately.

02:04:32.960 --> 02:04:35.480
Okay. So basically a thousand user will hit. So it is

02:04:35.480 --> 02:04:37.700
supposed to generate a response for everyone. Now if that

02:04:37.700 --> 02:04:41.580
server is not able to handle all of these traffic, all of

02:04:41.580 --> 02:04:44.480
these requests, so that would be your main blocking point. I

02:04:44.480 --> 02:04:47.420
can say choking point for you and you have to like basically

02:04:47.420 --> 02:04:51.560
handle it, uh, in our auto skill, uh, basically more. So it

02:04:51.560 --> 02:04:54.940
comes, latency comes from everywhere. It starts with my

02:04:54.940 --> 02:04:59.080
system, which is like a, like a routing the query, routing

02:04:59.080 --> 02:05:03.320
the request. It even comes from the model. Again, model

02:05:03.320 --> 02:05:06.800
wise, two things, proprietary model or self-hosted model. So

02:05:06.800 --> 02:05:09.780
if I have used, I'm using self-hosted model. So again, I

02:05:09.780 --> 02:05:13.100
have to make my system, uh, like, uh, I have to build my

02:05:13.100 --> 02:05:17.520
system in such a way that that, uh, within my accepted

02:05:17.520 --> 02:05:20.720
latency, it should give me a response, right? And that

02:05:20.720 --> 02:05:23.020
accepted latency can be, can depends upon the business.

02:05:23.100 --> 02:05:26.820
Otherwise I can use and, uh, use the proprietary model and

02:05:26.820 --> 02:05:30.880
how framework is responsible for the latency. Yeah. Now. So

02:05:30.880 --> 02:05:33.360
coming to the framework part. So let's suppose we are using

02:05:33.360 --> 02:05:36.140
Langchain, Langraph, Lama indexes, a crew AI, or maybe

02:05:36.140 --> 02:05:39.380
Autogen. Right? Now these are the framework. These are what?

02:05:39.440 --> 02:05:42.440
These are basically orchestrated framework for an agent.

02:05:42.580 --> 02:05:45.980
These framework just helped me out in terms of creating the

02:05:45.980 --> 02:05:52.520
agents. Simple. Right? Uh, now, so if I'm trying to build an

02:05:52.520 --> 02:05:56.820
agent, right? And uh, let's suppose that the agent, which is

02:05:56.820 --> 02:05:58.900
trying to call another tool, because at the end of the day,

02:05:58.940 --> 02:06:01.440
when I'm building an agent, right? So nothing is good.

02:06:01.440 --> 02:06:04.740
Nothing is a bad kind of a situation. So let's suppose if

02:06:04.740 --> 02:06:07.480
I'm trying to use maybe a Langchain and I'm trying to use a

02:06:07.480 --> 02:06:09.680
crew AI. So it will end up calling a multiple different,

02:06:09.740 --> 02:06:15.320
different kind of a tool. Now the way these frameworks are

02:06:15.320 --> 02:06:18.120
helped or the way this frameworks are helping me out in

02:06:18.120 --> 02:06:20.680
terms of creating a tool, every tool will be having its own

02:06:20.680 --> 02:06:23.440
latency or like whenever you are trying to create an agent.

02:06:23.500 --> 02:06:27.920
So it will be having its own latency. So yeah. Uh, but yeah,

02:06:27.980 --> 02:06:30.540
uh, if I'll talk about the framework. So. So I believe all

02:06:30.540 --> 02:06:35.280
the frameworks, uh, is technically calling the LLMs, I would

02:06:35.280 --> 02:06:39.420
say. So mostly, uh, your frameworks are not going to give

02:06:39.420 --> 02:06:43.500
you much of pain, uh, but yeah, maybe your models are going

02:06:43.500 --> 02:06:46.380
to give you, so I can try to use a crew. I can try to use

02:06:46.380 --> 02:06:50.560
maybe, uh, uh, like a Langchain to create a agents, almost

02:06:50.560 --> 02:06:53.820
both are going to give you a similar kind of a latency. But

02:06:53.820 --> 02:06:56.540
again, we, we do a lot of like a testing even over there. So

02:06:56.540 --> 02:07:02.380
for example, if I have built some agent in Langchain. So the

02:07:02.380 --> 02:07:05.600
way it is trying to route or it is trying to like invoke the

02:07:05.600 --> 02:07:09.260
agent that matters a lot and how much optimized their code

02:07:09.260 --> 02:07:12.860
is. So for example, if I'm trying to use a Langchain, right,

02:07:12.940 --> 02:07:16.660
and maybe their code is not that great or optimized in terms

02:07:16.660 --> 02:07:19.520
of building an agent, it will end up giving me a lot of

02:07:19.520 --> 02:07:22.480
latencies, but still I will be having control. This is what

02:07:22.480 --> 02:07:25.180
I was trying to say. So most of the time when we try to

02:07:25.180 --> 02:07:28.420
develop this kind of a project, what we do, we try to modify

02:07:28.420 --> 02:07:31.840
a couple of their functions. As well, you are anyhow doing

02:07:31.840 --> 02:07:34.420
pip install, right, pip install when you are doing it simply

02:07:34.420 --> 02:07:36.160
means what you are downloading their package, you're

02:07:36.160 --> 02:07:39.200
downloading their Python file. And most of the time, most of

02:07:39.200 --> 02:07:41.520
the time you will find out that when we are creating a very

02:07:41.520 --> 02:07:44.320
optimized project. So we are looking into their libraries

02:07:44.320 --> 02:07:47.880
and we are trying to optimize their library itself. And this

02:07:47.880 --> 02:07:51.980
is the beauty of using a code. So yeah, this is the place

02:07:51.980 --> 02:07:54.820
where you can optimize, I believe, if I'm able to give you

02:07:54.820 --> 02:07:57.880
an answer. Yeah. Next question is like, uh, Asit, go ahead

02:07:57.880 --> 02:08:03.500
please. Okay. Sorry. Yeah. So, uh, yeah. Okay. Happy Diwali.

02:08:03.660 --> 02:08:07.940
Uh, so happy Diwali. Be late in Diwali. Yeah. So, and, and,

02:08:07.940 --> 02:08:11.360
uh, it was a, uh, really, it was, uh, good on this, um,

02:08:11.440 --> 02:08:15.640
discussions in the last. Uh, right. So, uh, yeah. So my

02:08:15.640 --> 02:08:21.100
question is, uh, um, so don't sue, uh, when we host a model,

02:08:21.260 --> 02:08:25.600
right. So, uh, what is the approach to how you choose this

02:08:25.600 --> 02:08:29.360
model to, to fine tune it? Right. So what are the states we

02:08:29.360 --> 02:08:33.920
need to look into, uh, before choosing a model for either or

02:08:34.420 --> 02:08:38.280
fine tuning? Okay. So, uh, I think I discussed about this in

02:08:38.280 --> 02:08:41.300
my previous class, but let me read the same thing. So see,

02:08:41.380 --> 02:08:46.260
uh, how do we choose a particular model? So it depends upon

02:08:46.260 --> 02:08:50.340
a task that I have in my hand, for example, so let's suppose

02:08:50.340 --> 02:08:54.260
I have a task where I have to train with my simple document

02:08:54.260 --> 02:08:56.620
or data, which is available to me. And model should give me

02:08:56.620 --> 02:08:59.620
a response, right. And that too in English or maybe in

02:08:59.620 --> 02:09:02.820
other, other languages. So I'll go, I'll try to select that.

02:09:02.880 --> 02:09:08.620
Okay. Uh, which model is trying to fulfill my bare minimum

02:09:08.620 --> 02:09:11.740
requirement, bare minimum requirements, language, uh, plus

02:09:11.740 --> 02:09:14.980
the model pre-training, which has been done on what kind of

02:09:14.980 --> 02:09:17.880
a data. So for example, if I'm trying to train a model for

02:09:17.880 --> 02:09:22.100
finance, right, I'll prefer to go and choose some model,

02:09:22.160 --> 02:09:24.980
which is an open source model in a market, which has been

02:09:24.980 --> 02:09:27.800
trained, or which has seen a finance kind of a data, or

02:09:27.800 --> 02:09:30.380
which has seen a healthcare kind of a data, which has seen

02:09:30.380 --> 02:09:34.700
similar kind of a data. If that model is available, well and

02:09:34.700 --> 02:09:38.720
good, I'll go and pick and choose that model. Otherwise I'll

02:09:38.720 --> 02:09:41.880
try to pick and choose my second approach will be, I'll try,

02:09:41.940 --> 02:09:45.600
I'll go and try to choose a model which is having a very

02:09:45.600 --> 02:09:48.940
good understanding about a, just a language, right. Maybe if

02:09:48.940 --> 02:09:51.060
I'm trying to build for English, so obviously English

02:09:51.060 --> 02:09:54.180
language, Arabic. So Arabic language is Hindi. So Hindi

02:09:54.180 --> 02:09:57.240
languages. So that at least I understand the language. And

02:09:57.240 --> 02:10:00.120
then eventually I will be able to give, uh, like, uh, you

02:10:00.120 --> 02:10:02.900
know, my data and it will be able to interpret my data set.

02:10:03.400 --> 02:10:07.740
That's a second, uh, criteria, third criteria. So let's

02:10:07.740 --> 02:10:11.400
suppose if I'm trying to like, you know, uh, train a model

02:10:11.400 --> 02:10:15.680
for my HR policies, max to max in any company, even if it is

02:10:15.680 --> 02:10:18.720
very, very big company, right. How many HR documents will

02:10:18.720 --> 02:10:21.840
exist or how many HR policies will exist thousand, 2000,

02:10:22.060 --> 02:10:25.400
right. And per document wise. There will be like five pages,

02:10:25.520 --> 02:10:28.460
10 pages, max to max, right. Not more than that. Even if

02:10:28.460 --> 02:10:31.680
I'll talk about a world's biggest company, right. HR policy

02:10:31.680 --> 02:10:33.560
is going to be same for every employees and departments.

02:10:33.760 --> 02:10:36.260
Right. And obviously like thousand, let's suppose thousand

02:10:36.260 --> 02:10:39.920
policies we have right now in that situation, if I'll go and

02:10:39.920 --> 02:10:42.900
select OSS 120 billion parameter model, now that is not

02:10:42.900 --> 02:10:45.760
going to be a wise choice. The reason is I will end up

02:10:45.760 --> 02:10:48.560
burning money at the time of training. Right. And forget

02:10:48.560 --> 02:10:51.980
about that. Training is just a one time job inferencing

02:10:51.980 --> 02:10:53.760
because that is something which will happen, like

02:10:53.760 --> 02:10:57.680
frequently, right. So again, that is not an ideal approach.

02:10:57.800 --> 02:11:01.040
So I should always like, uh, take a decision that, okay,

02:11:01.100 --> 02:11:05.220
this model is having a bare the minimum number of parameter.

02:11:05.420 --> 02:11:08.380
It understands all of my data. That's the reason. So I've

02:11:08.380 --> 02:11:11.440
used tiny model over here. Right. And it's not like that is

02:11:11.440 --> 02:11:14.160
small 1.1 billion parameter we are talking about, which is

02:11:14.160 --> 02:11:17.100
more than enough, which is more than sufficient. Right. And,

02:11:17.200 --> 02:11:21.420
uh, for kind of a data, which I have taken. So language

02:11:21.420 --> 02:11:23.600
understanding of the model. Pre-understanding. Size of the

02:11:23.600 --> 02:11:27.260
model, size of the model basically matters a lot. Uh, what

02:11:27.260 --> 02:11:30.340
kind of a task that we have in our hand? So let's suppose if

02:11:30.340 --> 02:11:33.200
we are like, uh, trying to build just a HR finance kind of a

02:11:33.200 --> 02:11:36.160
model, not much is required, but let's suppose if I'm trying

02:11:36.160 --> 02:11:40.480
to build a model for a teaching purpose, right. Complete

02:11:40.480 --> 02:11:43.420
ITJU teaching or need to teaching or maybe UPSC teaching or

02:11:43.420 --> 02:11:46.880
something like that. Then in that case, obviously I need a

02:11:46.880 --> 02:11:51.180
very, very powerful model because teaching is a very, very

02:11:51.180 --> 02:11:55.020
broad, uh, things in itself. So these are the initial

02:11:55.020 --> 02:11:58.740
criteria that we should like, uh, go ahead and then we

02:11:58.740 --> 02:12:01.040
should check the trade off and then we should like, uh, you

02:12:01.040 --> 02:12:03.700
know, select the models, but yeah, language should always be

02:12:03.700 --> 02:12:08.400
the very first constraint, which I have seen. Okay. Okay.

02:12:08.500 --> 02:12:12.860
And, and, uh, one more is, uh, so, uh, um, here for the

02:12:12.860 --> 02:12:15.560
different, different domain, you change the different model

02:12:15.560 --> 02:12:19.780
or, uh, one model you put all the, I have used the same

02:12:19.780 --> 02:12:23.260
model actually. Okay. Okay. I think my screen is visible.

02:12:23.360 --> 02:12:28.360
Right. Yeah. So if you it's visible, right. Huh? Yeah. So

02:12:28.360 --> 02:12:30.880
everywhere you will be able to see only this model. Okay.

02:12:31.160 --> 02:12:33.620
Yeah. Tiny Lama. I have used basically as a Facebook, one of

02:12:33.620 --> 02:12:37.960
the best model, one of the like a moderated kind of a model

02:12:37.960 --> 02:12:41.360
you can say, uh, which is neither bad, neither good, I would

02:12:41.360 --> 02:12:44.340
say, but yeah, it's a, it's a, one of the good one to like a

02:12:44.340 --> 02:12:47.600
train. So I have used this model every day. Yeah. So, so

02:12:47.600 --> 02:12:50.260
it's a, it created a five model then, right. So for five,

02:12:50.300 --> 02:12:52.620
technically five models. Okay. So I can create a five model,

02:12:52.620 --> 02:12:57.080
20 model, like whatever I want, I can create it. Yeah. Okay.

02:12:57.360 --> 02:13:01.140
Oh yeah. Got it. Thank you. Oh, yeah. It's actually a very

02:13:01.140 --> 02:13:04.520
interesting and, uh, so very, uh, learning, uh, session

02:13:04.520 --> 02:13:07.460
today. This is what we do in real time also. So even if you

02:13:07.460 --> 02:13:10.200
will go and train, uh, let's suppose today itself for your

02:13:10.200 --> 02:13:13.500
organization, right. You can use this code directly. You

02:13:13.500 --> 02:13:15.840
don't have to like do anything. And if you would like to

02:13:15.840 --> 02:13:17.980
change the model, change the model. I mean, like you just

02:13:17.980 --> 02:13:20.880
have to copy the ID of the model from hugging phase because

02:13:20.880 --> 02:13:23.920
every model is available in hugging phase. Right. So what do

02:13:23.920 --> 02:13:26.380
you have to do? Let's suppose if you are planning to choose

02:13:26.380 --> 02:13:30.360
a different model, right. So this is the model ID I have.

02:13:30.420 --> 02:13:32.680
Uh, this is the model ID, which I have taken from hugging

02:13:32.680 --> 02:13:35.540
phase. Right. You will just have to change this one. Let's

02:13:35.540 --> 02:13:38.240
suppose you would like to train with the GPT OSS for

02:13:38.240 --> 02:13:46.920
example, um, Google, um, hugging phase. So let's suppose if

02:13:46.920 --> 02:13:50.700
I would like to, you know, uh, train with, uh, basically GPT

02:13:50.700 --> 02:13:53.740
OSS. So. Okay.

02:13:58.580 --> 02:14:01.820
Yeah. GPT OSS 20 B or something. So don't have to do

02:14:01.820 --> 02:14:05.520
anything. Just copy this, this ID, model ID. Right. It will

02:14:05.520 --> 02:14:08.200
download and we'll start training it. So this is how simple

02:14:08.200 --> 02:14:11.960
it is. And then, then if you have to change the data, then

02:14:11.960 --> 02:14:14.580
just go here, change the data. I have already done the

02:14:14.580 --> 02:14:19.060
mapping for that. Yeah. Hmm. And one final question is, uh,

02:14:19.140 --> 02:14:23.040
so, uh, that is, we, we play in the model with our dirt,

02:14:23.120 --> 02:14:27.360
right. But yeah, also, um, if we, uh, obtain, uh, uh, uh,

02:14:27.360 --> 02:14:28.240
uh, uh, uh, uh, uh, uh, uh, uh, uh, uh, build a model for a

02:14:28.240 --> 02:14:30.740
certain kind of task for a, for a person and setting for or

02:14:30.740 --> 02:14:35.800
for summarization for, um, translations. So then the kind of

02:14:35.800 --> 02:14:41.740
data sets we use to do, um, um, as far as, as far as for the

02:14:41.740 --> 02:14:45.580
task you have to prepare. Ah, okay. Yeah. So simple as, as

02:14:45.580 --> 02:14:49.880
for the task you'll have to prepare. Yeah. Okay. Thank you.

02:14:49.880 --> 02:14:53.920
Yeah. Yeah. Thank you. Parameter also will change. Right. If

02:14:53.920 --> 02:14:57.260
I, if I change the model, uh, no, basically parameter is. is

02:14:57.260 --> 02:15:00.300
with respect to most of the training, especially if I talk

02:15:00.300 --> 02:15:02.240
about the training parameters, training parameter is with

02:15:02.240 --> 02:15:04.460
respect to a kind of a training that we are doing, LoRa,

02:15:04.560 --> 02:15:07.400
Quora, pre-fit of full fine tuning. So that is not going to

02:15:07.400 --> 02:15:09.740
change. For example, Epoch, batch size, learning rate, beta,

02:15:09.880 --> 02:15:12.900
these are basically like independent of the model parameter.

02:15:13.220 --> 02:15:16.460
Yeah. So this doesn't depends upon the model because for any

02:15:16.460 --> 02:15:19.540
model is what network, right? So for any network, we have to

02:15:19.540 --> 02:15:20.140
set these things.

02:15:24.750 --> 02:15:28.050
Yeah. Dhruv, Sumit, Sumit has raised it, yeah, Sumit, go

02:15:28.050 --> 02:15:32.290
ahead. First of all, Happy Diwali related on the Happy

02:15:32.290 --> 02:15:37.190
Chhattopadhyay, sir. Yeah, me too. So, sir, actually, I was

02:15:37.190 --> 02:15:41.410
trying to like building RAG and like both fine tuning from

02:15:41.410 --> 02:15:44.710
the last batch. The issue which I'm facing that the

02:15:44.710 --> 02:15:50.970
retrieval IQA that is was like, deprecated it from 0.1.7.

02:15:51.580 --> 02:15:54.830
Now langchain divided into three core part like langchain,

02:15:54.970 --> 02:15:59.110
like community and core. So in that case, Even I am checked

02:15:59.110 --> 02:16:02.670
in your coding as well in API of the lan chain, there is a

02:16:02.670 --> 02:16:07.410
code specific for LCAL compatibility. Yeah, LCAL. Langchain

02:16:07.410 --> 02:16:10.450
expression language. Yeah. Yes. So even I tried that one,

02:16:10.490 --> 02:16:14.750
but still I'm facing multiple issues because if I upgrade to

02:16:14.750 --> 02:16:16.650
the latest one. I think it should work. I have, I have, I

02:16:16.650 --> 02:16:20.850
think in some class I taught that recently, LCAL because

02:16:20.850 --> 02:16:23.970
this keeps on changing, right? So maybe whatever you're

02:16:23.970 --> 02:16:25.930
working this month, maybe next month, this will not work.

02:16:26.030 --> 02:16:28.510
There is a chance. With the latest installation. So I have

02:16:28.510 --> 02:16:31.390
to go back to the previous, but yeah, just tell me what is

02:16:31.390 --> 02:16:34.150
the issue you're facing. Maybe I can look into it. Like

02:16:34.150 --> 02:16:37.630
generally, if, if like, if some kind of issue happens, so if

02:16:37.630 --> 02:16:41.870
we want to do any like old classes, then still we have to go

02:16:41.870 --> 02:16:45.210
with the older, go with the older installation. Simple. Just

02:16:45.210 --> 02:16:47.890
change the version. So whenever you're doing pip, just

02:16:47.890 --> 02:16:50.630
change the version. It'll work as it is because it doesn't

02:16:50.630 --> 02:16:53.010
matter what version they have released, but that previous

02:16:53.010 --> 02:16:56.090
version will be available. Okay. Second thing. Second thing

02:16:56.090 --> 02:16:59.730
in the last, in the last batch of the fine tuning, most of

02:16:59.730 --> 02:17:04.230
the dependency in a requirement of .txt are taking from your

02:17:04.230 --> 02:17:07.890
own system. So the problem I'm facing that whenever I am

02:17:07.890 --> 02:17:10.650
like installing that one, it asking me for that particular

02:17:10.650 --> 02:17:13.330
file, which is not in my file. So, and even I'm not getting

02:17:13.330 --> 02:17:15.950
that particular version, which component has been installed.

02:17:16.310 --> 02:17:19.430
So if I can get it, then only I can set it up. In

02:17:19.430 --> 02:17:23.630
requirement.txt file for fine tuning. Yes. I think you have

02:17:23.630 --> 02:17:28.210
freeze the requirement. I don't remember, I take so many

02:17:28.210 --> 02:17:33.290
classes, so I, I mean, fine tuning wise, just, just use this

02:17:33.290 --> 02:17:37.210
one. So you are saying that here. So I have, so in this

02:17:37.210 --> 02:17:41.330
requirement, very less, if we could check the last lecture,

02:17:41.450 --> 02:17:49.410
the last batch with NLP. Okay. So maybe I have done pip

02:17:49.410 --> 02:17:51.450
freeze at that point of a time. Yes. Yes.

02:17:55.130 --> 02:17:58.710
It was, it was, it was, it was unnecessary, like when we do

02:17:58.710 --> 02:18:01.630
pip freeze. So basically whatever installation is done in

02:18:01.630 --> 02:18:05.230
my, that current environment, it just copies it. Okay. Maybe

02:18:05.230 --> 02:18:08.550
I have done pip freeze at that point of a time. If I

02:18:08.550 --> 02:18:11.230
remember the remove it, remove it, and then like, you know,

02:18:11.270 --> 02:18:15.170
just like use this one for fine tuning. We have a new code

02:18:15.170 --> 02:18:20.250
base. Right. But in that monster, there are a lot of like

02:18:20.250 --> 02:18:22.850
requirement, like these are only like 14. There are, I think

02:18:22.850 --> 02:18:26.290
more than 20. Even. And even I can't check the version.

02:18:27.290 --> 02:18:32.790
Yeah. Maybe I have done pip freeze at that point of a time.

02:18:33.310 --> 02:18:39.570
Uh huh. That, that would be the issue. Should I copy that

02:18:39.570 --> 02:18:43.390
content and they should over here? Yeah. Yeah. Or you can

02:18:43.390 --> 02:18:46.170
share your screen if you want. You want to share your

02:18:46.170 --> 02:18:48.970
screen? Yeah. Yeah. I think that would be better. Okay. So

02:18:48.970 --> 02:18:51.310
you can just share your screen. I think we can look into it

02:18:51.310 --> 02:18:53.510
in between. So yeah. Dhruv, what do you think? What is your

02:18:53.510 --> 02:18:57.730
question? Yeah. So my question is, is regarding this storage

02:18:57.730 --> 02:19:00.750
deployment and training the model and all. So right now when

02:19:00.750 --> 02:19:04.070
I'm, I'm using Mac M1 on the storage is somewhere around

02:19:04.070 --> 02:19:08.710
eight to 16 GB. Even if I upgrade, I can go maximum 16 to 24

02:19:08.710 --> 02:19:13.190
GB. Right. And if I'm having so big system that will cost me

02:19:13.190 --> 02:19:16.210
more. So right now from learning, deploying, and at least

02:19:16.210 --> 02:19:20.650
making, I use money till for next one and a half year. Uh, I

02:19:20.650 --> 02:19:23.590
should totally, should I totally rely on cloud? Yeah. Uh,

02:19:23.710 --> 02:19:26.490
just to train and build something. Even even after one and a

02:19:26.490 --> 02:19:29.090
half year, see you will never be able to build a complete

02:19:29.090 --> 02:19:31.390
server on your own. Right. No one does that because

02:19:31.390 --> 02:19:35.270
obviously it comes with the cost. So again, industry runs on

02:19:35.270 --> 02:19:38.990
cloud. Industry never runs on local. Right. Right. Right. So

02:19:38.990 --> 02:19:41.950
obviously for like, see for testing purposes, something is

02:19:41.950 --> 02:19:44.670
working in my system. It will work. Uh, yeah. I got it.

02:19:44.690 --> 02:19:49.010
Sumit. It's a PIP freeze. It's a, it's a PIP freeze. Just

02:19:49.010 --> 02:19:52.490
remove everything. Like in, like in such situation, although

02:19:52.490 --> 02:19:53.550
I know the name of Mac M1. I know the name of the

02:19:53.550 --> 02:19:56.190
dependency, but I don't know the version which is working

02:19:56.190 --> 02:19:59.550
for this. Uh, leave it. Leave it. Just go ahead with the

02:19:59.550 --> 02:20:03.010
latest version. Any how system will give you issue. So if

02:20:03.010 --> 02:20:06.870
the version there will be a version compatibility issue. 79

02:20:06.870 --> 02:20:09.850
dependencies. Yeah. I got it. Got it. I have done PIP

02:20:09.850 --> 02:20:13.470
freeze. So basically this is coming from PIP freeze. Okay.

02:20:13.890 --> 02:20:20.730
Yeah. So move only the name and install it and whichever

02:20:20.730 --> 02:20:23.870
issues come, I have to resolve it. Yeah. Copy the entire

02:20:23.870 --> 02:20:28.510
thing. Let me make your life bit easier. Copy. Go to URI.

02:20:29.950 --> 02:20:30.390
Okay.

02:20:35.110 --> 02:20:39.130
Yeah. Paste it, paste it and say, yeah, paste everything and

02:20:39.130 --> 02:20:41.950
say that, that just give me a dependency file with the

02:20:41.950 --> 02:20:44.730
compatible version. So most of the issue will be fixed in

02:20:44.730 --> 02:20:47.730
that way. Yeah. So give me the, like a dependency with a

02:20:47.730 --> 02:20:51.370
compatible version and just to just give me like a library

02:20:51.370 --> 02:20:55.690
name and it's a wasn't nothing else. So clean and give you

02:20:55.690 --> 02:20:59.230
the final file. Got it. Got it. And so I will do it. And

02:20:59.230 --> 02:21:03.430
then next thing is that I'm planning to like fine tune and

02:21:03.430 --> 02:21:06.890
write my own, like for the telecom domain to showcase and

02:21:06.890 --> 02:21:13.750
resume. Okay. Like I chuck the data data is in like in

02:21:13.750 --> 02:21:17.750
hugging phase I have in a user machine learning and Kaggle.

02:21:17.910 --> 02:21:21.110
But even that data is available or public. That means the

02:21:21.110 --> 02:21:25.430
same data can be used by a model, like a by a model, by our

02:21:25.430 --> 02:21:31.470
model as well. So don't need to create my own data. If you

02:21:31.470 --> 02:21:37.010
are training a model, which is like a, which requires your

02:21:37.010 --> 02:21:39.870
propriety data means your company data or your personal data

02:21:39.870 --> 02:21:46.580
in that case. Yes. Like there is no other way I can, how,

02:21:46.740 --> 02:21:48.840
how, how model will be able to understand your finances.

02:21:49.540 --> 02:21:53.340
Basically. If I mix a two or three source of data, like I

02:21:53.340 --> 02:21:57.380
took, they took them. So let's see. Every company finances.

02:21:57.380 --> 02:21:59.040
Every company's finance will be different. Every company's

02:21:59.040 --> 02:22:02.800
HR policy is going to be different, right? Yeah. So every

02:22:02.800 --> 02:22:04.720
company marketing funnel is going to be different. Every

02:22:04.720 --> 02:22:07.220
company sales pitch is going to be different. Every company

02:22:07.220 --> 02:22:10.540
sales product is going to be different, right? That's the

02:22:10.540 --> 02:22:13.680
reason. So we talked about fine tuning, right? Yeah. If, if

02:22:13.680 --> 02:22:16.160
like, if you don't, your company don't have like a, such

02:22:16.160 --> 02:22:18.380
kind of a differences, then you go and use generic model.

02:22:18.600 --> 02:22:20.940
Why to do even a fine tuning? Why to take that? Like, you

02:22:20.940 --> 02:22:21.620
know, kind of a headache.

02:22:25.580 --> 02:22:29.900
Yeah. I have to go. Actually. I want to do it from my own

02:22:29.900 --> 02:22:34.340
demo purpose. Yeah. Then today, today, last week, the

02:22:34.340 --> 02:22:38.620
project that I have discussed, right. Simple. Use it. And

02:22:38.620 --> 02:22:44.240
even in organization, just to use this one. Yeah. So it's

02:22:44.240 --> 02:22:49.040
ready to be like a use code for you. Yeah. Just, we just

02:22:49.040 --> 02:22:51.620
made some other, other summit we have to submit just, just

02:22:51.620 --> 02:22:54.920
take your question. Yeah. Let me, let me close the question

02:22:54.920 --> 02:22:58.660
from through. Yeah. Through. So, uh, like, uh. The cloud

02:22:58.660 --> 02:23:01.680
part. Yeah. So can you please repeat your question once

02:23:01.680 --> 02:23:05.280
again? Yeah. So the question, so let me give you an example

02:23:05.280 --> 02:23:08.940
also. So initially when deep seek, uh, deep seek came,

02:23:09.060 --> 02:23:12.760
right. They had, uh, open sourced it. So there were multiple

02:23:12.760 --> 02:23:17.560
model based on the storage, like eight GB, 16 GB, 32 GB. So

02:23:17.560 --> 02:23:21.440
in my local system, I was able to deploy and test it. The

02:23:21.440 --> 02:23:27.760
basic one, but when I went in 16 GB and all it crashed. Just

02:23:27.760 --> 02:23:30.800
out of curiosity. Yeah. I tested it, but I didn't go with

02:23:30.800 --> 02:23:34.380
full and all, and then I just load it and all. So in future

02:23:34.380 --> 02:23:37.140
perspective, also like you have trained this part, the thing

02:23:37.140 --> 02:23:40.080
that we were seeing right now to make such system, which

02:23:40.080 --> 02:23:44.480
can, uh, answer anything in a behalf of my organization. So

02:23:44.480 --> 02:23:47.380
training in a cloud. So there is two parts. The first one is

02:23:47.380 --> 02:23:50.980
training and making it workable. Right. And there is the end

02:23:50.980 --> 02:23:53.740
part when actually people will use. So we're actually

02:23:53.740 --> 02:23:57.280
inferencing. Yeah. Inferencing. So that, that's where, of

02:23:57.280 --> 02:24:00.180
course, I think every company will go with it. Uh, the AWS

02:24:00.180 --> 02:24:03.800
as your, or GCP, but for the training purpose, like us, who

02:24:03.800 --> 02:24:06.260
is just learning, who is just. No, no, no. Even both the

02:24:06.260 --> 02:24:09.040
parts. See training itself is a heavy task. Right. And it

02:24:09.040 --> 02:24:13.060
depends upon my model base model, basically, for example, if

02:24:13.060 --> 02:24:16.140
I'm using a OSS 120 billion parameter base model, right.

02:24:16.440 --> 02:24:20.620
Even in my system, I will not be able to run it 120 billion

02:24:20.620 --> 02:24:24.140
parameter, 120 B is fine when to be somehow I will be able

02:24:24.140 --> 02:24:27.540
to run it even tiny Lama, which is basically 1.1 billion

02:24:27.540 --> 02:24:31.980
parameter model. It took around 50 to 60 minutes of time for

02:24:31.980 --> 02:24:34.740
me to train only, or to find you in only one to do a full

02:24:34.740 --> 02:24:38.260
fine tuning. In your system. It's very huge. Obviously. Like

02:24:38.260 --> 02:24:41.460
my system is, you can say a little bit better system, right.

02:24:41.540 --> 02:24:43.960
Better system as compared to the average system that you

02:24:43.960 --> 02:24:46.620
would see in a market. I'm not saying that it's a best one

02:24:46.620 --> 02:24:49.540
when people have like a big, big system than this one, but

02:24:49.540 --> 02:24:53.680
yeah. So benchmark wise, like a 16 core, like a 32 logical

02:24:53.680 --> 02:24:58.040
core, and then a 40, 90, basically RTX. Uh, Nvidia GPU. Uh,

02:24:58.300 --> 02:25:01.260
graphics card is available, but it's a 96 GB of a Ram is

02:25:01.260 --> 02:25:03.940
available, but it's still, it is taking like that. When the

02:25:03.940 --> 02:25:07.680
time and 120 B, not even, not even 70, even I have tried 70

02:25:07.680 --> 02:25:10.360
billion parameter model, right. It classes my system even

02:25:10.360 --> 02:25:15.880
120 B. So obviously, uh, the training instances that we are

02:25:15.880 --> 02:25:20.140
going to choose that depends upon my model selection that

02:25:20.140 --> 02:25:23.400
depends upon our data size that depends upon the model

02:25:23.400 --> 02:25:27.740
training parameter. If my is maybe. I've traded for two.

02:25:28.220 --> 02:25:31.580
Let's go to one epoch, right. But maybe in one epoch, it is

02:25:31.580 --> 02:25:34.680
not going to decrease my loss at the time of, at the end of

02:25:34.680 --> 02:25:36.240
the day, we are decreasing. We are trying to decrease the

02:25:36.240 --> 02:25:39.880
loss, right? Loss loss of the models with respect to my data

02:25:39.880 --> 02:25:42.280
set. Yeah. This is what we are doing in a backward, backward

02:25:42.280 --> 02:25:46.360
propagation, right? So if I'm training it, maybe for a 300

02:25:46.360 --> 02:25:49.280
epoch or 3000 epochs. So where I know, like my model is

02:25:49.280 --> 02:25:52.160
going to give me the best performance. So if, uh, then I

02:25:52.160 --> 02:25:54.700
have to wait for 300 hours in that case, in my system, even

02:25:54.700 --> 02:25:59.320
with the one point only parameter model. Hmm. Right. So even

02:25:59.320 --> 02:26:02.520
the time of training, we need a high computation that how

02:26:02.520 --> 02:26:05.500
much computation that depends on my model selection, my data

02:26:05.500 --> 02:26:09.340
set and my training parameter. Okay. Got it. And

02:26:09.340 --> 02:26:12.460
influencing. So obviously at the time of inferencing, uh,

02:26:12.580 --> 02:26:15.740
basically I have to choose the system based on the request,

02:26:15.960 --> 02:26:18.260
how many hits we are going to get, what is the final model

02:26:18.260 --> 02:26:21.880
size, what time or what, like, uh, for, to generate a

02:26:21.880 --> 02:26:24.580
hundred token, how much time my model takes for one request,

02:26:24.740 --> 02:26:27.840
basically, uh, token wise, let's go. Hmm. So basically, uh,

02:26:27.840 --> 02:26:29.960
like a time it is going to take, how many parallel

02:26:29.960 --> 02:26:33.160
concurrent requests we are expecting, even in our peak time

02:26:33.160 --> 02:26:36.800
from the user. And then what kind of a latency I'm

02:26:36.800 --> 02:26:39.240
committing to my user, someone will commit that. Okay. I'll

02:26:39.240 --> 02:26:42.620
take 10 seconds. Right. In that case, I don't need my heavy

02:26:42.620 --> 02:26:45.300
machines, but someone say that, no, I will try to give you a

02:26:45.300 --> 02:26:48.880
response within 200 milliseconds. Yes. You need a high

02:26:48.880 --> 02:26:51.980
compute, even at a time of influencing. So basically these

02:26:51.980 --> 02:26:54.960
are the trade offs that we always, uh, consider when we try

02:26:54.960 --> 02:26:58.000
to, you know, set up this entire system. Okay. Got it. And

02:26:58.000 --> 02:27:00.840
here's my second question for the same. So how these cloud

02:27:00.840 --> 02:27:04.160
system, um, billing happens generally do the charge only

02:27:04.160 --> 02:27:07.820
when we consume compute or even when, when we have a story,

02:27:07.860 --> 02:27:10.880
both the parties available. So one is a result. So basically

02:27:10.880 --> 02:27:13.340
you try to take a pool of it and then like, uh, it is going

02:27:13.340 --> 02:27:17.200
to charge. And again, it happens with all the clouds, GCP,

02:27:17.300 --> 02:27:20.000
Azure and AWS depends upon what services we are going to

02:27:20.000 --> 02:27:24.120
spawn. And it is, it is like a, even like a, you can do it

02:27:24.120 --> 02:27:29.280
on a serverless mode paper as you go. Right. Okay. Pay as

02:27:29.280 --> 02:27:31.800
you go. Models are available. Pay as you go is like a,

02:27:31.800 --> 02:27:35.460
again, inside that there is a bifurcation taking a reserve

02:27:35.460 --> 02:27:39.440
instances. Right. Yeah. So for example, so bare minimum, I'm

02:27:39.440 --> 02:27:42.480
saying that, okay, so T2, like a large system I'm going to

02:27:42.480 --> 02:27:45.400
take or M2 system I'm going to take or something like that

02:27:45.400 --> 02:27:47.720
I'm going to take. Right. So obviously there will be a fixed

02:27:47.720 --> 02:27:51.180
cost to it. Right. Right. My server, or let's suppose I'm

02:27:51.180 --> 02:27:53.340
saying that, okay, my five server will be up and running all

02:27:53.340 --> 02:27:56.440
the time. And then in a pool, there will be 50 servers. So

02:27:56.440 --> 02:27:59.560
as per request, five, then six, then seven, then eight, it

02:27:59.560 --> 02:28:02.860
will keep on responding. So it is going to charge me for a

02:28:02.860 --> 02:28:06.380
response, uh, like, uh, for the results. Plus it is going to

02:28:06.380 --> 02:28:09.360
give me a charge for my pool, right? A small charge,

02:28:09.400 --> 02:28:13.120
fraction of charge. Plus when my pool will be utilized, then

02:28:13.120 --> 02:28:15.560
it will go with pay as you go means fixed cost plus pay as

02:28:15.560 --> 02:28:20.220
you go. Got it. Got it. Right. And then in AWS or GCP or

02:28:20.220 --> 02:28:23.680
Azure. So again, uh, there is a long term reserve, uh,

02:28:23.780 --> 02:28:26.160
reservation you can do and short term reservation. Long term

02:28:26.160 --> 02:28:29.620
means like I'm just, uh, using it every month. And long term

02:28:29.620 --> 02:28:32.240
means for three year, five year, I have done the contract.

02:28:32.380 --> 02:28:35.100
So every big company will be having a contract over there.

02:28:35.140 --> 02:28:38.840
They give you 80% to 90% a discount. So instead of paying a

02:28:38.840 --> 02:28:42.400
hundred rupees, you will end up paying just 10 rupees. Okay.

02:28:42.580 --> 02:28:45.200
And every company have, have that kind of a tie up. Every

02:28:45.200 --> 02:28:48.340
company almost will be having that kind of a tie up because,

02:28:48.380 --> 02:28:51.740
uh, uh, we all like go for a long-term contract with AWS

02:28:51.740 --> 02:28:54.720
because we don't change a cloud, right? Every day. Not even

02:28:54.720 --> 02:28:58.140
a 10 year. We change once, right? Once we have selected AWS,

02:28:58.340 --> 02:29:01.640
we'll just go with that or Azure or GCP. So long-term

02:29:01.640 --> 02:29:03.840
contract. So this is how charging happens. I mean like

02:29:03.840 --> 02:29:06.920
costing, costing happens. And when, whenever we are going to

02:29:06.920 --> 02:29:10.220
connect with cloud, just, I wanted to know from your side,

02:29:10.340 --> 02:29:12.220
like you have trained on your local system, but you could

02:29:12.220 --> 02:29:16.320
have take on cloud also. You do not have any training in

02:29:16.320 --> 02:29:19.360
local and also why you didn't choose in the cloud because

02:29:19.360 --> 02:29:23.100
there you can do it more earlier. We'll do it. We'll do it.

02:29:23.140 --> 02:29:25.600
Okay. Okay. Just, just, just wait for your couple. That's

02:29:25.600 --> 02:29:28.660
the reason I told you. Right. So from the, I think learning

02:29:28.660 --> 02:29:31.520
perspective, we have to know every single way. Right.

02:29:32.000 --> 02:29:34.760
Exactly. So basically I will, because there is a cloud

02:29:34.760 --> 02:29:37.440
chapter, if you'll go after MCPA two way. So there is

02:29:37.440 --> 02:29:40.460
something called as cloud chapter. I have to teach you

02:29:40.460 --> 02:29:45.060
something over there, right? Okay. I'll teach you this. Got

02:29:45.060 --> 02:29:48.180
it. Got it. So you are suggesting me. So if, if I'm taking

02:29:48.180 --> 02:29:51.060
my roadmap, at least for next one in two years, at least to

02:29:51.060 --> 02:29:55.820
land up at up. Best project or something. Right. So you're

02:29:55.820 --> 02:29:59.220
suggesting I must go with one cloud because I think that is

02:29:59.220 --> 02:30:03.600
also a part of learning the cloud system. Yeah. And then see

02:30:03.600 --> 02:30:06.700
cloud, you don't have to like learn or explore all the

02:30:06.700 --> 02:30:09.260
services. For example, AWS gives you 270 plus services.

02:30:09.400 --> 02:30:11.440
Right. Okay. Now you don't have to know about all the

02:30:11.440 --> 02:30:13.460
services. You should know about the basic of cloud. So

02:30:13.460 --> 02:30:16.760
generally user management, right. IEMs and its user

02:30:16.760 --> 02:30:19.020
management, its principles, everything you have to know,

02:30:19.060 --> 02:30:21.740
which is a very, very easy part to understand. In terms of

02:30:21.740 --> 02:30:24.960
creating a user, assigning a user, allocating resources, and

02:30:24.960 --> 02:30:29.300
then a specific that, okay, to run the model. So what kind

02:30:29.300 --> 02:30:33.320
of instances I have to do, I have to use or like to host

02:30:33.320 --> 02:30:36.680
some of these things. So how I can use an EKS and then how I

02:30:36.680 --> 02:30:38.660
will be able to do a domain mapping and everything. So I

02:30:38.660 --> 02:30:42.580
believe by the end of like this entire lectures, we will be

02:30:42.580 --> 02:30:45.740
having that much of understanding till domain mapping means

02:30:45.740 --> 02:30:49.140
on your domain, you can publish any services, right. That.

02:30:49.140 --> 02:30:53.340
So, and that is just a one-time game. So once you are going

02:30:53.340 --> 02:30:55.600
to learn it, it doesn't matter whether you are like

02:30:55.600 --> 02:30:58.060
deploying or training or doing like a hundreds of

02:30:58.060 --> 02:31:01.180
applications or thousands, it's going to be same. Right. And

02:31:01.180 --> 02:31:03.960
then later part is same everywhere. So whether we have

02:31:03.960 --> 02:31:07.200
trained on cloud or local, we can connect with any real time

02:31:07.200 --> 02:31:10.540
domain so we can give it to them. Yeah. Yeah. It's just a

02:31:10.540 --> 02:31:12.660
one time, one time learning things. You don't have to learn

02:31:12.660 --> 02:31:16.300
even twice because once you know that this is how it works

02:31:16.300 --> 02:31:19.060
every time. It doesn't matter whether you are creating a web

02:31:19.060 --> 02:31:21.400
application or mobile application, or maybe like a, you

02:31:21.400 --> 02:31:25.200
know, AML application. Things are the same. Yeah. And I

02:31:25.200 --> 02:31:28.520
think this will be good also because anyhow we have to know

02:31:28.520 --> 02:31:31.680
that the AWS part. That is the last, last phase basically,

02:31:31.680 --> 02:31:34.800
because after that we don't do anything with application. We

02:31:34.800 --> 02:31:40.020
just maintain it. Okay. Okay. Fine. Yeah. Got it. Yeah.

02:31:40.140 --> 02:31:43.460
Yeah. So that, that we'll see, that we'll see a lot. I

02:31:43.460 --> 02:31:47.500
think. Yeah. Okay. So now Sumit Sahu, please go ahead with

02:31:47.500 --> 02:31:54.030
the question. Yeah. Sumit. Hello. Hello sir. Yeah. Hi. Hi.

02:31:54.130 --> 02:31:57.750
Please go ahead. Yeah. Hi sir. Sir, actually I have a fine

02:31:57.750 --> 02:32:02.130
tuning task in my organization. I just need one suggestion.

02:32:02.290 --> 02:32:07.550
Sir, actually I have a code and I have to predict that code

02:32:07.550 --> 02:32:11.470
is vulnerable or not. And I have a CVID in the other

02:32:11.470 --> 02:32:15.670
feature. So what sort of the model and what sort of the

02:32:15.670 --> 02:32:19.170
training strategy? I should do. Can you please repeat once

02:32:19.170 --> 02:32:21.790
again, Sumit? Can you please repeat once again? Yes. I have

02:32:21.790 --> 02:32:26.330
a code, sir. A source code. And that has a label of whether

02:32:26.330 --> 02:32:30.590
that code is clean or that code is vulnerable. And I have

02:32:30.590 --> 02:32:34.910
another label that has a CVID. CVID means the, why that code

02:32:34.910 --> 02:32:38.790
is a vulnerable and that has the description and that has an

02:32:38.790 --> 02:32:43.030
ID basically. So I want to predict. Basically, if I have an

02:32:43.030 --> 02:32:46.410
input of a code, basically I have to predict both to a CVID

02:32:46.410 --> 02:32:53.490
and a CVID. So what

02:32:53.490 --> 02:32:57.350
sort of fine tuning task I have to do, and that is the my

02:32:57.350 --> 02:33:02.610
questions. Fine tuning, you are saying that you have a code

02:33:02.610 --> 02:33:07.730
that is one file and then you have already a data annotation

02:33:07.730 --> 02:33:11.090
with respect to a vulnerability, right? Yes. Yes. Yes. I

02:33:11.090 --> 02:33:15.230
have. Good model. See what kind of fine tuning means? It

02:33:15.230 --> 02:33:18.490
means like a similar, like the fine tuning that we have

02:33:18.490 --> 02:33:21.110
seen, we can go ahead with that. But which model I have to

02:33:21.110 --> 02:33:26.710
choose, let me search something for you. So code model, code

02:33:26.710 --> 02:33:29.210
model you have to train, right? Yes. Yes.

02:33:40.140 --> 02:33:42.920
Any LLM is going to work by the way, because every LLM

02:33:42.920 --> 02:33:46.040
nowadays understands a code, but I'm just searching like if

02:33:46.040 --> 02:33:50.820
there is some specific best model which is available just

02:33:50.820 --> 02:33:54.100
for coding. Yeah. So there is something called as code gen

02:33:54.100 --> 02:33:58.000
model. There is something called as a deep seek coder model,

02:33:58.180 --> 02:34:04.920
basically as per the suggestion from a GPT, I would say for

02:34:04.920 --> 02:34:08.840
now. So maybe model wise, you can try to go ahead with that.

02:34:08.980 --> 02:34:13.360
If you are just looking for like some specialized model with

02:34:13.360 --> 02:34:16.300
respect to a code only for a code. So deep seek coder, code

02:34:16.300 --> 02:34:19.800
T5 plus code gen, or there is something called as Maggie

02:34:19.800 --> 02:34:24.440
coder. Answer. I have another question. Hmm. Hmm. Okay. Can

02:34:24.440 --> 02:34:26.980
I do fine tuning? Can I predict two levels? Means I can

02:34:26.980 --> 02:34:30.720
predict like a generated, uh, uh, means like, uh, GPT is a

02:34:30.720 --> 02:34:34.240
generated model. Uh, if I will give a, uh, it will

02:34:34.240 --> 02:34:37.120
definitely give me the, whether that code is vulnerable or

02:34:37.120 --> 02:34:40.060
not. And why that code is vulnerable. Yeah. These two levels

02:34:40.060 --> 02:34:44.340
I have, and the PD also can predict both of those. Hmm. Can,

02:34:44.600 --> 02:34:48.840
with the help of fine tuning, can I do this both the task? I

02:34:48.840 --> 02:34:52.840
can predict the, whether the code is vulnerable or not. And

02:34:52.840 --> 02:34:56.760
also I can predict by. It means basically the CVE part.

02:34:56.980 --> 02:35:00.200
Okay. So let me tell you something basic over here, right?

02:35:00.300 --> 02:35:02.780
So you are, you are trying to say that, okay, so my model

02:35:02.780 --> 02:35:05.780
will be able to give me a response with respect to our

02:35:05.780 --> 02:35:09.640
vulnerability and it will be able to generate a new set of a

02:35:09.640 --> 02:35:14.240
code. Both it should do, right? Yes. Yes. Yes. Technically

02:35:14.240 --> 02:35:17.340
it's from a model side. If I'll say both are basically a

02:35:17.340 --> 02:35:21.200
generation task. Yes. Yes. Yes. Both are generation tasks,

02:35:21.300 --> 02:35:24.080
right? Yes. Yes. A hundred percent. Yes. Obviously it will

02:35:24.080 --> 02:35:27.400
be able to do it. We'll see. It doesn't matter. Like, see,

02:35:27.500 --> 02:35:31.200
we as a human, so we are able to see that one noble or non

02:35:31.200 --> 02:35:34.920
-verbal is a classification task, right? This is what we as

02:35:34.920 --> 02:35:37.660
a human being is able to think. And then the next code

02:35:37.660 --> 02:35:40.320
generation is basically a generation task for me. But if I

02:35:40.320 --> 02:35:42.960
think from a LLM perspective, I'm not just building

02:35:42.960 --> 02:35:44.920
classification model, right? LLM is not a classification

02:35:44.920 --> 02:35:48.460
model. LLM is technically a generation model. So we try to

02:35:48.460 --> 02:35:50.700
take our context and then based on that, it generates

02:35:50.700 --> 02:35:53.460
something. So maybe it generates a classes or it generates a

02:35:53.460 --> 02:35:58.420
new context or maybe a new kind of a code. So my answer is

02:35:58.420 --> 02:36:01.700
yes, it will be able to do both because in case of LLMs,

02:36:01.800 --> 02:36:05.740
even classification is your generation. But sir, in this

02:36:05.740 --> 02:36:08.760
case, the, whatever the fine tuning scripts you are saying,

02:36:08.940 --> 02:36:13.500
it will work or it will, I have to change the scripting. It

02:36:13.500 --> 02:36:19.400
will. Okay. So the data, which I have shared with you. So,

02:36:19.540 --> 02:36:26.120
okay. Now, uh, let me share my screen once again and let's

02:36:26.120 --> 02:36:28.260
talk about it. So whether this is going to work or not. So

02:36:28.260 --> 02:36:31.460
for example, I'm talking about this HR data set, right here.

02:36:32.400 --> 02:36:35.520
So this is the input or maybe system instruction, and this

02:36:35.520 --> 02:36:39.000
is the output, which I'm expecting, right? So if you are

02:36:39.000 --> 02:36:41.560
looking for a vulnerable, not vulnerable, mentioned to

02:36:41.560 --> 02:36:43.720
vulnerable, not vulnerable over here as a label, as output,

02:36:43.960 --> 02:36:48.140
right. In other places, answer and question and answer.

02:36:48.880 --> 02:36:53.620
Okay. Then, then it will work. Okay. Okay. Yeah. Understood.

02:36:54.220 --> 02:37:00.180
Okay. Okay. Uh, next is, uh, yes. Uh, who I should call, uh,

02:37:00.380 --> 02:37:07.160
Tabis go ahead. Yeah. Yeah. Hi. Yeah. Go ahead, please.

02:37:07.340 --> 02:37:11.920
Yeah. Actually, I recently, I have given the interview, so I

02:37:11.920 --> 02:37:16.800
explained that, uh, Nexus RG project. So in that project,

02:37:16.980 --> 02:37:21.460
uh, the interviewer asked about, uh, like, uh, HR policy. We

02:37:21.460 --> 02:37:25.600
usually have the text and images kind of thing as a, as a

02:37:25.600 --> 02:37:29.040
data. So they, they asked me means how you're going to

02:37:29.040 --> 02:37:32.680
validate, uh, that images, which you have in your data set,

02:37:32.860 --> 02:37:36.980
you're extracted correctly or not. So this is the first

02:37:36.980 --> 02:37:40.300
question they asked, the validation, how you're going to

02:37:40.300 --> 02:37:43.840
validate the images has been extracted or not. And second

02:37:43.840 --> 02:37:48.240
question, uh, about the rag means how you validate your

02:37:48.240 --> 02:37:50.960
model output, which are coming correctly. The output is,

02:37:51.040 --> 02:37:55.420
which are getting, uh, how you're missing that output, it is

02:37:55.420 --> 02:37:58.820
correct or not. So this question means I like the answer

02:37:58.820 --> 02:38:02.140
from you. Okay. So the very first question that you have

02:38:02.140 --> 02:38:04.460
asked that, uh, okay, so let's suppose we have our tech data

02:38:04.460 --> 02:38:06.960
or image data and we are like, uh, doing it for the fine

02:38:06.960 --> 02:38:12.370
tuning, right? Uh, it's, it's for the rag application. Okay.

02:38:12.910 --> 02:38:15.130
Okay. Fine. So we are, we are trying to build a rag

02:38:15.130 --> 02:38:18.450
application. We are trying to store. So fine tuning or rag

02:38:18.450 --> 02:38:21.310
application, because both are different. It's for rag. Like

02:38:21.310 --> 02:38:23.310
I said, it's fine. Like I explained the rag, like, uh, what

02:38:23.310 --> 02:38:25.910
are the HR policy and different policy, which we have, we

02:38:25.910 --> 02:38:29.330
are creating the rag application and the text to the text,

02:38:29.430 --> 02:38:33.090
we are converting all those data into the embeddings. So

02:38:33.090 --> 02:38:37.010
next question he asked that you have the images in the data

02:38:37.010 --> 02:38:39.810
as well. So how are you validating those images in the data

02:38:39.810 --> 02:38:43.830
set that data has been extracted correctly or not validation

02:38:43.830 --> 02:38:49.590
of, uh, image data, uh, in the PDF file, which we have text

02:38:49.590 --> 02:38:51.570
and the images. So how are you validating those images? Some

02:38:51.570 --> 02:38:55.610
tables are there. Yeah. Something else. Yeah. Table or

02:38:55.610 --> 02:38:59.970
content are there in the, in the HR policy. So how are you

02:38:59.970 --> 02:39:03.770
validating those images? Got it. Got it. So basically see,

02:39:03.870 --> 02:39:07.150
the thing is that, that, uh, we can use at the end of the

02:39:07.150 --> 02:39:10.790
day, if something is available in a PDF format. So pipe

02:39:10.790 --> 02:39:13.010
resident kind of a library, which will help me out in terms

02:39:13.010 --> 02:39:16.870
of reading the entire PDF. Now, inside a PDF, I will be

02:39:16.870 --> 02:39:20.370
having images. I will be having a text. These two kinds of

02:39:20.370 --> 02:39:22.810
entities, which is like a probability possible, or maybe a

02:39:22.810 --> 02:39:26.390
table third entity, which could be possible. So wherever I'm

02:39:26.390 --> 02:39:29.010
able to identify a table. So I will be using some table

02:39:29.010 --> 02:39:32.050
library where ever I have images, I will be using a best

02:39:32.050 --> 02:39:37.510
possible either LLM model for an image understanding. If I'm

02:39:37.510 --> 02:39:41.510
building something now, otherwise, if there is something

02:39:41.510 --> 02:39:44.610
like a OCR related things, which I have to do with respect

02:39:44.610 --> 02:39:48.250
to our images, I can use that OCR. Yeah. Which will be able

02:39:48.250 --> 02:39:51.450
to give me some sort of understanding of my images. So these

02:39:51.450 --> 02:39:53.550
are the techniques we can use, but yeah, currently if I'm

02:39:53.550 --> 02:39:57.290
trying to build, so I'll try to use some vision based LLMs

02:39:57.290 --> 02:40:02.430
to give me a better understanding about images. This is,

02:40:02.430 --> 02:40:07.850
this is like a, what we are trying to use. So in that case,

02:40:07.970 --> 02:40:11.570
do we have to use separate, separate model? Like in the PDF,

02:40:11.730 --> 02:40:16.210
we have the text and the images. Do we need to use different

02:40:16.210 --> 02:40:19.110
models? Hmm. Obviously we have to use a different model. So

02:40:19.110 --> 02:40:21.670
for example, even in PDF, so we might have a different,

02:40:21.710 --> 02:40:24.350
different languages, which is available. So for text

02:40:24.350 --> 02:40:26.670
parsing, I should use a model which will be able to

02:40:26.670 --> 02:40:29.370
understand a multiple languages. Part number one for table.

02:40:29.510 --> 02:40:31.850
So obviously I have to use some libraries because libraries

02:40:31.850 --> 02:40:34.710
are going to be like a work fine for me in terms of parsing

02:40:34.710 --> 02:40:38.630
a data. Now third part is our images. So if we have our

02:40:38.630 --> 02:40:41.370
images, so we have to decide that, okay, so we have to do

02:40:41.370 --> 02:40:44.450
our OCR means just some character inside the images we have

02:40:44.450 --> 02:40:47.830
to extract. Or. Or we have some graphical understanding

02:40:47.830 --> 02:40:51.110
based images means our image, which is trying to explain

02:40:51.110 --> 02:40:54.470
something like maybe my HR policy, maybe my finance or maybe

02:40:54.470 --> 02:40:56.910
etiquette inside a company. So that image is representing

02:40:56.910 --> 02:41:00.210
that. So if we have that kind of images in that case, best

02:41:00.210 --> 02:41:04.790
approach will be to use a model which is having a image

02:41:04.790 --> 02:41:05.910
understanding.

02:41:07.830 --> 02:41:10.570
So vision based model we can try to use over there or vision

02:41:10.570 --> 02:41:14.150
based LLM we can try to use, which will be able to like, uh,

02:41:14.150 --> 02:41:16.330
help me out in terms of understanding. The images. And

02:41:16.330 --> 02:41:19.190
eventually I will be able to convert that into embeddings.

02:41:21.010 --> 02:41:24.770
Okay. So that's how, how, how we are going to validate,

02:41:24.890 --> 02:41:28.790
like, uh, the images which we have inside the PDF. It is

02:41:28.790 --> 02:41:33.310
giving the correct, uh, output if you are going to ask

02:41:33.310 --> 02:41:39.190
anything validation wise. So if I'll talk about the RG

02:41:39.190 --> 02:41:41.890
validation, so we try to use a recall, we try to use a

02:41:41.890 --> 02:41:45.250
precision. We try to use maybe a context based precision. We

02:41:45.250 --> 02:41:48.830
try to use even a freshness, a freshness of the data. So

02:41:48.830 --> 02:41:51.230
these are the validate in general validation criteria we try

02:41:51.230 --> 02:41:54.390
to perform. So if I'll talk about a recall, so recall simply

02:41:54.390 --> 02:41:58.890
means that that percentage of query where we are going to

02:41:58.890 --> 02:42:02.690
get at least one correct kind of a answer. That is something

02:42:02.690 --> 02:42:06.050
called as a recall precision. So basically like, uh, again,

02:42:06.170 --> 02:42:10.170
it is going to measure the relevancy of the measured outcome

02:42:10.170 --> 02:42:13.410
that we are getting. Now you will ask the next question

02:42:13.410 --> 02:42:16.490
that, okay, fine. So, uh, for precision validation or for

02:42:16.490 --> 02:42:19.910
recall validation, so what should be the approach? Approach

02:42:19.910 --> 02:42:22.290
is very simple. So whenever we try to build any RG system

02:42:22.290 --> 02:42:26.850
parallel to that, we try to create a test data right before

02:42:26.850 --> 02:42:29.770
deploying any, any solution. So we try to prepare the test

02:42:29.770 --> 02:42:32.610
data, we try to write a test cases now inside the test data.

02:42:32.710 --> 02:42:37.050
So we try to write the expected outcome and then we try to

02:42:37.050 --> 02:42:40.530
give a query to my RG system. So whatever RG system is going

02:42:40.530 --> 02:42:43.130
to give it to me, we try to do a comparison. This is where

02:42:43.130 --> 02:42:46.050
precision recall comes in. So precision recall is in generic

02:42:46.050 --> 02:42:49.430
approach for doing any kind of, uh, you know, uh,

02:42:49.690 --> 02:42:53.990
measurement in terms of like, uh, doing a general, uh, with

02:42:53.990 --> 02:42:58.010
respect to RG system that how reliable my RG system is apart

02:42:58.010 --> 02:43:01.030
from that, a freshness, we also try to measure in case of RG

02:43:01.030 --> 02:43:04.230
freshness of the data, because if my data set is like a two

02:43:04.230 --> 02:43:07.930
old, and if it is not able to cater my current query, then

02:43:07.930 --> 02:43:11.510
obviously my freshness score is going to be low. Same

02:43:11.510 --> 02:43:16.030
approach. We try to follow. Now for test data, so we'll be

02:43:16.030 --> 02:43:19.270
having a input and output. Now try to give an input to your

02:43:19.270 --> 02:43:22.770
RG system and then, uh, just like, uh, you know, do the

02:43:22.770 --> 02:43:25.590
comparison that how much percentage for percentage of record

02:43:25.590 --> 02:43:28.770
it is able to give me the fresh, uh, like, uh, news or maybe

02:43:28.770 --> 02:43:32.430
reports and for how many it is not able to give it to me. So

02:43:32.430 --> 02:43:34.210
these are the, like, uh, you know, a measurement criteria

02:43:34.210 --> 02:43:38.930
that we have to use. Yeah. Okay. So I have given a

02:43:38.930 --> 02:43:43.930
freshness. Yeah. I have given FN score. FN score is also

02:43:43.930 --> 02:43:46.110
good. It's also fine. See precision. Recall FN score. All of

02:43:46.110 --> 02:43:49.270
these things are related. If we all remember everything is

02:43:49.270 --> 02:43:52.990
getting derived from my confusion matrixes, right? So yeah.

02:43:53.050 --> 02:43:56.570
FN score is also a good, uh, like, uh, measuring criteria. I

02:43:56.570 --> 02:44:03.010
would say. Oh, okay. Yeah. Okay. Thanks. Thanks. Yeah. But

02:44:03.010 --> 02:44:05.530
with RG, so we even try to check a freshness score all the

02:44:05.530 --> 02:44:09.810
time. Okay. There is, there is something called as, uh,

02:44:09.850 --> 02:44:12.030
again, which I remember a context precision. So context

02:44:12.030 --> 02:44:15.110
precision is again. One of the. Uh, like a way by which we

02:44:15.110 --> 02:44:19.150
try to measure, but all together, it's a very simple, a test

02:44:19.150 --> 02:44:22.150
data versus the output. Even if I'll talk about a FN score,

02:44:22.410 --> 02:44:28.130
which is eventually a precision recall itself. Yeah. So this

02:44:28.130 --> 02:44:31.270
is how we can try to measure any other question for me. No,

02:44:31.370 --> 02:44:34.750
no. Thanks. Thanks. Okay. So our next, uh, so these go at

02:44:34.750 --> 02:44:38.010
least. Yeah. Hello, sir. Good morning. Yeah. Good morning.

02:44:38.010 --> 02:44:41.370
Yeah. Go ahead. mm. Smell goodforce?

02:44:44.090 --> 02:44:51.450
Mr. This is a squad

02:44:51.450 --> 02:44:56.330
server structure.

02:44:57.830 --> 02:45:04.390
I appreciate you. I appreciate you. I appreciate you. Now,

02:45:04.390 --> 02:45:05.930
earlier you wanted to say, check. Future. Future. Future.

02:45:05.930 --> 02:45:12.570
Future. So, we have to create a chatbot on top of it, which

02:45:12.570 --> 02:45:16.630
will answer all the questions. The data has different

02:45:16.630 --> 02:45:21.930
modules in it. For example, there is an RFI module, a

02:45:21.930 --> 02:45:25.350
submitters module. Along with that, there is data. So, we

02:45:25.350 --> 02:45:29.110
have to create a chatbot on top of it. Okay. So, right now,

02:45:29.270 --> 02:45:35.470
what is my role? Project management. Along with me, there

02:45:35.470 --> 02:45:40.990
are two AI engineers. So, in our existing system, we have

02:45:40.990 --> 02:45:48.630
RTX 4090. Okay. So, the model we are using is LAMA 3.1 8B

02:45:48.630 --> 02:45:52.350
Instruct. Okay. So, right now, I will tell you the problems

02:45:52.350 --> 02:45:58.790
we are facing. The first problem is latency. It takes 2-3

02:45:58.790 --> 02:46:02.910
minutes. Are you talking about training or inferencing?

02:46:03.630 --> 02:46:06.450
Inferencing. Inferencing, okay. So, inferencing and latency.

02:46:06.550 --> 02:46:10.670
So, your latency is 2-3 minutes. Okay. Yes. And concurrent

02:46:10.670 --> 02:46:15.270
queries are not being handled by us. Multiple. That means,

02:46:15.310 --> 02:46:18.810
if two users are hitting simultaneously, then the whole

02:46:18.810 --> 02:46:22.390
system is crashing down. Okay. Got it. But the token limit.

02:46:22.770 --> 02:46:27.390
Okay. Yes. These are the major problems. Okay. But there is

02:46:27.390 --> 02:46:32.450
no problem. This is a system bottleneck that you have. So,

02:46:32.490 --> 02:46:35.790
you don't have to do anything. Just spin H100 and run it on

02:46:35.790 --> 02:46:38.650
top of it. That is the first approach. The second approach

02:46:38.650 --> 02:46:41.650
is that the code base we have shared with you today, where

02:46:41.650 --> 02:46:44.610
we are using a tiny model, which is comparatively a small

02:46:44.610 --> 02:46:49.090
model. You are using 8 billion model, right? 8B. Yes. 8B in

02:46:49.090 --> 02:46:53.830
4090. Okay. I am using 1.1B in 4090. Okay. So, the second

02:46:53.830 --> 02:46:57.130
approach is that you should reduce the size of the model.

02:46:58.030 --> 02:47:01.150
Third. Okay. So, what is the first approach? If it is not

02:47:01.150 --> 02:47:02.250
running directly in the local, then take it to the cloud.

02:47:02.790 --> 02:47:05.430
Second. No, I have to test it in the local. I have to run it

02:47:05.430 --> 02:47:08.990
in the local. And multiple. You will find it easily. And in

02:47:08.990 --> 02:47:11.530
such a case, you have to take it to the cloud. So, this is

02:47:11.530 --> 02:47:15.650
what we

02:47:15.650 --> 02:47:17.430
have learned. We will take it and run it in the local. And I

02:47:17.430 --> 02:47:20.770
had already said that, after facing the log of the model,

02:47:20.770 --> 02:47:23.550
then you have to test Plus when you If I want to run it,

02:47:23.550 --> 02:47:25.170
then I've to test it? Yes, you have to run it in the local.

02:47:25.270 --> 02:47:29.430
Does it work? Yes. So, what happens, what happens, I have to

02:47:29.430 --> 02:47:32.990
do it manually right? So when the size will be less, it

02:47:32.990 --> 02:47:36.190
will be the light-weighted one means you do 8B only. Let's

02:47:36.190 --> 02:47:39.690
suppose that I am not going to change my model, so even

02:47:39.690 --> 02:47:41.730
though I have suggested but yeah, let's suppose that you are

02:47:41.730 --> 02:47:44.450
not going to change the model, okay, what you are doing is

02:47:44.450 --> 02:47:46.290
that you are using 8B only for the instruct model of deep

02:47:46.290 --> 02:47:48.150
seek, whatever model you are using, which you were having an

02:47:48.150 --> 02:47:51.030
issue with, okay, instead of going ahead with full fine

02:47:51.030 --> 02:47:54.870
tuning, just go with a Q LoRa, quantized LoRa, okay, so in

02:47:54.870 --> 02:47:56.650
this the model gets smaller automatically, which is your

02:47:56.650 --> 02:47:58.530
main model, which is the parent model, it makes the parent

02:47:58.530 --> 02:48:02.390
model smaller, okay, and then if you will do the

02:48:02.390 --> 02:48:04.970
inferencing, you can easily do concurrent 10B, it will work

02:48:04.970 --> 02:48:11.490
in 4090, okay, third one more check, you are saying that you

02:48:11.490 --> 02:48:16.370
have 4090, okay, but is it using 4090 at the time of

02:48:16.370 --> 02:48:18.950
inferencing, okay, because this major, if you are using

02:48:18.950 --> 02:48:22.410
windows, then you will always see this major problem, that

02:48:22.410 --> 02:48:25.190
there is GPU in your system, but still it is not using it.

02:48:25.250 --> 02:48:27.350
Okay. Let's talk about compatibility and library dependency.

02:48:27.850 --> 02:48:32.890
So, if you hit a single query, then the user will not be

02:48:32.890 --> 02:48:36.090
affected. No, no, no, CPU does not mean single, double, what

02:48:36.090 --> 02:48:39.990
is it using in the back, it makes a lot of difference, you

02:48:39.990 --> 02:48:41.990
are saying that you have 4090, right, you will hit a single

02:48:41.990 --> 02:48:45.710
query, CPU is used by default, right, CPU is something that

02:48:45.710 --> 02:48:49.110
every processor, means every code, every process that runs

02:48:49.110 --> 02:48:51.470
in your system, it is all used, so if GPU is not available,

02:48:51.690 --> 02:48:55.710
then CPU hits it. No, sir, GPU is enabled and 100% of the

02:48:55.710 --> 02:48:59.890
CPU is enabled. Okay, so your model size is the problem in

02:48:59.890 --> 02:49:03.110
that case, okay, so first of all, just go with the Q LoRa,

02:49:03.230 --> 02:49:06.690
this one, okay, keep the model the same, the model you are

02:49:06.690 --> 02:49:09.570
using, keep the same model, so just replace this model,

02:49:09.710 --> 02:49:14.290
okay, instead of Tiny Lama, use your struct model 8B, and go

02:49:14.290 --> 02:49:16.570
with the quantized LoRa, okay, the model size will

02:49:16.570 --> 02:49:19.250
automatically go down and then you will be able to do a

02:49:19.250 --> 02:49:21.590
faster inferencing, because I believe that what you are

02:49:21.590 --> 02:49:25.790
doing now is full fine tuning, right? Yes. Yes. This is the

02:49:25.790 --> 02:49:30.570
biggest problem in full time editing, that is the problem.

02:49:30.730 --> 02:49:34.210
Go with Q LoRa and then check, it will not stop you, yes,

02:49:34.290 --> 02:49:38.370
there may be a slight compromise in the result, but I don't

02:49:38.370 --> 02:49:41.090
think that you will get an issue in the compute in that

02:49:41.090 --> 02:49:47.410
case. Okay. That is the solution for you. Okay. Sir, there

02:49:47.410 --> 02:49:49.890
is a second use case, where we had a project for MediChat

02:49:49.890 --> 02:49:54.630
Pro, we have the same same document, with two types of

02:49:54.630 --> 02:50:00.890
documents. Okay. in all pdf formats ok ok so what is the use

02:50:00.890 --> 02:50:05.230
case now means one solution we have prepared that in one

02:50:05.230 --> 02:50:08.250
document there is instruction set of instruction according

02:50:08.250 --> 02:50:13.470
to that second document that is made or not based on that

02:50:13.470 --> 02:50:16.990
repeat repeat repeat once again what was that there is one

02:50:16.990 --> 02:50:22.610
pdf in which set of instructions ok ok so based on that

02:50:22.610 --> 02:50:27.250
instruction the second document made in pdf document so that

02:50:27.250 --> 02:50:32.370
instruction is followed or not in that second document ok ok

02:50:32.860 --> 02:50:36.670
so what is the use case now that we have to compare document

02:50:36.670 --> 02:50:42.670
one instruction document with the ok no problem so you do

02:50:42.670 --> 02:50:45.350
RIG with first document whatever letter it will give you

02:50:45.350 --> 02:50:50.190
technically instruction and you match it with other document

02:50:50.860 --> 02:50:55.430
ok similarity match you don't have to do RIG in second step

02:50:55.430 --> 02:50:58.290
in second step you have to do similarity match that you have

02:50:58.290 --> 02:51:01.130
given instruction and whether it is followed or not in first

02:51:01.130 --> 02:51:04.670
case you have to go and take out the instruction if i am

02:51:04.670 --> 02:51:07.530
able to understand it correctly so first it should go and

02:51:07.530 --> 02:51:10.190
extract the information now after extracting the information

02:51:10.190 --> 02:51:12.550
it should go and check that this instruction is been

02:51:12.550 --> 02:51:16.270
followed or not in other right so in first we will do RIG

02:51:16.270 --> 02:51:18.090
technically we are doing similarity match

02:51:23.150 --> 02:51:25.390
that how close is the instruction which i am able to

02:51:25.390 --> 02:51:33.350
generate in end case we have to generate a report ok ok in

02:51:33.350 --> 02:51:36.750
tabular format that this instruction is followed in second

02:51:36.750 --> 02:51:41.230
document we can tell you another very simple solution that

02:51:41.230 --> 02:51:46.610
you are storing the document whatever you have to do finally

02:51:46.610 --> 02:51:51.150
call a simple agent write system problem in agent this is my

02:51:51.150 --> 02:51:55.490
source document where you should go and check and after

02:51:55.490 --> 02:51:58.050
extracting the information you should do a check with other

02:51:58.050 --> 02:52:03.090
document ok and generate this kind of a report for me i

02:52:03.090 --> 02:52:05.610
think that will be a very simple approach and the best one

02:52:05.610 --> 02:52:11.730
ok one single agent and then it will work for you you have

02:52:11.730 --> 02:52:15.310
to call the agent who is going to read pdf or if you are

02:52:15.310 --> 02:52:20.430
storing in vector then you have to give access to vector ok

02:52:20.430 --> 02:52:24.450
sir so this is the first approach i think it can be built

02:52:24.450 --> 02:52:28.990
within an hour of time what happened sir we did project of

02:52:28.990 --> 02:52:33.950
medichat same concept i explained to our CEO that we can do

02:52:33.950 --> 02:52:41.090
this he accepted for initial purpose we used model chat gbt

02:52:41.090 --> 02:52:47.410
ok we compared both and generated report and showed to

02:52:47.410 --> 02:52:49.370
client client liked it

02:52:53.150 --> 02:52:57.890
but we did not get in development ok so now CEO is forcing

02:52:57.890 --> 02:53:03.770
that we need to complete the project within 2-3 days ok no

02:53:03.770 --> 02:53:10.390
problem this class is very useful yes ok this is the

02:53:10.390 --> 02:53:15.210
learning that we are able to use in company yes ok

02:53:17.550 --> 02:53:23.450
next question for me anyone can go ahead Tabiz, Nares, Ram i

02:53:23.450 --> 02:53:29.530
can see can i go ahead yeah i think you can go i think it's

02:53:29.530 --> 02:53:33.810
been a long time sir since you have joined the class yeah

02:53:33.810 --> 02:53:37.250
it's very long time we were missing your voice by the way we

02:53:37.250 --> 02:53:43.010
all were missing your voice really ok yeah that's correct

02:53:43.010 --> 02:53:49.170
but your lecture today is very interesting ok now i have

02:53:49.170 --> 02:53:54.890
been building lots of products ok and i revamped my site

02:53:54.890 --> 02:53:59.970
atalacloud.com and if you just look at that i just pinged

02:53:59.970 --> 02:54:06.630
you and it is totally revamped it's now maybe you can click

02:54:06.630 --> 02:54:14.710
and share it and it's now like a modern site it's no more

02:54:14.710 --> 02:54:20.910
like legacy site i have made some product live over there

02:54:20.910 --> 02:54:25.570
one more product and if you can do a small test

02:54:28.490 --> 02:54:33.850
can you share your machine and click this atalacloud.com ok

02:54:33.850 --> 02:54:42.090
so i pinged you on the chat box ok ok then

02:54:44.070 --> 02:54:47.910
we will talk about fine tuning also because i have done the

02:54:48.110 --> 02:54:56.190
fine tuning ok so i can yeah so now this site is just go

02:54:56.190 --> 02:55:01.490
over little bit this is now a new site and if you go to the

02:55:01.490 --> 02:55:09.890
projects page projects page ok and now see generative ai i

02:55:09.890 --> 02:55:17.090
made it like a card looking ok ok nice nice touch with vibe

02:55:17.090 --> 02:55:21.850
coding i can i can feel it feel the vibe coding here really

02:55:21.850 --> 02:55:28.450
ok yeah so now the second project which i made live is this

02:55:28.450 --> 02:55:33.050
note taking application go up little bit note taking ok this

02:55:33.050 --> 02:55:36.470
one ok yeah yeah that's live there is a live url go up

02:55:36.470 --> 02:55:40.250
little bit up little bit up ok ok so click that smart note

02:55:40.250 --> 02:55:44.810
no no stay there stay on the card ok and there is a live

02:55:44.810 --> 02:55:51.850
project live link ok ok and just can you create an account

02:55:51.850 --> 02:55:57.090
and just i mean real account it should be your real email

02:55:57.090 --> 02:55:59.030
account yeah ok

02:56:04.120 --> 02:56:06.840
confirmation code did

02:56:13.620 --> 02:56:17.240
you get got the verification code

02:56:31.760 --> 02:56:37.800
ok ok now on the left side there is an icon so you can

02:56:37.800 --> 02:56:41.560
basically change the color there are six seven colors so

02:56:41.560 --> 02:56:43.660
that's the one thing then

02:56:46.090 --> 02:56:55.700
uh you can create note i can create a note over here ok so

02:56:55.700 --> 02:57:00.520
create any note put any file choose any file from your

02:57:00.520 --> 02:57:05.200
system one file or as many files you want attach the file

02:57:05.200 --> 02:57:10.400
attach the file attach the selected file click the attach ok

02:57:10.400 --> 02:57:15.960
and then go ahead modify that file as many times as you want

02:57:15.980 --> 02:57:21.300
as you want ok or attach as many files as you want ok and

02:57:21.300 --> 02:57:27.840
this is an encrypted end to end with all the functionalities

02:57:27.840 --> 02:57:33.360
that some of my friends who are white gora people here 30 40

02:57:33.360 --> 02:57:36.580
years into it they said they have never seen anything like

02:57:36.580 --> 02:57:45.080
this ok because this this is a real time using web sockets

02:57:45.080 --> 02:57:53.420
and it's using this cognito with jwt token that's forward to

02:57:53.420 --> 02:57:59.860
aws and then it is using the like acid compliant that means

02:57:59.860 --> 02:58:05.940
your files and your information which is coming from dynamo

02:58:05.940 --> 02:58:11.740
database and your this front end is always 100% of the time

02:58:11.740 --> 02:58:16.100
in sync so you cannot so basically i have tested this very

02:58:16.100 --> 02:58:22.080
thoroughly the files that you can add files you can move

02:58:22.080 --> 02:58:28.180
files and they will go away from the data lake as soon as

02:58:28.180 --> 02:58:31.500
you remove it if you add it it will go there and it will

02:58:31.500 --> 02:58:41.960
stay in sync with your data in dynamo database so ok yeah so

02:58:41.960 --> 02:58:46.820
it's using web sockets it's not using a because app sync

02:58:47.740 --> 02:58:55.480
amplify and it's using graphql it's not using those rest

02:58:55.480 --> 02:59:03.060
apis so for the real time update you can and i tried with i

02:59:03.060 --> 02:59:07.880
put a few thousand notes with files ok so it is super super

02:59:07.880 --> 02:59:13.660
fast ok because it can handle even million notes without any

02:59:13.660 --> 02:59:20.080
issue because of this uh the graphql and it's very fast very

02:59:20.080 --> 02:59:25.480
very fast and i have tried up to 39 megabyte of file but i

02:59:25.480 --> 02:59:31.560
believe it it can take 99 megabyte of file i tried 150 that

02:59:31.560 --> 02:59:36.460
it didn't take ok maybe with multi part yes it could but but

02:59:36.460 --> 02:59:44.480
but that means it can take a big files also and now this is

02:59:44.480 --> 02:59:48.740
i just made live this uh i think last week and now there is

02:59:48.740 --> 02:59:52.180
another application i am building is which is almost going

02:59:52.180 --> 02:59:59.240
to be live in couple of days ok that that will have this

02:59:59.240 --> 03:00:04.180
plus lot of other like i clone this application and added a

03:00:04.180 --> 03:00:07.860
few more functionalities and that is going to come as my

03:00:07.860 --> 03:00:11.220
third application and the fourth application is fine tuning

03:00:11.220 --> 03:00:14.420
which you were just now talking about ok that is for a law

03:00:14.420 --> 03:00:18.700
department ok and company has eight departments so i have

03:00:18.700 --> 03:00:23.840
fine tuned all their eight departments and that is like you

03:00:23.840 --> 03:00:28.020
were showing this guy about qlaura and and and that's what

03:00:28.020 --> 03:00:32.680
what i did with qlaura i could do all that ok very very

03:00:32.680 --> 03:00:36.720
easily but for fine tuning full fine tuning i got stuck ok

03:00:36.720 --> 03:00:42.220
ok and and and now the good thing is all my research is

03:00:42.220 --> 03:00:46.320
fully funded by aws so i don't have to pay for anything ok

03:00:46.320 --> 03:00:50.480
that's amazing yeah and so so basically they fund all my

03:00:50.480 --> 03:00:56.340
crazy ideas ok and and and i believe maybe next week or so i

03:00:56.340 --> 03:01:00.700
will show you this fine tuning which is which is a workflow

03:01:00.700 --> 03:01:04.460
based ok this fine tuning which i have done for for a law

03:01:04.460 --> 03:01:09.520
company ok there is actually it's just my it's it's my

03:01:09.520 --> 03:01:17.740
wife's name company i call it ruby law ok so and this

03:01:17.740 --> 03:01:24.660
projector fine tuning projector fine tunes eight departments

03:01:24.660 --> 03:01:30.160
data ok ok and and and with workflow so basically if it's a

03:01:30.160 --> 03:01:35.300
partner partner can see only partners data ok if it's a

03:01:35.980 --> 03:01:40.420
attorney attorney can see only attorneys data ok if it is a

03:01:40.420 --> 03:01:45.740
front desk they can see their own data so basically with

03:01:45.740 --> 03:01:48.860
workflow workflow and i'm going to add maybe i'll showcase

03:01:48.860 --> 03:01:54.420
that demo next time whenever i'm ready to show it i mean i

03:01:54.420 --> 03:01:59.260
should be ready maybe in a week or so and and that gives me

03:01:59.260 --> 03:02:04.660
i mean your your lecture today was very interesting many of

03:02:04.660 --> 03:02:10.440
the things that you are showing i have done it and and and i

03:02:10.440 --> 03:02:15.540
was very happy to see that how that even the training data

03:02:16.340 --> 03:02:22.000
versus the data which is fine tuned that how those data i

03:02:22.000 --> 03:02:30.160
can showcase both data on the web to anybody so yeah so

03:02:30.160 --> 03:02:33.960
these are the things which i am building yeah that's amazing

03:02:33.960 --> 03:02:38.360
that's amazing to see yeah yeah yeah so i have lots of ideas

03:02:38.360 --> 03:02:41.900
and it's just i'm passing my time because i don't have

03:02:41.900 --> 03:02:47.220
anything to do so i can develop products right and and all

03:02:47.220 --> 03:02:51.120
these products are basically my own creation but based on

03:02:51.120 --> 03:02:56.320
your ideas so like when you deliver lecture i get your idea

03:02:56.320 --> 03:03:01.760
but i don't use any of your technologies because i i use

03:03:01.760 --> 03:03:06.740
sage maker i use bedrock i use nova light nova microdose

03:03:06.740 --> 03:03:10.080
kind of models so i use all those because they are free for

03:03:10.080 --> 03:03:14.660
me and they are high-end okay so that's okay that's okay you

03:03:14.660 --> 03:03:18.140
can use any of these things that's fine yeah so from you i

03:03:18.140 --> 03:03:23.420
love the ideas and then i use those ideas they convert those

03:03:23.420 --> 03:03:27.940
ideas into into aws technologies and that that's that's

03:03:27.940 --> 03:03:31.480
amazing yeah yeah i had a i had an interview couple of days

03:03:31.480 --> 03:03:35.420
with capgemini but i refuse refuse to join them because

03:03:35.980 --> 03:03:41.240
because that was like migrating mainframe to aws but i said

03:03:41.240 --> 03:03:45.760
i am more interested in jnai now and when they say that it's

03:03:45.760 --> 03:03:50.140
a java i said no i don't want to deal with java so because i

03:03:50.140 --> 03:03:54.060
used to work with capgemini before but if there is like a

03:03:54.060 --> 03:03:58.960
these native ai project when it comes some project i will

03:03:58.960 --> 03:04:02.840
take it so how do you like it is my new apple so is that

03:04:02.840 --> 03:04:06.640
what what was that what did you like my new app yeah well

03:04:06.640 --> 03:04:09.020
that's good actually it's good like ui interface wise and

03:04:09.020 --> 03:04:14.280
everything it's good yeah and yeah i mean those people who

03:04:14.280 --> 03:04:17.260
have seen my friends they say yeah they they like it very

03:04:17.260 --> 03:04:22.280
much yeah it's actually good yeah okay so yeah this is what

03:04:22.280 --> 03:04:25.900
i wanted to share yeah thanks thanks uh elengo please go

03:04:25.900 --> 03:04:30.200
ahead with the question hello sir um as we discussed before

03:04:30.200 --> 03:04:36.460
uh i have one project that i have the pdf file like a rfp

03:04:36.460 --> 03:04:40.480
document in the document the image and related uh

03:04:40.480 --> 03:04:44.220
informations like like image means how much quantity how

03:04:44.220 --> 03:04:48.760
much the description and uh more details will be there maybe

03:04:48.760 --> 03:04:52.460
sometime it will be the table for me sometime it will be

03:04:52.460 --> 03:04:57.460
like a like a description like a step by step i need to get

03:04:57.460 --> 03:05:02.500
the um related image and related description related

03:05:02.500 --> 03:05:08.080
quantity these things so after i retrieved i will uh i will

03:05:08.080 --> 03:05:13.160
uh match with the data or if i have in the big query uh i

03:05:13.160 --> 03:05:23.620
will match this um uh data with that table and get the price

03:05:23.620 --> 03:05:27.760
and calculate this is the things now i i don't know how to

03:05:27.760 --> 03:05:32.780
get the image and relevant data maybe same what you said

03:05:32.780 --> 03:05:36.560
with this i said before uh with the library we can get the

03:05:36.560 --> 03:05:42.620
informations but i need it the image and related data how i

03:05:42.620 --> 03:05:46.200
can get this so imagine literally you want to fetch from a

03:05:46.200 --> 03:05:51.160
pdf right yes pdf uh image and data just try to use any like

03:05:51.160 --> 03:05:53.780
a vision based lms i think that will be able to like a work

03:05:53.780 --> 03:05:56.840
and that will be able to bring all the images for you um but

03:05:56.840 --> 03:06:01.760
uh yeah i tried before also but if you take the image i need

03:06:01.760 --> 03:06:06.140
to bring the uh for example in the table in the one column

03:06:06.140 --> 03:06:09.980
is image another column is different related description

03:06:09.980 --> 03:06:15.460
related quantity or something i need to take the related

03:06:15.460 --> 03:06:18.980
information if i have only the image i can take the image

03:06:18.980 --> 03:06:22.680
and can take the text from the image i can do i need to the

03:06:22.680 --> 03:06:26.120
image and also related informations belongs to that image

03:06:27.880 --> 03:06:31.900
related information of that and that that is available right

03:06:31.900 --> 03:06:36.640
in a pdf document yeah in the pdf is available but sometime

03:06:36.640 --> 03:06:41.920
it will come as a table of uh in table that's okay that's

03:06:41.920 --> 03:06:44.780
okay yeah that's okay so i think yeah vlm is going to work

03:06:44.780 --> 03:06:47.360
at plus uh text based parsing anyhow you will be doing a

03:06:47.360 --> 03:06:50.500
text with parsing of the entire pdf right so by using maybe

03:06:50.500 --> 03:06:53.380
a pipe resurrect pdf for like a reader or libraries

03:06:53.380 --> 03:06:56.060
basically and there are like other library as well generally

03:06:56.060 --> 03:06:59.220
i use pipe resurrect or something like that so just just use

03:06:59.220 --> 03:07:04.100
that it will be able to do it so i can take how i take the

03:07:04.100 --> 03:07:08.760
image from that time vlm just try to use a vision based lms

03:07:10.840 --> 03:07:14.180
based lms yeah so that it will be able to understand the

03:07:14.180 --> 03:07:18.700
images and will be able to extract it so it will bring the

03:07:18.700 --> 03:07:21.600
uh like just just if you have to if you have to just extract

03:07:21.600 --> 03:07:24.900
the image part then we don't need it if we are looking for

03:07:24.900 --> 03:07:27.640
our understanding about the images then we'll need it so if

03:07:27.640 --> 03:07:30.700
we just have to extract the images in that case like a pdf

03:07:30.700 --> 03:07:33.820
parser is going to work for me right because i i can just

03:07:33.820 --> 03:07:36.420
try to like categorize it that if there is an image uh like

03:07:36.420 --> 03:07:39.080
let's try to separate it and keep it in some other document

03:07:39.080 --> 03:07:42.740
in some other folder or some other places right i mean you

03:07:42.740 --> 03:07:50.360
take the image and also the related related um then vlm vlm

03:07:50.360 --> 03:07:55.520
is required then right because vision based lms basically

03:07:55.520 --> 03:08:00.040
not like a exactly a vlm for hosting so vision based lms uh

03:08:00.040 --> 03:08:02.580
which is required so which will be able to understand your

03:08:02.580 --> 03:08:04.940
images as well as it will be able to understand your text

03:08:04.940 --> 03:08:10.680
then so it it will take the images in the table and if it's

03:08:10.680 --> 03:08:13.160
available in the table if you take the images are related

03:08:13.160 --> 03:08:17.740
values in the information yeah exactly everything everything

03:08:17.740 --> 03:08:20.900
because it's been built for a image understanding right

03:08:20.900 --> 03:08:24.900
image and text also so image and text also more or less uh

03:08:24.900 --> 03:08:28.120
those who will be able to understand the images they are

03:08:28.120 --> 03:08:35.580
very good with the text okay okay yeah so solution okay i

03:08:35.580 --> 03:08:39.540
have another doubt today the session is good for example if

03:08:39.540 --> 03:08:44.860
i use a fine tune i know the data which is not updated

03:08:44.860 --> 03:08:49.000
frequently the best i can go for the fine tuning if i update

03:08:49.000 --> 03:08:53.220
frequently i need to use the rag functionality yes yeah the

03:08:53.220 --> 03:08:58.220
thing is if i use a fine tune this is the one-time work i

03:08:58.220 --> 03:09:03.120
don't need to pay every time uh right so if i use a rag do i

03:09:03.120 --> 03:09:08.700
need to the the cloud provider will calculate will pay for

03:09:08.700 --> 03:09:11.920
each time when i go and take the data from the vector

03:09:11.920 --> 03:09:14.120
database and bring the data they will calculate the data

03:09:14.140 --> 03:09:17.620
charge for each time each time yeah so basically it depends

03:09:17.620 --> 03:09:20.620
you have to check their pricing tier for example if i'm not

03:09:20.620 --> 03:09:25.140
using uh any like uh you know managed cloud services for

03:09:25.140 --> 03:09:28.360
example quadrant or something so i'm using my own edelweiss

03:09:28.360 --> 03:09:31.020
and i'm trying to spin my own machine and i'm installing

03:09:31.020 --> 03:09:34.300
everything in my own machine in that case my machine is

03:09:34.300 --> 03:09:36.660
reserved so i will have to pay the reserve charge or fixed

03:09:36.660 --> 03:09:40.360
charge by the way right let's suppose if i'm going ahead

03:09:40.360 --> 03:09:43.420
with quadrant or this kind of a provider so they are going

03:09:43.420 --> 03:09:46.740
to charge you on storage plus they are going to charge you

03:09:46.740 --> 03:09:49.640
on transactions as well so whenever you will hit the query

03:09:49.640 --> 03:09:52.200
whenever they will try to do a cosine similarity search or

03:09:52.200 --> 03:09:54.280
any other similarity search they are going to charge you so

03:09:54.280 --> 03:09:56.420
everyone tries to charge you in a different different way

03:09:56.420 --> 03:10:00.160
right uh so you can either go with that or maybe go with a

03:10:00.160 --> 03:10:03.880
fixed cost maybe you can like spin your own instances and

03:10:03.880 --> 03:10:06.460
then uh use it there in that way like it is going to be

03:10:06.460 --> 03:10:11.420
fixed okay sir thank you thank you yeah yeah thanks okay

03:10:11.420 --> 03:10:15.020
next one hi shudan shuk uh could you please share your

03:10:15.020 --> 03:10:18.240
screen i have some question from the coding part okay fine

03:10:20.080 --> 03:10:24.500
uh so yeah my screen is visible i believe yeah yes loading

03:10:25.320 --> 03:10:28.920
yeah we we are able to see a lot of things today but where

03:10:28.920 --> 03:10:32.920
is the designing factor to improve the inferencing part and

03:10:33.500 --> 03:10:37.060
reduce the size of the model or something so which which

03:10:37.060 --> 03:10:40.380
code section is it relevant for does it control those part

03:10:40.380 --> 03:10:43.980
so basically you have to watch a last sunday you have to

03:10:43.980 --> 03:10:45.220
watch a last sunday is lecture i believe you have missed

03:10:45.220 --> 03:10:48.540
last sunday's lecture because all the discussion have

03:10:48.540 --> 03:10:50.920
started from there this is a second discussion from this

03:10:50.920 --> 03:10:54.640
project so i think we already have done two three hour of

03:10:54.640 --> 03:10:57.960
discussion in my previous session so where we already talked

03:10:57.960 --> 03:11:01.440
about our design side of it that why we are doing what we

03:11:01.440 --> 03:11:03.960
are doing what happens in a quora what happens in a prophet

03:11:03.960 --> 03:11:07.420
what happens in a quantized laura why quantizer is one of

03:11:07.420 --> 03:11:10.940
the optimized one everything has been discussed in our last

03:11:10.940 --> 03:11:15.500
section yeah yeah the batch size and laning rates and yeah

03:11:15.500 --> 03:11:18.700
no no not just not that is the training parameter right but

03:11:18.700 --> 03:11:21.540
we have started our discussion with the type of the training

03:11:21.540 --> 03:11:23.880
that what is quora what is preferred what is laura what is

03:11:23.880 --> 03:11:27.440
fine tuning what is dpo and then data set discussion also we

03:11:27.440 --> 03:11:31.340
have done yes yes correct yeah i i watched the video last

03:11:31.340 --> 03:11:34.960
video i know the difference on those my specific question on

03:11:34.960 --> 03:11:39.740
those laura and q laura are the best than other full time

03:11:39.740 --> 03:11:43.120
full fine tuning right so not the best one i would say yeah

03:11:43.120 --> 03:11:45.540
not the best one i would say optimize i mean like the

03:11:45.540 --> 03:11:48.600
smallest one the light weighted one a light weighted one

03:11:48.600 --> 03:11:52.340
correct yeah so uh my question is that there is some laning

03:11:52.340 --> 03:11:55.600
rate and epoch or a few more things when we pass in our code

03:11:55.600 --> 03:12:00.040
right yes so those things are the control and to improve our

03:12:00.040 --> 03:12:03.760
inferencing on the model or no no not inferencing actually

03:12:03.760 --> 03:12:06.280
training so these are the trainable parameter right and

03:12:06.280 --> 03:12:11.120
basically these parameter is coming from a basic very very

03:12:11.120 --> 03:12:14.160
basic neural network concept even that i have explained in

03:12:14.160 --> 03:12:18.040
that class and i have i said that that if you are not aware

03:12:18.040 --> 03:12:20.740
about a basic neural network so maybe you can go and watch

03:12:20.740 --> 03:12:23.840
uh just one or two my deep learning videos so where i have

03:12:23.840 --> 03:12:26.780
like uh done uh these kind of a training because this is

03:12:26.780 --> 03:12:29.460
epoch means wherever you will train the network you will set

03:12:29.460 --> 03:12:31.320
the epoch you will set the batch size you will set the

03:12:31.320 --> 03:12:34.900
learning rate you will set all of these parameter so this is

03:12:34.900 --> 03:12:38.520
by default for any network you are going to use it and you

03:12:38.520 --> 03:12:41.680
will have to play with the parameter right okay i think i

03:12:41.680 --> 03:12:44.880
have done the explanation for that okay all right i will go

03:12:44.880 --> 03:12:48.040
through that one and one more thing for this bunch of coding

03:12:48.040 --> 03:12:51.100
right is there any possibility can we make it break points

03:12:51.100 --> 03:12:54.560
and uh try to understand the whole flow is it possible yeah

03:12:54.560 --> 03:12:57.460
any any time you can make a break point so if you have to

03:12:57.460 --> 03:12:59.580
put a break point anywhere so you can just try to like put

03:12:59.580 --> 03:13:04.160
this this this right and then you can run it yeah yeah i

03:13:04.160 --> 03:13:07.260
mean like it's an ids feature right it's like uh not nothing

03:13:07.260 --> 03:13:10.140
related to this one so it's a basically vs code feature you

03:13:10.140 --> 03:13:11.780
can run it and then you can put a break point on it run with

03:13:11.780 --> 03:13:15.940
the back point debug mode yeah i am asking uh when we run

03:13:15.940 --> 03:13:19.140
the after the docker installation right you have done the

03:13:19.140 --> 03:13:22.140
docker part right when we get the api server right is it

03:13:22.140 --> 03:13:26.800
possible to uh is it possible to start the breakpoint from

03:13:26.800 --> 03:13:30.580
those place and when we click the url what is happening in

03:13:30.580 --> 03:13:34.280
behind can we track the whole thing inside a docker so you

03:13:34.280 --> 03:13:36.640
will be able to see everything inside a docker logs by the

03:13:36.640 --> 03:13:40.320
way or so here that is a first place or maybe you can just

03:13:40.320 --> 03:13:43.500
try to call locker compose uh log f so that is going to show

03:13:43.500 --> 03:13:47.760
you all the uh like uh logs inside it now what is happening

03:13:47.760 --> 03:13:51.880
inside it so technically this uh like uh this is happening

03:13:51.880 --> 03:13:55.160
api server we are running right so if you have to track this

03:13:55.160 --> 03:13:59.240
one so call api server and then track it inside a docker

03:13:59.240 --> 03:14:01.740
will show you a docker log that okay so what what has

03:14:01.740 --> 03:14:04.620
happened basically if you have to like run something into a

03:14:04.620 --> 03:14:07.240
debugger mode uh basically and then you have to check all

03:14:07.240 --> 03:14:12.460
these technically your docker is calling api server.pi file

03:14:12.460 --> 03:14:16.620
right this file yeah correct yeah so run so you can i have

03:14:16.620 --> 03:14:19.220
already shown you right so you can run like a python api

03:14:19.220 --> 03:14:22.000
server.pi file directly and then run it in a debug mode and

03:14:22.000 --> 03:14:27.160
then you can test it okay okay fine just a docker log

03:14:27.160 --> 03:14:31.020
basically yeah yeah so when we docker compose it up on all

03:14:31.020 --> 03:14:33.580
the application everything it is in running inside the

03:14:33.580 --> 03:14:36.600
docker it's not in our inside inside yeah inside a box a

03:14:36.600 --> 03:14:42.000
small box yes inside the box okay so how do we often run the

03:14:42.000 --> 03:14:44.680
fine tuning in real time projects so it is a routine

03:14:44.680 --> 03:14:48.600
activity or is it to be will it be it's a once in a while

03:14:48.600 --> 03:14:51.320
kind of activity fine tuning so once we have trained because

03:14:51.320 --> 03:14:54.120
we don't do a model training on a daily basis by the way

03:14:54.120 --> 03:14:59.480
maybe once in a once in a quarter we try to do it something

03:14:59.480 --> 03:15:03.100
like that yeah will it be done by the developer or in

03:15:03.100 --> 03:15:06.820
protein it will be done by a development team development

03:15:06.820 --> 03:15:10.200
team now nowadays nowadays anyhow uh much more than that

03:15:10.220 --> 03:15:13.040
people are like team sizes getting swing right so

03:15:13.040 --> 03:15:15.300
expectation will be very simple that you should know

03:15:15.300 --> 03:15:18.120
everything uh even in our big companies nowadays right that

03:15:18.120 --> 03:15:20.720
is a that is a trend because we all know that that uh if

03:15:20.720 --> 03:15:23.620
someone knows it it's very easy for them right you don't

03:15:23.620 --> 03:15:26.740
need a bigger team who will manage just an infra or this or

03:15:26.740 --> 03:15:30.160
that so technically it will be done by a small piece of the

03:15:30.160 --> 03:15:34.160
team all the time okay and everything starting from like you

03:15:34.160 --> 03:15:38.720
know training till hosting yeah right okay but challenging

03:15:38.720 --> 03:15:41.560
will be also increased right the team has been reduced to

03:15:41.560 --> 03:15:44.260
the challenging will be increased no not like that depends

03:15:44.260 --> 03:15:47.400
upon how efficient my team members are right if team members

03:15:47.400 --> 03:15:49.820
are not well learned and well trained in that case obviously

03:15:49.820 --> 03:15:53.300
uh there will be a huge challenges so they will end up

03:15:53.300 --> 03:15:56.560
scratching their heads if they are well trained and you know

03:15:56.560 --> 03:15:59.700
they have experience in that case you know managers can sit

03:15:59.700 --> 03:16:04.040
peacefully team will do the job okay it's a people thing

03:16:04.040 --> 03:16:07.400
right simple yeah yeah i agree with you and will it be

03:16:07.400 --> 03:16:11.240
applied this fine tuning in our normal machine learning this

03:16:11.240 --> 03:16:14.860
fine tuning technique it's a lm fine tuning basically isn't

03:16:14.860 --> 03:16:17.740
purely llm is there any fine tuning methods for our normal

03:16:17.740 --> 03:16:21.060
human-based uh human-based machine learning techniques there

03:16:21.580 --> 03:16:23.660
i'm like for machine learning we don't do a fine tuning for

03:16:23.660 --> 03:16:26.500
the vision waste model pps model we do a fine tuning machine

03:16:26.500 --> 03:16:27.980
learning so we build a model

03:16:30.560 --> 03:16:32.900
it's a very very small equation right it's a very

03:16:32.900 --> 03:16:35.100
lightweight equation so we don't go with the fine tuning in

03:16:35.100 --> 03:16:39.440
general for machine learning okay okay yeah but uh or if we

03:16:39.440 --> 03:16:42.200
go with the volume data huge volume data with our machine

03:16:42.200 --> 03:16:45.120
learning doesn't matter that doesn't matter doesn't depend

03:16:45.120 --> 03:16:47.280
on the data how many parameter we are training that matters

03:16:47.280 --> 03:16:49.900
most right that is the actual thing which matters so in

03:16:49.900 --> 03:16:52.520
machine learning if i'm trying to like uh train y is equal

03:16:52.520 --> 03:16:55.160
to mx plus c i just have two parameter m and c so i just

03:16:55.160 --> 03:16:58.520
have to train those two parameter right so like uh doesn't

03:16:58.520 --> 03:17:03.680
matter how much data we have okay okay got it uh yeah okay

03:17:03.680 --> 03:17:09.260
sure thank you yeah yeah okay uh nares please go ahead with

03:17:09.260 --> 03:17:14.480
the question yeah uh hi it was a great lecture thank you uh

03:17:14.480 --> 03:17:17.680
so i just wanted to know uh in between my internet

03:17:17.680 --> 03:17:20.600
connection was stable so i am not aware whether someone

03:17:20.600 --> 03:17:25.460
asked this question or not how do you be uh evaluate that uh

03:17:25.460 --> 03:17:29.200
whatever fine tuning we have done so how do we evaluate uh

03:17:29.200 --> 03:17:32.500
whether this uh you know how do you evaluate basically like

03:17:32.500 --> 03:17:37.840
fine tuning is it went well or not how do we evaluate same

03:17:37.840 --> 03:17:42.460
precision we call model evaluation technique okay yeah it's

03:17:42.460 --> 03:17:44.540
a model by the end of the day after finding what we have

03:17:44.540 --> 03:17:47.960
done we have created the model right correct so because the

03:17:47.960 --> 03:17:52.440
technique is going to be same precision recall f1 score uh

03:17:52.440 --> 03:17:55.700
because the question uh the question reason behind the

03:17:55.700 --> 03:18:00.420
question is uh i told you open a model i have fine tune but

03:18:00.420 --> 03:18:04.100
when i asked some question because i have given very uh less

03:18:04.100 --> 03:18:08.900
data set around data sets maybe less data set maybe less

03:18:08.900 --> 03:18:13.400
epoch possible yeah so whenever i ask the question even

03:18:13.400 --> 03:18:18.080
though the the answer is related to my data set still it is

03:18:18.080 --> 03:18:22.720
uh you know picking from uh general uh content maybe you

03:18:22.720 --> 03:18:25.860
have not uh like a trended for like uh enough number of

03:18:25.860 --> 03:18:30.640
epoch that could be the problem means epoch means you have

03:18:30.640 --> 03:18:34.000
not reduced the loss basically okay are we going to cover

03:18:34.000 --> 03:18:36.520
this evaluation part also in the coming lectures or

03:18:36.520 --> 03:18:40.100
evaluation yeah i can like talk about this anytime but again

03:18:40.100 --> 03:18:43.040
i think we all know the model evaluation precision recall f1

03:18:43.040 --> 03:18:45.340
score just try to call it confusion matrices we are aware

03:18:45.340 --> 03:18:48.920
about and again like uh what kind of evaluation what point

03:18:48.920 --> 03:18:51.140
of like a model that we have built so if it is a

03:18:51.140 --> 03:18:53.980
mathematical model then uh evaluation score will be

03:18:53.980 --> 03:18:56.380
different if it is just a nlp so in that case blue score

03:18:56.380 --> 03:18:59.940
will come into a picture right uh which has been used for a

03:18:59.940 --> 03:19:03.360
language evaluation by the way so it depends what is the

03:19:03.360 --> 03:19:05.600
nature of the model we are creating but in general yeah

03:19:05.600 --> 03:19:08.560
precision recall and f1 but uh it also depends upon the

03:19:08.560 --> 03:19:14.200
specific creation in a specific segment okay and my second

03:19:14.200 --> 03:19:18.060
question is like related to my company's project we are into

03:19:18.060 --> 03:19:23.020
home automation domain okay okay oh we have a smart fan

03:19:23.020 --> 03:19:27.920
smart lights etc right so we are going to give uh we have

03:19:27.920 --> 03:19:31.580
already app to control all these appliances now we are

03:19:31.580 --> 03:19:36.400
thinking of giving you know uh wise uh wise based control of

03:19:36.400 --> 03:19:41.840
these devices okay okay so what should be my approach should

03:19:41.840 --> 03:19:47.240
i use direct any open a open a llm so that command i will

03:19:47.800 --> 03:19:51.260
send it to open a llm and then i will expect structured json

03:19:51.260 --> 03:19:54.080
output what operation user wants to do just use some library

03:19:54.080 --> 03:19:56.700
tts library text to a speech library just use it uh don't

03:19:56.700 --> 03:20:00.340
have to use a model for that that will work but how do i i

03:20:00.340 --> 03:20:03.220
know that which operation user wants to do and what is the

03:20:03.220 --> 03:20:07.080
device name because device name will be uh different for uh

03:20:07.080 --> 03:20:10.820
different devices right so if i want to extract from the

03:20:10.820 --> 03:20:15.640
text okay so device name how you are going to extract so in

03:20:15.640 --> 03:20:18.400
that case whenever we build any kind of application so

03:20:18.400 --> 03:20:21.860
basically it try to you know uh like uh fetch a lot of

03:20:21.860 --> 03:20:24.140
information from a client itself so for example if something

03:20:24.140 --> 03:20:26.620
is running in my mobile right uh so let's suppose if i'm

03:20:26.620 --> 03:20:29.140
using whatsapp so whatsapp can see that which while i'm

03:20:29.140 --> 03:20:31.500
using what is the version of the os running in my mobile

03:20:31.500 --> 03:20:34.840
what time everything everything i will be able to like a get

03:20:34.840 --> 03:20:37.500
as a detail so that by default any application you are going

03:20:37.500 --> 03:20:42.360
to create you will be able to access no no like let's say if

03:20:42.360 --> 03:20:47.280
user asks turn on my fan one right i need to know that he

03:20:47.280 --> 03:20:50.740
wants to turn on fan one right and the command will be uh

03:20:50.740 --> 03:20:53.720
different in different cases so sometimes he may say he may

03:20:53.720 --> 03:20:57.660
say uh turn on fan one or fan one turn on you know in

03:20:57.660 --> 03:21:00.820
different okay so if this is the okay i was i was like

03:21:00.820 --> 03:21:03.700
giving you answer in a different context so okay if this is

03:21:03.700 --> 03:21:06.620
the context of this one pass so you have done text to his

03:21:06.620 --> 03:21:08.560
speech right after text to his speech you will pass your

03:21:08.560 --> 03:21:11.940
text to the speech to text so speech to text conversion and

03:21:11.940 --> 03:21:15.140
then pass your text to lm lm can understand or agents

03:21:15.140 --> 03:21:19.340
technically okay so should i use this and my question is

03:21:19.340 --> 03:21:23.800
should i use uh llms like open ai so that it is very uh you

03:21:23.800 --> 03:21:27.380
know easy to implement or should i take any uh any tiny

03:21:27.380 --> 03:21:30.960
model and you know do the fine tuning that if command is

03:21:30.960 --> 03:21:34.200
like this you have to pass if it should if it should run in

03:21:34.200 --> 03:21:37.280
a very very small devices right without even internet so in

03:21:37.280 --> 03:21:42.640
that case quantize model to one second answer so let's

03:21:42.640 --> 03:21:47.720
suppose if device is having a internet access right in that

03:21:47.720 --> 03:21:50.580
case you can try to use any model which has been hosted on

03:21:50.580 --> 03:21:53.260
any server in this entire world that could be a open AI

03:21:53.260 --> 03:21:56.180
model that could be your own model which is hosted on your

03:21:56.180 --> 03:22:00.060
server so that will be my answer so tiny device without

03:22:00.060 --> 03:22:04.540
internet then quantized it's not tiny devices internet

03:22:04.540 --> 03:22:10.180
access in that case anywhere okay so now related to fine

03:22:10.180 --> 03:22:15.760
-tuning so say I have a llama model with the 1.1 people up

03:22:15.760 --> 03:22:18.720
1b parents are three three billion parameters whatever it is

03:22:18.720 --> 03:22:24.580
okay now if I want to do this model for a specific language

03:22:24.580 --> 03:22:31.840
like Canada or Telugu whatever language so what kind of fine

03:22:31.840 --> 03:22:34.620
-tune approach should I follow first of all you have to

03:22:34.620 --> 03:22:36.700
check whether the smaller support that language or not

03:22:36.700 --> 03:22:39.500
because if you are like you know going ahead with your

03:22:40.120 --> 03:22:42.840
languages and if that doesn't understand it's not been

03:22:42.840 --> 03:22:45.960
trained then it's not ideal way to go ahead with so first

03:22:45.960 --> 03:22:48.660
select a model which understand your language in whatever

03:22:48.660 --> 03:22:53.300
language you are looking for and then if you would like to

03:22:53.300 --> 03:22:56.680
like go ahead with basically full fine-tuning go ahead with

03:22:56.680 --> 03:22:59.400
that Laura preferred Cora you can go out with anything that

03:22:59.400 --> 03:23:03.040
you want dip but again case by case like everywhere you

03:23:03.040 --> 03:23:06.080
should not like what is the your end goal depends upon that

03:23:06.080 --> 03:23:09.400
you should choose a training approach you see my end goal is

03:23:09.400 --> 03:23:13.600
like let's say user speaks in Canada if it should convert

03:23:13.600 --> 03:23:17.840
into related English command okay so language translation

03:23:17.840 --> 03:23:22.320
basically right yes okay so in that case why to train the

03:23:22.320 --> 03:23:24.540
model you will not be able to get all enough kind of a data

03:23:24.540 --> 03:23:27.940
so it's always better to use a already trained model if

03:23:27.940 --> 03:23:29.900
someone has already released a model which can understand

03:23:29.900 --> 03:23:32.460
English to Canada Canada to English just go ahead and use

03:23:32.460 --> 03:23:37.120
that model simple it will do it I think all this big model

03:23:37.120 --> 03:23:40.600
that we have we have seen nowadays it's it's actually very

03:23:40.600 --> 03:23:43.980
good with most of this like these languages yeah I have

03:23:43.980 --> 03:23:47.020
implemented using open AI already but my company is forcing

03:23:47.020 --> 03:23:51.520
me to have you know one LLM in our infrastructure so then

03:23:51.520 --> 03:23:56.120
then open I have also released open AI 20b OSS right and

03:23:56.120 --> 03:24:01.120
120b use that okay simple that that that can work in

03:24:01.120 --> 03:24:03.300
anywhere right in your server you don't have to rely upon

03:24:03.300 --> 03:24:06.780
the open AI servers got it got it thank you thank you yes

03:24:06.780 --> 03:24:14.350
yes thanks okay please go ahead uh hi sir I'm out yeah yes

03:24:14.350 --> 03:24:17.450
so there you are devil yeah uh sir a question

03:24:46.510 --> 03:24:54.250
but normally hurry. But I don't know. What scripton could

03:24:54.250 --> 03:25:00.230
GPU making for even your training 1500 eight since you are

03:25:00.230 --> 03:25:02.990
relaxing a Attenborough laded and. I'm sorry man. I love

03:25:02.990 --> 03:25:07.330
that. Subhanallah. Just because. Yeah it was less be. So we

03:25:07.330 --> 03:25:09.530
are going to do this in the cloud, so GPT will run the same

03:25:09.530 --> 03:25:13.330
thing in the cloud as well, training and inferencing both.

03:25:14.330 --> 03:25:17.330
Yes, sir, I mean this training, maybe there is something

03:25:17.330 --> 03:25:20.330
connected to it. We can access it from Patti and WinSAP,

03:25:21.570 --> 03:25:24.130
which we have already done, HyperStack Cloud, right, we have

03:25:24.130 --> 03:25:26.790
already shown that. And if I want to transfer this file

03:25:26.790 --> 03:25:29.730
there, then we will use WinSAP for that. I mean, if you are

03:25:29.730 --> 03:25:31.890
doing it from Windows, then this file will be transferred

03:25:31.890 --> 03:25:34.430
from WinSAP to my GPU, no matter where the instance is, and

03:25:34.430 --> 03:25:38.890
then we can run the training. If you want to do this, you

03:25:38.890 --> 03:25:41.270
can do it tomorrow, it won't take much time, at least half

03:25:41.270 --> 03:25:44.570
an hour. Yes, sir, if this is parted, then it will be very

03:25:44.570 --> 03:25:47.170
good. There is nothing, I think, yes, okay, we can do this

03:25:47.170 --> 03:25:50.530
thing tomorrow. Yes, sir. It's very, very easy. Yes, sir.

03:25:50.570 --> 03:25:52.530
There was one more question, sir. Like we have just made

03:25:52.530 --> 03:25:58.270
different fine-tuned models, HR, marketing. Like our use

03:25:58.270 --> 03:26:01.090
cases are similar. Finance, HR, they are all the same, but

03:26:01.090 --> 03:26:05.610
do they make different fine-tuned models or do they make

03:26:05.610 --> 03:26:08.670
only one model in which all the categories are different?

03:26:08.870 --> 03:26:11.730
No, it depends. If a client comes and says, no, my finance

03:26:11.730 --> 03:26:14.210
department is very big, so we need only one model, every

03:26:14.210 --> 03:26:16.150
department-wise. Because all the companies will be big,

03:26:16.450 --> 03:26:18.570
right? It's not like we will make one and use it everywhere,

03:26:18.710 --> 03:26:21.770
right? Everyone has different, like HR. The HR policy is

03:26:21.770 --> 03:26:27.030
almost the same for all companies. Okay. Because the same

03:26:27.030 --> 03:26:28.550
thing is applicable to everyone. So, there is a model for

03:26:28.550 --> 03:26:33.190
HR. Okay. So, we keep it different. If I will talk about a

03:26:33.190 --> 03:26:36.190
big process, then we will keep it separate. It's better to

03:26:36.190 --> 03:26:38.470
keep it separate. I mean, we do modular coding, right? If my

03:26:38.470 --> 03:26:41.690
code base is big. Right. Similarly, here also we are doing

03:26:41.690 --> 03:26:46.750
modular fine-tuning. So, let's keep it separate. Okay, sir.

03:26:46.950 --> 03:26:50.190
So, if we keep it separate, then before that, we have to put

03:26:50.190 --> 03:26:54.110
a classification layer, right? That the user query should be

03:26:54.110 --> 03:26:57.850
classified first. Put an agent there. Put a simple agent

03:26:57.850 --> 03:26:59.270
there that these five models should be classified. You can

03:26:59.270 --> 03:27:03.050
call these five models. So, you first understand that there

03:27:03.050 --> 03:27:05.730
is a query related to HR. First this and this. And then

03:27:05.730 --> 03:27:08.410
based on that, if there is a query related to HR, then it

03:27:08.410 --> 03:27:10.190
will call the HR model. If there is a query related to

03:27:10.190 --> 03:27:12.270
finance, then it will call the finance. This is how we do

03:27:12.270 --> 03:27:17.630
it. Right. I mean, the agent is routing me, basically. Which

03:27:17.630 --> 03:27:22.270
model to use to get the response. Right. I think. Thanks,

03:27:22.330 --> 03:27:26.590
sir. That's it. Thank you, sir. Thank you. Okay. So, fine

03:27:26.590 --> 03:27:28.850
everyone. Hope all of you have enjoyed this session a lot.

03:27:28.850 --> 03:27:32.690
So, tomorrow, as Uday has requested, so like he told me that

03:27:32.690 --> 03:27:37.730
if we can see the same thing running on a GPU. So, before

03:27:37.730 --> 03:27:41.250
starting, we'll see if we can start MCP or not. So, if all

03:27:41.250 --> 03:27:44.610
of you will agree, I'll run the same thing on H100 in terms

03:27:44.610 --> 03:27:48.950
of training plus inferencing, both. And then like we can get

03:27:48.950 --> 03:27:52.810
a real time global API. So, what do you say, guys? Whoever

03:27:52.810 --> 03:27:58.170
is here. Yeah. Shall we do it? Okay. Then let's do that

03:27:58.170 --> 03:28:01.570
first. And then we'll start with MCP, Model Context

03:28:01.570 --> 03:28:04.750
Protocol. Okay. Fine. So, fine guys. With that, thank you so

03:28:04.750 --> 03:28:06.710
much. Take care and see you again. File I have already

03:28:06.710 --> 03:28:09.530
uploaded in a previous section and even with this recording,

03:28:09.650 --> 03:28:11.710
I'll upload the file. But yeah. So, files are already

03:28:11.710 --> 03:28:14.470
available. So, you should not wait for me to upload the

03:28:14.470 --> 03:28:17.010
recording. You can start your work and experimentation if

03:28:17.010 --> 03:28:19.730
you want even now. So, fine guys. With that, take care. Take

03:28:19.730 --> 03:28:22.050
care, everyone. See you tomorrow. Same time.

